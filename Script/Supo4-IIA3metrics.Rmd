---
title: "IIA-3 Econometrics: Supervision 4"
author: "Emre Usenmez"
date: "Christmas Break 2024"
output: pdf_document
header-includes: 
  - \usepackage{amsmath, tcolorbox, dashrule, booktabs, fancyhdr, multirow}
  - \tcbuselibrary{listings,most}
  - \allowdisplaybreaks
---

<!-- This comment will not be displayed in the output. Below change to CSS style is to ensure the blocktexts are in the same form size as the rest of the text.-->

```{css style settings, echo = FALSE} 
blockquote {
    padding: 10px 20px;
    margin: 0 0 20px;
    font-size: 14px;
    border-left: 5px solid #eee;
}
```

<!-- below ensures the output are not presented in Scientific mode (e.g. 0.023+e4) but regular decimals -->
```{r, echo=FALSE}
options(scipen = 999)
```


\pagestyle{fancy}
\fancyhead[L]{2024-25 Part IIA Paper 3}
\fancyhead[R]{Supervision 4 Solutions}
\fancyfoot[L]{Gonville \& Caius}
\fancyfoot[R]{Emre Usenmez}


\textbf{\underline{Topics Covered}}
\begin{description}
\item[Faculty Qs:] Endogeneity; proxy variable; instrumental variable
\item[Supplementary Qs:] Heteroskedasticity; White's robust standard errors; Goldfeld-Quandt test; Breusch-Pagan test; Lagrange Multiplier test; White test; autocorrelation; Durbin-Watson $d$ test; Breusch-Godfrey test; Durbin's alternative test; Prais-Winsten transformation; Cochrane-Orcutt Two-Step Procedure
\end{description}

\bigskip

\textbf{\underline{Related Reading:}}
\begin{description}
\item Dougherty, \textit{Introduction to Econometrics}, $5^{th}$ ed, OUP
  \subitem Chapter 7: Heteroskedasticity
  \subitem Chapter 8: Stochastic Regressors and Measurement Errors 
  \subitem Chapter 9: Simultaneous Equations Estimation
  \subitem Chapter 12: Autocorrelation
\item Wooldridge J M (2021) \textit{Introductory Econometrics: A Modern Approach}, $7^{th}$ ed,
  \subitem Chapter 8: Heteroskedasticity
  \subitem Chapter 9: More on Specification and Data Issues 
  \subitem Chapter 12: Serial Correlation and Heteroskedasticity in Time Series Regressions
  \subitem Chapter 16: Simultaneous Equations Models
\item Gujarati, D N and Porter, D (2009) \textit{Basic Econometrics} $7^{th}$ International ed, McGraw Hill
  \subitem Chapter 11 Heteroscdasticity: WHat Happens If the Error Variance is Nonconstant?
  \subitem Chapter 12: Autocorrelation: What Happens If the Error Terms are Correlated?
  \subitem Chapter 13: Econometric Modeling: Model Specification and Diagnostic Testing 
  \subitem Chapter 20: Simultaneous-Equation Methods
\end{description}

\bigskip

\small Very grateful to Dr Oleg Kitov and Dr Clive Lawson for the very informative stylized answers to previous iterations of the supervision questions.
\normalsize



\pagebreak

# FACULTY QUESTIONS

\bigskip\bigskip
 

## QUESTION 1

\textbf{Consider the following bivariate linear regression}
\[
y = \alpha + T\beta + u
\]
\textbf{where $T$ is a binary treatment regressor, $\alpha$ and $\beta$ are unknown parameters, and $u$ is an error term.}


\bigskip

### (a) Describe in two sentences an empirical, real-life example where such an equation might arise.

\begin{description}
\item[Answer:] 

We can think of $T$ as "graduated from university" and $y$ as "annual earning after 10 years of graduation."

\end{description}

\bigskip\bigskip
***
\bigskip\bigskip

### (b) Why might $u$ be heteroskedastic in your example.

\begin{description}
\item[Answer:] 

The variance of earnings will likely to be smaller across people who did not graduate from a university compared to those who did it. This may be because those who did not go to university are less likely to be in the professions such as lawyers or doctors, and more likely to be in lower-paying jobs, or unemployed, or out of labor force.

\end{description}


\bigskip\bigskip
***
\bigskip\bigskip

### (c) Why might T be endogenous in your example?

\begin{description}
\item[Answer:] 

Broadly, variables that are correlated with the error term are called \textit{endogeneous variables}, and those that are uncorrelated with the error term are called \textit{exogeneous variables}.\footnote{See Chapter 12: Instrumental Variables Regression p.428 in Stock J H, and Watson M W (2020) Introduction to Econometrics, $4^{th}$ Global Ed, Pearson; and Section 8.5: Instrumental Variables in Dougherty C (2016) Introduction to Econometrics $5^{th}$ ed, OUP in addition to Chapter 9: More on Specification and Data Issues in Wooldridge J M (2021) Introductory Econometrics: A Modern Approach, $7^{th}$ ed, Cengage}

Thus the question is asking us to consider some of the reasons as to why $T$ might be correlated with the error term. There are certainly nonnegligible number of high earners who either never went to a university or dropped out. There may be omitted variable or even simultaneity is possible.

Let's consider what the implications of of endogeneity are for the OLS estimator of $\beta$.

Variable $T$ would be endogenous if $\mathbb{E}(u|T) \neq 0$. Endogeneity would imply that $Cov(T,u)\neq 0$. 

We can first look at whether it is biased. For that, we need to use the law of iterated expectations whereby
\[
\mathbb{E}(\hat{\beta}^{OLS}) = \mathbb{E}\Big[\mathbb{E}(\hat{\beta}^{OLS}\ |\ T_1,\dots,T_n)\Big]
\]
The OLS estimator of $\beta$ would be:
\begin{align*}
\mathbb{E}(\hat{\beta}^{OLS} | T_1,\dots,T_n) 
  &= \mathbb{E}\Bigg( \frac{\widehat{Cov}(T_i,Y_i)}{\widehat{Var}(T_i)} \ \Bigg{|}\ T_1,\dots,T_n \Bigg) = \mathbb{E} \Big( \frac{\hat{\sigma}_{TY}}{\hat{\sigma}_{TT}} \ \bigg{|}\ T_1,\dots,T_n \Big) \\[4pt]
  &= \mathbb{E}\left( \frac{\displaystyle\sum_{i=1}^n(T_i-\bar{T})(Y_i-\bar{Y})}{\displaystyle\sum_{i=1}^n(T-\bar{T})^2}\ \Bigg{|}\  T_1,\dots,T_n \right) \\[4pt]
  &= \mathbb{E}\left( \frac{\displaystyle\sum_{i=1}^n(T_i-\bar{T})\big((\alpha + \beta T_i + u_i) - (\alpha + \beta \bar{T} + \bar{u})\big)}{\displaystyle\sum_{i=1}^n(T-\bar{T})^2} \ \Bigg{|}\ T_1,\dots,T_n \right)\\[4pt]
  &= \mathbb{E}\left( \frac{\displaystyle\sum_{i=1}^n(T_i-\bar{T})\big(\beta(T_i-\bar{T}) + u_i - \bar{u}\big)}{\displaystyle\sum_{i=1}^n(T-\bar{T})^2}  \ \Bigg{|}\ T_1,\dots,T_n \right) \\[4pt]
  &= \mathbb{E}\left( \frac{\displaystyle\sum_{i=1}^n \beta(T_i-\bar{T})^2 + \displaystyle\sum_{i=1}^n(T_i-\bar{T})(u_i - \bar{u})}{\displaystyle\sum_{i=1}^n(T-\bar{T})^2} \ \Bigg{|}\ T_1,\dots,T_n \right) \\[4pt]
  &= \mathbb{E}\left( \beta + \frac{\displaystyle\sum_{i=1}^n(T_i-\bar{T})(u_i-\bar{u})}{\displaystyle\sum_{i=1}^n(T_i-\bar{T})^2} \ \Bigg{|}\ T_1,\dots,T_n \right) \\[4pt]
  &= \mathbb{E}\left( \beta + \frac{\displaystyle\sum_{i=1}^n(T_i-\bar{T})u_i - \bar{u}\displaystyle\sum_{i=1}^n(T_i-\bar{T})}{\displaystyle\sum_{i=1}^n(T_i-\bar{T})^2} \ \Bigg{|}\ T_1,\dots,T_n \right) \\[4pt]
  &= \mathbb{E}\left(\beta + \frac{\displaystyle\sum_{i=1}^n(T_i-\bar{T})u_i - \bar{u}\left(\sum_{i=1}^nT_i-n\bar{T}\right)}{\displaystyle\sum_{i=1}^n(T_i-\bar{T})^2} \ \Bigg{|}\ T_1,\dots,T_n \right) \\[4pt]
  &= \mathbb{E}\left( \beta + \frac{\displaystyle\sum_{i=1}^n(T_i-\bar{T})u_i - \bar{u}\left(n\bar{T}-n\bar{T})\right)}{\displaystyle\sum_{i=1}^n(T_i-\bar{T})^2} \ \Bigg{|}\ T_1,\dots,T_n \right) \\[4pt]
  &= \mathbb{E}\left( \beta + \frac{\displaystyle\sum_{i=1}^n(T_i-\bar{T})u_i}{\displaystyle\sum_{i=1}^n(T_i-\bar{T})^2} \ \Bigg{|}\ T_1,\dots,T_n \right) \\[4pt]
  &= \beta + \frac{\displaystyle\sum_{i=1}^n(T_i-\bar{T})\ \mathbb{E}(u_i \ |\ T_1,\dots,T_n)}{\mathbb{E}\left( \displaystyle\sum_{i=1}^n(T_i-\bar{T})^2 \ \Big{|}\ T_1,\dots,T_n \right)} \\[4pt]
\end{align*}
Notice that since $\mathbb{E}(u|T) \neq 0$, the numerator of this last expression is also nonzero. That is, $\sum_{i=1}^n(T_i-\bar{T})\ \mathbb{E}(u_i \ |\ T_1,\dots,T_n) \neq 0$. Therefore the expectation of this expectation is also not equal to $\beta$:
\[
\mathbb{E}(\hat{\beta}^{OLS}) = \mathbb{E}\Big[\mathbb{E}(\hat{\beta}^{OLS}\ |\ T_1,\dots,T_n)\Big] = \mathbb{E}\left[ \mathbb{E}\left( \beta + \frac{\displaystyle\sum_{i=1}^n(T_i-\bar{T})u_i}{\displaystyle\sum_{i=1}^n(T_i-\bar{T})^2} \ \Bigg{|}\ T_1,\dots,T_n \right] \right) \neq \beta
\]
which means the OLS estimator is \textit{not} unbiased.

We can also check for consistency by examining the probability limit of this expression as $n$ tends towards infinity. For that, we can rewrite the OLS estimator as:
\[
\hat{\beta}^{OLS} = \beta + \frac{\displaystyle\frac{1}{n}\displaystyle\sum_{i=1}^n(T_i-\bar{T})u_i}{\displaystyle\frac{1}{n}\displaystyle\sum_{i=1}^n(T_i-\bar{T})^2} 
\]
Using the law of large numbers, we can see that as $n \to \infty$
\begin{align*}
\frac{1}{n}\sum_{i=1}^n(T_i-\bar{T})u_i &\overset{p}{\to} \mathbb{E}\big[(T_i - \bar{T})u_i\big] = Cov(T_i,u_i)\neq 0 \\
\text{and}\\
\frac{1}{n}\sum_{i=1}^n(T_i-\bar{T})^2 &\overset{p}{\to} \mathbb{E}\Big[(T_i-\bar{T})^2\Big] = Var(T_i) = \sigma^2_T < \infty
\end{align*}
Note that $Var(T_i) = \sigma^2_T < \infty$ is an additional assumption.

Since $Cov(T_i,u_i)\neq 0$, the OLS estimator as $n \to \infty$ (using Slutsky's theorem):
\[
\hat{\beta}^{OLS} \overset{p}{\to}\beta+\frac{Cov(T_i,u_i)}{Var(T_i)}\neq \beta
\]
which means the OLS estimator is not only biased but also inconsistent for $\beta$.
\end{description}

\bigskip\bigskip
***
\bigskip\bigskip

### (d) Suppose a single instrument $z$ is available. Show that the population coefficient $\beta$ satisfies
\[
\beta = \frac{Cov(z,y)}{Cov(z,T)}
\]
\textbf{where $Cov(z,y)$ and $Cov(z,T)$ denote, respectively, the population covariance between $z$ and $y$, and $z$ and $T$. How can you use this information to obtain a consistent estimate of $\beta$?}
\begin{description}
\item[Answer:]
Instrument $z$ needs to satisfy the following conditions:
\begin{itemize}
\item \textit{Instrument relevance}: $z$ must have non-trivial explanatory power for $T$, namely $Cov(z,T)\neq 0$.
\item \textit{Instrument exogeneity}: $z$ must affect $Y$ only through its influence on $T$ and not in any other way. That is, $z$ must be exogenous with respect to $u$ in regression $y = \alpha + \beta T + u$. Formally, $\mathbb{E}(u|z)=0$. This is why it is said "z is exogenous in $y = \alpha + \beta T + u$. Exogeneity of instrument $z$ implies that $Cov(z, u)=0$.
\end{itemize}

In the context of omitted variables, instrument exogeneity means that $z$ should be uncorrelated with the omitted variables, i.e. $Cov(z,u)=0$, and $z$ should be related, positively or negatively, to the endogeneous explanatory variable $T$, i.e. $Cov(z,T)\neq 0$.\footnote{see Section 15-1: Omitted Variables in a Simple Regression Model in Wooldridge J M (2021) Introductory Econometrics: A Modern Approach, $7^{th}$ ed, Cengage}

The underlying reasoning is that if an instrument is relevant, then variation in that instrument $z$ is related to variation in $T$, and if it is also exogeneous, then that part of the variation of $T$ captured by $z$ is exogeneous. Therefore, an instrument that is relevant and exogeneous can capture movements in $T$ that are exogeneous. This exogeneous variation can in turn be used to estimate the population coefficient $\beta$.\footnote{see Section 12.1: The IV Estimator with a Single Regressor and a Single Instrument in Stock and Watson (2020, $4^{th}$ ed.).}  

These conditions serve to \textit{identify} the parameter $\beta$. In this context, \textit{identification of a parameter} means that we can write $\beta$ in terms of population moments that can be estimated using a sample of data. 

To write $\beta$ in terms of population covariances we use $y = \alpha + \beta T + u$:
\[
Cov(z,y) = Cov(z, \ \alpha+\beta T + u) = \beta Cov(z,T) + Cov(z,u)
\]

Since instrument exogeneity condition assumes that $Cov(z,u)=0$ then $Cov(z,y)=\beta Cov(z,T)$. Rearranging this gives:
\[
\beta=\frac{Cov(z,y)}{Cov(z,T)}
\]
as desired. Notice that this only holds if instrument relevance also holds, since this expression would fail if $Cov(z,T)=0$. What this expression is telling us is that $\beta$ is identified by the ratio of population covariance between $z$ and $y$ to population covariance between $z$ and $T$. 

Given a random sample, we estimate the population quantities by the sample analogs:
\[
\hat{\beta}^{IV} = \frac{\displaystyle\frac{1}{n}\displaystyle\sum_{i=1}^n(z_i-\bar{z})(y_i-\bar{y})}{\displaystyle\frac{1}{n}\displaystyle\sum_{i=1}^n(z_i-\bar{z})(T_i-\bar{T})} = \frac{\displaystyle\sum_{i=1}^n(z_i-\bar{z})(y_i-\bar{y})}{\displaystyle\sum_{i=1}^n(z_i-\bar{z})(T_i-\bar{T})}.
\]
With a sample data on $T, y,$ and $z$ we can obtain the IV estimator above. The IV estimator for the intercept $\alpha$ is $\alpha = \bar{y} - \hat{\beta}^{IV}\bar{T}$. Also notice that when $z=T$, we get the OLS estimator of $\beta$. That is, when $T$ is exogeneous, it can used as its own IV, and the IV estimator is then identical to the OLS estimator.

A similar set of steps we used in part (c) will show that IV estimator is consistent for $\beta$. That is, $\underset{n\to \infty}{plim}(\hat{\beta}) = \beta$.

Note that, an important feature of IV estimator is that when $T$ and $u$ are in fact correlated, and thus instrumental variables estimation is actually needed, it is essentially \underline{never unbiased}. This means, in small samples, the IV estimator can have a substantial bias, which is one reason why large samples are preferred. 
\end{description}

\bigskip\bigskip
***
\bigskip\bigskip

### (e) Can you give an example of an instrument in your example? Argue why it might be a sensible IV.
\begin{description}
\item[Answer:]
Distance from nearest college can be an example of an instrument, where $z=1$ if individual lived near college and $0$ otherwise. This may be violated for a number of reasons, though; for e.g. if wealthy parents choose to live near college. This would mean that $z$ is correlated with unobserved factors that also affect wage, our $y$. For any example, exogeneity and relevance conditions need to be checked.
\end{description}




\bigskip\bigskip\bigskip
\hdashrule[0.5ex]{\textwidth}{1pt}{3mm}
\bigskip\bigskip\bigskip


## QUESTION 2

\textbf{Consider the following wage equation that explicitly recognizes that ability affects $log(wage)$}
\[
log(wage) = \alpha + \beta_1 educ + \beta_2 ability + u
\]
\textbf{The above model shows explicitly that we would like to hold ability fixed when measuring the returns on education. Assuming that the primary interest is in obtaining a reliable estimate of the slope parameters $\beta_1$, and that there is no direct measurement for ability, explain how you would do this using a method based upon a proxy variable and an IV estimator. In doing so evaluate the following statement:}

"\textbf{\textit{whilst $IQ$ is a good candidate as a proxy for variable for ability, it is not a good instrumental variable for $educ$.}}"

\begin{description}
\item[Answer:] 
This question is essentially aiming to ensure the students understand the difference between proxy variable and instrumental variable.
\begin{itemize}
\item[proxy variable] refers to an \textit{observed} variable that is correlated with but not identical to the \textit{unobserved} variable.
\item[instrumental variable] refers to a variable that does not appear in the regression, uncorrelated with the error in the equation, and partially correlated with the endogenous explanatory variable in an equation where such endogenous explanatory variable exists.
\end{itemize}


\underline{Proxy Variable:}

Notice in this question $educ$ is observed but $ability$ is unobserved, and we would not even know how to interpret it's coefficient $\beta_2$ since 'ability' itself is a vague concept. We can instead use intelligence quotient, or $IQ$, as a proxy for ability as long as $IQ$ is correlated with ability. This is captured by the following simple regression:
\[
ability = \delta_0 + \delta_2IQ + v_2
\]
where $v_2$ is an error due to the fact that $ability$ and $IQ$ are not exactly related. The parameter $\delta_2$ measures the relationship between $ability$ and $IQ$. If $\delta_2=0$ then $IQ$ is not a suitable proxy for $ability$.

Note that the intercept $\delta_0$ allows $ability$ and $IQ$ to be measured on different scales and thus can be positive or negative. That is, the unobserved $ability$ is not required to have the same average value as $IQ$ in the population.

In order to use $IQ$ to get unbiased, or at least consistent, estimators for $\beta_1$, which is the coefficient of $educ$, we would regress $log(wage)$ on $educ$ and $IQ$. This is called \textit{the plug-in solution to the omitted variables problem} since we plug-in $IQ$ for $ability$ before running the OLS. However, since $IQ$ and $educ$ are not the same, we need to check if this does give consistent estimator for $\beta_1$. 

For the plug-in solution to provide consistent estimator for $\beta_1$ the following two assumptions need to hold true:
\begin{itemize}
\item The error $u$ is uncorrelated with $educ$ and $ability$ as well as $IQ$. That is, $\mathbb{E}(u|educ,ability,IQ)=0$. What this means is that $IQ$ is irrelevant in the population model which is true by definition since $IQ$ is a proxy for $ability$, it is $ability$ that directly affects $log(wage)$ not $IQ$.
\item The error $v_2$ is uncorrelated with $educ$ and $IQ$. For $v_2$ to be a uncorrelated with $educ$, $IQ$ needs to be a 'good' proxy for $ability$. 
\end{itemize}

What is meant by 'good' proxy in this sense is that
\[
\mathbb{E}(ability\ |\ educ,IQ) = \mathbb{E}(ability\ |\ IQ) = \delta_0 + \delta_2IQ.
\]
Here, the first equality, which is the most important one, says that once $IQ$ is controlled for, the expected value of $ability$ does not depend on $educ$. In other words, $ability$ has zero correlation with $educ$ once $IQ$ is partialled out. Thus the average level of $ability$ only changes with $IQ$ and not with $educ$.

To see why these two assumptions are enough for the plug-in solution to work, we can rewrite the $log(wage)$ equation in the question as:
\begin{align*}
log(wage) 
  &= \alpha + \beta_1 educ + \beta_2 ability + u \\
  &= \alpha + \beta_1 educ + \beta_2 (\delta_0 + \delta_2IQ + v_2) + u \\
  &= (\alpha + \beta_2\delta_0) + \beta_1 educ + \beta_2\delta_2IQ + u + \beta_2v_2 \\
  &= (\alpha + \beta_2\delta_0) + \beta_1 educ + \beta_2\delta_2IQ + \epsilon \\
  &= \gamma_0 + \beta_1 educ + \gamma_2 IQ + \epsilon.
\end{align*}

Notice that the composite error $\epsilon$ depends on both the error in the model of interest in the question, $u$, and on the error in the proxy variable equation, $v_2$. Since both $u$ and $v_2$ have zero mean and each is uncorralated with $educ$ and $IQ$, $\epsilon$ also has zero mean and is uncorrelated with $educ$ and $IQ$.

So when we regress $log(wage)$ on $educ$ and $IQ$, we will \underline{not} get unbiased estimators of $\alpha$ and $\beta_2$. Instead, we will get unbiased, or at least consistent, estimators of $\gamma_0, \beta_1,$ and $\gamma_2$. The important thing is that we get good estimators of $\beta_1$. 

In most cases, the estimate of $\gamma_2$ is more interesting than an estimate of $\beta_2$ anyway, since $\gamma_2$ measures the return to $log(wage)$ given one more point on $IQ$ score.


\begin{tcolorbox}[breakable, title=Bias and Multicollinearity when using a proxy, skin=enhancedlast]
\textbf{When using a proxy variable can still lead to bias?}

If the two assumptions above are not satisfied, then using a proxy variable can lead to a bias. To see this, suppose now that ability is not only related to $IQ$ but also to $educ$:
\[
ability = \delta_0 + \delta_1 educ + \delta_2 IQ + v_3
\]
where the error $v_3$ has a zero mean and uncorrelated with $educ$ and $IQ$. In the proxy variable discussion above, it was essentially assumed that $\delta_1=0$. We can re-write $log(wage)$ with this plug-in solution:
\begin{align*}
log(wage) 
  &= \alpha + \beta_1 educ + \beta_2 ability + u \\
  &= \alpha + \beta_1 educ + \beta_2 (\delta_0 + \delta_1 educ + \delta_2 IQ + v_3) + u \\
  &= (\alpha + \beta_2\delta_0) + (\beta_1 + \beta_2\delta_1) educ + \beta_2\delta_2 IQ + u + \beta_2v_3 \\
\end{align*}
Since the error term $u+\beta_2v_3$ has zero mean and is uncorrelated with $educ$ and $IQ$, we have $plim(\hat{\beta}_1)=\beta_1 + \beta_2\delta_1$. If $educ$ is partially and positively correlated with $ability$, i.e. $\delta_1 > 0$, and if the coefficient of $ability$ is positively correlated with $log(wage$, i.e. $\beta_2 > 0$, then $plim(\hat{\beta}_1)=\beta_1 + \beta_2\delta_1 > \beta_1$ giving us an upward bias. That is, in this case where $IQ$ is not a good proxy for $ability$ but we still use it, then we'd still be getting an upward bias for the coefficient of $educ$.

Having said that, the bias is likely to be smaller than if we ignored the problem of omitted ability entirely.

\bigskip

\textbf{What about multicollinearity?}

Even if $IQ$ is a good proxy for $ability$, using it in a regression that includes $educ$ can exacerbate the multicollinearity problem, which, in turn, is likely to lead a less precise estimate of the coefficient for $educ$, i.e. $\beta_1$.

However, notice that
\begin{itemize}
\item inclusion of $IQ$ in the regression means that the part of $ability$ explained by $IQ$ is removed from the error term, reducing the error variance. This is likely to be reflected in a smaller standard error of the regression, though that reduction may not happen because of degrees of freedom adjustment.

\item if we want to have a less bias for $\beta_1$, ie, the estimator of the coefficient for $educ$, then we have to live with increased multicollinearity. This is an important point. Since $educ$ and $ability$ are thought to be correlated, and if we could include $ability$ in the regression, then there would be inevitable multicollinearity caused by the correlation between these two variables. Since $IQ$ is a proxy for $ability$, $educ$ and $IQ$ are also correlated, and a similar argument ensues.
\end{itemize}
\end{tcolorbox}

\bigskip

\underline{Instrumental Variable}

Suppose now that the proxy variable does not have the required properties for a consistent estimator of $\beta_1$. Then we put $ability$ in the error term since it is unobserved and we don't have a proxy for it. This leaves us with:
\[
log(wage) = \beta_0 + \beta_1 educ + \epsilon
\]
where $\epsilon$ contains $ability$. If $ability$ and $educ$ are correlated, then we have a biased and inconsistent estimate of $\beta_1$.

However, we can still use this equation as the basis for estimation as long as we can find an instrumental variable for $educ$. For this we can introduce an \textit{instrumental variable} $z$ which satisfies the "instrument relevance", i.e. $Cov(z, educ)\neq 0$, and "instrument exogeneity", i.e. $Cov(z,\epsilon)=0$ conditions as discussed in Question 1(d).

Note that we cannot really test for "instrument exogeneity" assumption and need to consider economic behavior in order to maintain the $Cov(z, \epsilon)=0$ assumption. At times, there may be an observable proxy for some factor contained in $\epsilon$ and we can check if $z$ and the proxy variable are more or less uncorrelated. And, of course, as discussed above, if we have a good proxy then we would add that variable to the equation and estimate the expanded form by OLS. 

This is exactly where we see a tension between a good proxy vs a good instrumental variable. For $IQ$ to be a good proxy, it needs to be as highly correlated with $ability$ as possible. Yet for $IQ$ to be a good instrumental variable, it needs to be uncorrelated with $ability$ since $ability$ is contained in $\epsilon$ and a good instrumental variable should not covary with the error term. That is, a good instrumental variable should affect $log(wage)$ only through its influence on $educ$ and not in any other way.

Thus, in this question, although $IQ$ is a good candidate as a proxy variable for $ability$, it is not a good instrumental variable for $educ$.

\end{description}








\bigskip\bigskip\bigskip
\hdashrule[0.5ex]{\textwidth}{1pt}{3mm}
\bigskip\bigskip\bigskip


## QUESTION 3
\textbf{The following regression explores the relationship between television watching and childhood obesity, using a cross-section of US children. The variables are:}

\begin{center}
\begin{tabular}{llccc}
Name   & Description & Minimum & Maximum & Mean \\ 
\midrule
tvyest & hours of TV watched yesterday & 0  & 6  & 3.14 \\
black  & dummy, 1 if black             & 0  & 1  & 0.31 \\
hisp   & dummy, 1 if hispanic          & 0  & 1  & 0.36 \\
ageyrs & age in years                  & 5  & 16 & 9.4  \\
bmi    & child's Body Mass Index       & 11 & 55 & 19   \\
dadbmi & father's BMI                  & 11 & 58 & 26   \\
mombmi & mother's BMI                  & 14 & 56 & 26   \\
\midrule
\end{tabular}
\end{center}

```{r include=FALSE}
libraries <- c("haven",       # to import/export SPSS, STATA, SAS files
               "readxl",      # to import/export Excel files   
               "tidyverse",   # for tidy data
               "Statamarkdown", # for using STATA commands in R
               "ivreg", #for regressions with instrumental variables
               "mnormt", #for creating bivariate normal distributions
               "kableExtra", # for creating nice tables in R
               "rstatix")     # converts stats functions to a tidyverse-friendly format

invisible(lapply(libraries, library, character.only=TRUE))  # will load the libraries

```

\bigskip

The output from a 2SLS regression appears below:


```{stata echo=FALSE}
* Change the working directory to access the data file:
    quietly cd .. 
* Load the data:
    use Data/obesekids.dta

* Run the instrumental variables regression
    ivregress 2sls tvyest black hisp ageyrs (bmi = dadbmi mombmi)

```

\textbf{Now answer the following questions.}

### (a) Why might an OLS regression of $tvyest$ on the child's BMI give us inconsistent estimates of the causal effect of BMI on TV watching?

\begin{description}
\item[Answer:]
Recall that correlation between the error term and any of the regressors generally causes all of the OLS estimators to be inconsistent. In fact, if the error term is correlated with any of the independent variables, then OLS is both biased and inconsistent. This means any bias persists even as the sample size grows.

Here, if we only regress $tvyest$ on $bmi$ then inevitably all the omitted variables would be contained in the error term and they would be correlated with $bmi$, which would give us inconsistent estimates of the causal effect of $bmi$ on tv watching.
\end{description}


\bigskip\bigskip
***
\bigskip\bigskip

### (b) Interpret the coefficient $0.73$ on $black$.
\begin{description}
\item[Answer:]
The coefficient implies that holding other variables constant, black children watched on average about 0.73 hours more tv than non-black children.
\end{description}


\bigskip\bigskip
***
\bigskip\bigskip

### (c) Can you state a reason why we may doubt the validity of the 2SLS estimates reported above?
\begin{description}
\item[Answer:]
In the least, the 2SLS estimation method have the following assumptions:

\begin{itemize}
\item the error term of the structural equation is uncorrelated with each of the exogenous explanatory variables
\item there exists at least one exogenous variable that is partially correlated with the endogenous variable in the structural equation but itself is not in the structural equation to ensure consistency
\item the structural error term cannot depend on any of the exogeneous variables, i.e. homoskedasticity assumption. This ensures the 2SLS standard errors and $t$-statistics to be asymptotically valid. 
\end{itemize}

\item Violation of any one of these assumptions would make us doubt the validity of the 2SLS estimates reported above.
\end{description}















































\pagebreak

# SUPPLEMENTARY QUESTIONS

\bigskip\bigskip

## QUESTION 1

\textbf{Consider the simple regression model:}
\begin{equation} \label{eq:SQ1}
Y_i = \alpha + \beta X_i + \varepsilon_i, \ \ i = 1,2,\dots,m
\end{equation}
\textbf{where $Y_i$ is the mean expenditure on alcohol in group $i$ and $X_i$ is the mean income of group $i$. Each group $i$ has $N_i$ members and the model satisfies all the classical assumptions except that the variance of $\varepsilon_i$ is equal to $\sigma^2/N_i$.}


### (a) What are the statistical properties of the OLS estimates of $\alpha$ and $\beta$ in this case?

\begin{description}
\item[Answer:] 
Recall that when demonstrating unbiasedness and consistency of OLS estimators, homoskedasticity assumption did not play any role. That is, if the variance of the unobserved error is not constant, i.e. heteroskedastic, it does not impact whether an estimator is unbiased or consistent. Similarly, the interpretation of the goodness-of-fit measures, $R^2$ and $\bar{R}^2$, are also unaffected by the presence of heteroskedasticity.

The problem with the presence of heteroskedasticity is that the estimators of the variances are biased. Since the OLS standard errors are based on these variances, they are no longer valid for constructing confidence intervals and $t$-statistics. In this situation the OLS $t$-statistics do not have $t$ distributions and the problem is not resolved by increasing the sample size. Similarly, $F$-statistics are not longer $F$-distributed. Finally, the OLS is no longer BLUE as it is no longer asymptotically efficient.

Recall that the OLS estimator is
\[
\hat{\beta} = \beta + \frac{\displaystyle\sum_{i=1}^m (X_i - \bar{X})\varepsilon_i}{\displaystyle\sum_{i=1}^m (X_i - \bar{X})^2} = \beta + \frac{\displaystyle\sum_{i=1}^m (X_i - \bar{X})\varepsilon_i}{{SST}_X^2} 
\]

and its variance when homoskedasticity is present is
\begin{align*}
Var(\hat{\beta})  
  &= Var \left( \beta + \frac{\displaystyle\sum_{i=1}^m (X_i - \bar{X})\varepsilon_i}{\displaystyle\sum_{i=1}^m (X_i - \bar{X})^2} \right) \\[6pt] 
  &= Var \left(\frac{\displaystyle\sum_{i=1}^m (X_i - \bar{X})\varepsilon_i}{\displaystyle\sum_{i=1}^m (X_i - \bar{X})^2} \right) \ \ \ \ \text{since $\beta$ is constant} \\[6pt] 
  &= \left(\frac{1}{\displaystyle\sum_{i=1}^m (X_i - \bar{X})^2} \right)^2 \ Var\Big( \displaystyle\sum_{i=1}^m (X_i - \bar{X})\varepsilon_i \Big)  \ \ \ \ \text{since we are conditioning on $X_i$, $SST_X$ is nonrandom}   \\[6pt]
  &= \Big( \frac{1}{SST_X} \Big)^2 \Big( \displaystyle\sum_{i=1}^m (X_i - \bar{X})^2 \ Var(\varepsilon_i) \Big) \ \ \ \ \text{since we are conditioning on $X_i$, $X_i - \bar{X}$ is nonrandom} \\[6pt]
  &= \Big( \frac{1}{SST_X} \Big)^2 \Big( \displaystyle\sum_{i=1}^m (X_i - \bar{X})^2\sigma_\varepsilon^2 \Big)  \ \ \ \ \text{since $Var(\varepsilon_i) = \sigma^2$ for all $i$ when homoskedastic} \\[6pt]
  &= \sigma_\varepsilon^2 \Big( \frac{1}{SST_X}\Big)^2 SST_X \\[6pt]
  &= \frac{\sigma_\varepsilon^2}{SST_X} 
  = \frac{\sigma_\varepsilon^2}{\displaystyle\sum_{i=1}^m(X_i - \bar{X})^2}
\end{align*}

and its variance when heteroskedasticity is present is
\[
Var(\hat{\beta}) 
  = \Big( \frac{1}{SST_X} \Big)^2 \Big( \displaystyle\sum_{i=1}^m (X_i - \bar{X})^2\sigma_i^2 \Big) 
  = \frac{\displaystyle\sum_{i=1}^m (X_i - \bar{X})^2\sigma_i^2}{\Big( \displaystyle\sum_{i=1}^m(X_i - \bar{X})^2 \Big)^2}.
\]

\underline{Spherical Errors}

We assume homoskedasticity and no autocorrelation in estimating the variance of OLS estimates. That is, we assume that all errors have the same variance $\sigma^2$ and that there is no correlation across errors. If these hold true, then we have \textit{spherical errors}, or that the error term follows a \textit{spherical distribution}. This is represented in matrix form as follows:
\begin{equation*}
\mathbb{E}(\vec{u}\vec{u}^T | \mathbf{X}) = 
\begin{bmatrix}
\sigma^2 & 0 & \dots & 0 \\
0 & \sigma^2 & \dots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \dots & \sigma^2
\end{bmatrix}
= \sigma^2 \mathbf{I}
\end{equation*}

To see why this is called a spherical distribution lets look at a special case of two dimensions, i.e. circular distribution, as opposed to three dimensions for spherical distribution. Consider two random errors, $u_i$ and $u_j$ which are graphed below as density plots and contour plots, the latter of which shows what you'd see when you look straight down from the top of the density plot.

The shapes of these plots depend on the variances and covariances of these two random errors. If $u_i$ and $u_j$ are homoskedastic and they are not correlated, then the contour lines will be circles. If there were three random error variables $u_i$, $u_j$, and $u_k$ then we would have four-dimensional density plot and the contours would form a sphere. If there were more than three random error variables then the contours would form a hyper-sphere. This is why the errors are spherically distributed.

What we are plotting is therefore:
\begin{equation*}
\mathbb{E}
\begin{pmatrix}
u_i \\
u_j
\end{pmatrix}
=
\begin{pmatrix}
0 \\
0
\end{pmatrix}
\ \ \ \ ; \ \ \ \
Var
\begin{pmatrix}
u_i \\
u_j
\end{pmatrix}
=
\begin{pmatrix}
1 & 0 \\
0 & 1
\end{pmatrix}
\end{equation*}

where errors are homoskedastic and there is no autocorrelation.

\end{description}


```{r echo=FALSE}
#build bivariate normal distribution
x <- seq(-3,3,0.25)
y <- seq(-3,3,0.25)
mu <- c(0,0)
sigma <- matrix(c(1,0,0,1), nrow = 2)
f <- function(x,y) dmnorm(cbind(x,y),mean = mu, varcov = sigma)
z <- outer(x,y,f)

#plot side by side
par(mfrow = c(1,2))
#density plot
persp(x,y,z, main="Density",  xlab = expression(error[i]), ylab=expression(error[j]), zlab=expression('density'))
#contour plot
contour(x,y,z, main="Contour", axes=TRUE, xlab=expression(error[i]),ylab=expression(error[j]))


```


If on the other hand, heteroskedasticity is present then we loose the symmetry of the joint density plot and get a more elliptic contours. Suppose now the variance-covariance matrix is as follows:
\begin{equation*}
Var
\begin{pmatrix}
u_i \\
u_j
\end{pmatrix}
=
\begin{pmatrix}
0.25 & 0 \\
0 & 2
\end{pmatrix}
\end{equation*}

```{r echo=FALSE}
#build bivariate normal distribution
x <- seq(-3,3,0.25)
y <- seq(-3,3,0.25)
mu <- c(0,0)
sigma <- matrix(c(0.25,0,0,2), nrow = 2)
f <- function(x,y) dmnorm(cbind(x,y),mean = mu, varcov = sigma)
z <- outer(x,y,f)

#plot side by side
par(mfrow = c(1,2))
#density plot
persp(x,y,z, main="Density",  xlab = expression(error[i]), ylab=expression(error[j]), zlab=expression('density'))
#contour plot
contour(x,y,z, main="Contour", axes=TRUE, xlab=expression(error[i]),ylab=expression(error[j]))

?contour
```

Similarly, we would also get an elliptic contours if the errors are homoskedastic but there is autocorrelation. The slope of the main axis of the ellipse would depend on the sign of the correlation between the errors. Suppose now the variance-covariance matrix is as follows:
\begin{equation*}
Var
\begin{pmatrix}
u_i \\
u_j
\end{pmatrix}
=
\begin{pmatrix}
1 & -0.5 \\
-0.5 & 1
\end{pmatrix}
\end{equation*}

```{r echo=FALSE}
#build bivariate normal distribution
x <- seq(-3,3,0.25)
y <- seq(-3,3,0.25)
mu <- c(0,0)
sigma <- matrix(c(1,-0.5,-0.5,1), nrow = 2)
f <- function(x,y) dmnorm(cbind(x,y),mean = mu, varcov = sigma)
z <- outer(x,y,f)

#plot side by side
par(mfrow = c(1,2))
#density plot
persp(x,y,z, main="Density",  xlab = expression(error[i]), ylab=expression(error[j]), zlab=expression('density'))
#contour plot
contour(x,y,z, main="Contour", axes=TRUE, xlab=expression(error[i]),ylab=expression(error[j]))


```



\bigskip\bigskip
***
\bigskip\bigskip


### (b) How should equation (\ref{eq:SQ1}) on page \pageref{eq:SQ1} be transformed so that the OLS estimates of $\alpha$ and $\beta$ are BLUE?
\begin{description}
\item[Answer:] The variances of the error terms are given in the question, thus \textit{known}. We can therefore estimate using the generalized least squares (GLS) estimators for correcting heteroskedasticity where we minimize a \textit{weighted sum of squared residuals.}

$\hookrightarrow$ For remedial measures when $\sigma_i^2$ is unknown, see Question 2 below.

 
\[
Var(\varepsilon_i) = \frac{\sigma^2}{N_i} = \sigma_i^2
\]
So we transform equation (\ref{eq:SQ1}) on page \pageref{eq:SQ1} by dividing it with theses known standard deviations, $\sigma_i$:
\[
\frac{Y_i}{\sigma_i} = \frac{\alpha}{\sigma_i} + \beta \frac{X_i}{\sigma_i} + \frac{\varepsilon_i}{\sigma_i}
\]
so that
\begin{align*}
Var(\frac{\varepsilon_i}{\sigma_i}) 
  &= \mathbb{E}\Bigg[ \bigg( \frac{\varepsilon_i}{\sigma_i} \bigg)^2 \Bigg] - \Bigg[ \mathbb{E}\bigg( \frac{\varepsilon_i}{\sigma_i} \bigg) \Bigg]^2 \\[6pt]
  &= \mathbb{E}\Bigg[ \bigg( \frac{\varepsilon_i}{\sigma_i} \bigg)^2 \Bigg] \ \ \ \ \text{since } \mathbb{E}\bigg( \frac{\varepsilon_i}{\sigma_i} \bigg) = 0 \\[6pt]
  &= \frac{1}{\sigma_i^2}\mathbb{E}(\varepsilon_i^2) \ \ \ \ \text{since $\sigma_i^2$ is known, thus it is a collection of constants} \\[6pt] 
  &= \frac{1}{\sigma_i^2}\sigma_i^2 = 1
\end{align*}
which is a constant. This means, the variance of the transformed disturbance term $\frac{\varepsilon_i}{\sigma_i}$ is now homoskedastic. Since all the other assumptions of classical model still hold true, this means that if we apply OLS method to the transformed model, we will get estimators that are BLUE.

Thus, GLS is OLS on the transformed variables that satisfy the standard least-squares assumptions. The estimators that are obtained these way are GLS estimators which are BLUE.
\end{description}




\bigskip\bigskip
***
\bigskip\bigskip

### (c) Derive $\hat{\alpha}$ in terms of $\hat{\beta}$ in this case.

\begin{description}
\item[Answer:]
In this case, what we want is a transformation of the equation \ref{eq:SQ1} on page \pageref{eq:SQ1} in such a way that the variance of the transformed error, $Var(\varepsilon_i*)$, is constant $\sigma^2$. 

For this, we can work backwards. We know that $Var(\varepsilon_i) = \mathbb{E}(\varepsilon_i^2) = \frac{\sigma^2}{N_i}$ so if the transformation resulted in $Var(\varepsilon_i^*)=N_i\mathbb{E}(\varepsilon_i^2)$ then it would equal to constant $\sigma^2$. For that to happen, we can set $\varepsilon_i^* = \varepsilon_i\sqrt{N_i}$, so that
\[
Var(\varepsilon_i^*) = \mathbb{E}\big((\varepsilon_i^*)^2\big) - \big[\mathbb{E}(\varepsilon_i^*)\big]^2 = \mathbb{E}\big((\varepsilon_i^*)^2\big) = \mathbb{E}\Big((\varepsilon_i\sqrt{N_i})^2\Big) = N_i\mathbb{E}(\varepsilon_i^2) = N_i\frac{\sigma^2}{N_i} = \sigma^2
\]
as desired.

Thus using the weighting of $\sqrt{N_i}$ the sample regression function becomes:
\begin{align*}
Y_i\sqrt{N_i} &= \alpha\sqrt{N_i} + \beta \sqrt{N_i}X_i + \varepsilon_i\sqrt{N_i} \\
Y_i^* &= \alpha^* + \beta^* X_i + \varepsilon^*
\end{align*}
In general, to obtain the estimators for the coefficients, the weighted least-squares method minimizes the weighted residual sum of squares:
\[
\sum w_i\hat{\varepsilon}_i^2 = \sum w_i(Y_i - \hat{\alpha}^* - \hat{\beta}^* X_i)^2
\]
where $\alpha^*$ and $\beta^*$ are the weighted least squares estimators. Differentiating these with respect to $\hat{\alpha}^*$ and $\hat{\beta}^*$ gives us:
\begin{align*}
\frac{\partial}{\partial\hat{\alpha}^*}\sum w_i\hat{\varepsilon}_i^2 &= 2 \sum w_i(Y_i - \hat{\alpha}^* - \hat{\beta}^* X_i)(-1) \\[6pt]
\frac{\partial}{\partial\hat{\beta}^*}\sum w_i\hat{\varepsilon}_i^2 &= 2 \sum w_i(Y_i - \hat{\alpha}^* - \hat{\beta}^* X_i)(-X_i)
\end{align*}
Setting these equal to $0$ gives us:
\begin{align*}
\sum w_iY_i &= \hat{\alpha}^*\sum w_i + \hat{\beta}^* \sum w_iX_i \\
\sum w_iX_iY_i &= \hat{\alpha}^*\sum w_iX_i + \hat{\beta}^*\sum w_iX_i^2
\end{align*}
Solving these simultaneously, we get:
\begin{align*}
\hat{\alpha}^* 
  &= \frac{\sum w_iY_i}{\sum w_i} - \hat{\beta}^* \frac{\sum w_iX_i}{\sum w_i} \\[6pt]
  &= \bar{Y}^* - \hat{\beta}^*\bar{X}^* \\[18pt]
\hat{\beta}^*
  &= \frac{(\sum w_i)(\sum w_iX_iY_i) - (\sum w_iX_i)(\sum w_iY_i)}{(\sum w_i)(\sum w_iX_i^2) - (\sum w_iX_i)^2}
\end{align*}
Notice that in this question $w_i = N_i$ and not $\sqrt{N_i}$.  
\end{description}










\bigskip\bigskip\bigskip
\hdashrule[0.5ex]{\textwidth}{1pt}{3mm}
\bigskip\bigskip\bigskip

## QUESTION 2
\textbf{Using the Heteroskedasticity worksheet in sup4.xls}

Load the data in R:
```{r, eval=FALSE}
property_df <- read_excel("../Data/sup4.xls")

# You can use any of the following to examine data frame (df): 
# `dim()`: for its dimensions, by row and column
# `str()`: for its structure
# `summary()`: for summary statistics on its columns
# `colnames()`: for the name of each column
# `head()`: for the first 6 rows of the data frame
# `tail()`: for the last 6 rows of the data frame
# `View()`: for a spreadsheet-like display of the entire data frame
```


\bigskip\bigskip


### (a) Estimate the following and comment on your results:
\begin{equation} \label{eq:SQ2a}
PRICE_t = \beta_0 + \beta_1LOTSIZE_t + \beta_2SQRFT_t + \beta_3BDRMS_t + u_t
\end{equation}

In R run the following:
```{r, eval=FALSE}
SQ2a_lm <- lm(PRICE ~ BDRMS + LOTSIZE + SQRFT, data = property_df)
print(summary(SQ2a_lm), digits=7)
```

and in STATA run the following:
```{stata}
/* load the data */
quietly cd ..
quietly import excel using Data/sup4.xls, sheet("heteroscedasticity") firstrow

/* `firstrow` indicates that the first row contains the variable names */
/* `describe` command would give basic information about the data set */

/* run the regression */
regress PRICE LOTSIZE SQRFT BDRMS
```

We see that the $F$-stat is high at $57.46$ with its $p$ value being $0$. We also see that both $LOTSIZE$ and $SQRFT$ are significant with $t$-values $3.22$ and $9.28$ with near $0$, or $0$, $p$-values, respectively. On the other hand, $BDRMS$ look insignificant with $t$-value at $1.54$, though it may perhaps be due to multicollinearity.

To check for heteroskedasticity, usually the first thing to do is to plot the residuals against the estimated values of the independent variable as an amalgamation of all the dependent variables.

In R we do this with the following:
```{r eval=FALSE}
# the following will provide four important plots that are usually needed
# since there are four graphs, we want to display in 2x2 format first then plot
par(mfrow = c(2,2))
plot(SQ2a_lm)

# if it is only the residuals vs fitted that we are interested, then
plot(SQ2a_lm, which=1)
# or
plot(fitted(SQ2a_lm), resid(SQ2a_lm))
# we can also add a horizontal line at 0
abline(0,0)

# to make this look nicer, we can  also use `autoplot` command from `ggfortify` library
library(ggfortify)
autoplot(SQ2a_lm)
```

In STATA we instead use the following:
```{stata, eval=FALSE}
/* plot residuals against fitted values */
rvfplot, yline(0)
```

In either case we get the following plot:

```{r, echo=FALSE}
property_df <- read_excel("../Data/sup4.xls")
SQ2a_lm <- lm(PRICE ~ BDRMS + LOTSIZE + SQRFT, data = property_df)
plot(SQ2a_lm, which=1)
abline(0,0)
```

There seems to be a downward trend which can suggest heteroskedasticity but it is difficult to tell, as it could be due to outliers.



\bigskip\bigskip
***
\bigskip\bigskip

### (b) Calculate robust standard errors for the equation \ref{eq:SQ2a} specified on page \pageref{eq:SQ2a} and compare your results.

\begin{description}
\item[Answer:]
White (1980)\footnote{White, H (1980) "A Heteroscedasticity Consistent Covariance Matrix Estimator and a Direct Test of Heteroscedasticity", \textit{Econometrica}, 48:817-828. Though the possibility of such heterskedasticity-robust standard errors were previously discussed by Eicker (1967) and Huber (1967) and so sometimes these are also called \textit{White-Huber-Eicker standard errors.} See Eicker, F (1967) "Limit Theorems for Regressions with Unequal and Dependent Errors", \textit{Proceedings of the Fifth Berkeley Symposium on Mathematical Statistics nd Probability} 1:59-82, and Huber, P J (1967) "The Behavior of Maximum Likelihood Estimates under Nonstandard Conditions",\textit{Proceedings of the Fifth Berkeley Symposium on Mathematical Statistics nd Probability} 1:221-233.} has shown that asymptotically consistent estimates of variances and covariances of OLS estimators can be obtained even if there is heteroskedasticity present so that asymptotically valid statistical inferences can be made about the true paramter values. White's heteroskedasticity-corrected standard errors are also known as \textit{robust standard errors}. 

\begin{tcolorbox}[breakable, title=White's robust standard errors, skin=enhancedlast]

\textbf{How do we get heteroskedasticity-consistent variances and standard errors?}\footnote{Gujarati and Porter (2009), Appendix 11A.4; Wooldridge (2021), Section 8.2}

Consider the simple regression model:
\[
Y_i = \beta_0 + \beta_1 X_1 + u_i
\]
where $Var(u_i)=\sigma_i$; that is, it is heteroskedastic. In Question 1 part (a) we have shown that 
\begin{equation}\label{eq:WhiteVar1}
Var(\hat{\beta_1}) = \frac{\displaystyle\sum_{i=1}^n (X_i - \bar{X})^2\sigma_i^2}{\Big( \displaystyle\sum_{i=1}^n(X_i - \bar{X})^2 \Big)^2}
\end{equation}
Since $\sigma_i^2$ are not directly observable, White argues for using the squared residual of each $i$, $\hat{u}_i^2$, instead and estimating the variance of the estimator via:
\begin{equation}\label{eq:WhiteVar2}
Var(\hat{\beta}_1) = \frac{\displaystyle\sum_{i=1}^n (X_i - \bar{X})^2\hat{u}_i^2}{\Big( \displaystyle\sum_{i=1}^n(X_i - \bar{X})^2 \Big)^2}
\end{equation}
White has shown that when this equation \ref{eq:WhiteVar2} is multiplied by the sample size $n$, it converges in probability to $\frac{\mathbb{E}[(X_i - \mu_x)^2u_i^2]}{(\sigma_X^2)^2}$ which is the probability limit of equation \ref{eq:WhiteVar1} multiplied by $n$, and where $\mu_x$ is the expected value of $X$, and $\sigma_X^2$ is the population variance of $X$. Thus, the law of large numbers and the central limit theorem are key in establishing these convergences, which are necessary for justifying the use of standard errors to construct confidence intervals and $t$-statistics.

$\hookrightarrow$ One can first obtain the residuals from the usual OLS regression and then calculate the variance using equation \ref{eq:WhiteVar2}. Statistical software do this automatically.

This can be extended to $k$-variable regression model
\[
Y_i = \beta_0 + \beta_1 X_{1i} + \beta_2 X_{2i} + \dots + \beta_kX_{ki} + u_i
\]
The variance of any partial regression coefficient, say $\hat{\beta}_j$ is then obtained via
\begin{equation}\label{eq:WhiteVar3}
Var(\hat{\beta}_j) 
  = \frac{\displaystyle\sum_{i=1}^n \hat{v}_{ji}^2 \hat{u}_i^2}{\Big( \displaystyle\sum_{i=1}^n \hat{v}_{ji}^2 \Big)^2}
  = \frac{\displaystyle\sum_{i=1}^n \hat{v}_{ji}^2 \hat{u}_i^2}{RSS_j^2}
\end{equation}
where $\hat{v}_{ji}$ denotes the $i^{th}$ residual from regressing $X_j$ on all other independent variables, and $RSS_j$ is the residual sum of squares from this regression. 

The square root of this expression in equation \ref{eq:WhiteVar3} is called \textbf{heteroskedasticity-robust standard error} for $\hat{\beta}_j$.

Also note that, sometimes the equation \ref{eq:WhiteVar3} is adjusted for degrees of freedom by multiplying it with $\frac{n}{n-(k+1)}$ before taking the square root. This is because if $\hat{u}_i$ were the same for all $i$ then we would get the usual OLS standard errors. Since all forms of this equation has asymptotic justification, and they are asymptotically equivalent, no one form is unanimously preferred over others. Usually, we use whatever form the software we work with uses. 
\end{tcolorbox}

\end{description}

We can now calculate this in R as follows:

```{r eval=FALSE}
#we need two additional libraries for this:
library(lmtest) #for `coeftest` function
library(sandwich) #for `vcovHC` function

coeftest(SQ2a_lm, vcov = vcovHC(SQ2a_lm, "HC1")) 
#the default in vcovHC is "HC3" but to get the exact result as STATA we use "HC1"
```

Similarly, we can calculate this in STATA as follows:
```{stata eval=FALSE}
/* run the regression with additional `robust` command */

regress PRICE LOTSIZE SQRFT BDRMS, robust
```

However, to present the "robust" and "nonrobust" results side by side in a table, we can use the following set of commands instead:

```{stata}
/* load the data */
  quietly cd ..
  quietly import excel using Data/sup4.xls, sheet("heteroscedasticity") firstrow

/* run the regression with `robust` command */
  quietly regress PRICE LOTSIZE SQRFT BDRMS, robust

/* store the estimates under the heading "robust" */
  estimates store robust
  
/* run the regression for nonrobust */
  quietly regress PRICE LOTSIZE SQRFT BDRMS 
  
/* store the estimates under the heading "nonrobust" */
  estimates store nonrobust
  
/* create the table for robust and nonrobust estimates of beta, s.e., and t-values */
  estimates table robust nonrobust, b se t

```

Notice that all the $t$-values are lower for each variable and in the case of $LOTSIZE$ this reduction means it is no longer significant. This is at least suggestive that $LOTSIZE$ may be a major source of the heteroskedasticity.




\bigskip\bigskip
***
\bigskip\bigskip

### (c) Using the specification in equation (\ref{eq:SQ2a}) on page \pageref{eq:SQ2a}, conduct a Goldfeld-Quandt test for heteroskedasticity in the $LOTSIZE$ dimension (exclude the middle 24 observations).

\begin{description}
\item[Answer:]
The Goldfeld-Quandt\footnote{Goldfeld S, and Quandt R E (1972) \textit{Nonlinear Methods in Econometrics}, North Holland Publishing Company, Amsterdam.} test is applicable when we assume that the heteroskedastic variance, $\sigma_i^2$, is positively related to \textit{one} of the explanatory variables in the regression model. In this question we are assuming that the heteroskedastic variance is related to $LOTSIZE$.

\begin{tcolorbox}[breakable, title=Goldfeld-Quandt Test, skin=enhancedlast]

\textbf{What are the reasoning and mechanics of the test?}\footnote{Gujarati and Porter (2009), Section 11.5}

Consider the simple regression model:
\[
Y_i = \beta_0 + \beta_1 X_1 + u_i
\]
and suppose $\sigma_i^2$ is monotonically related to $X_i$. One plausible assumption of this is 
\begin{equation}\label{eq:GQTest1}
\sigma_i^2 = \sigma^2X_i^2.
\end{equation}
What this assumption says is that $\sigma_i^2$ is proportional to the square of the $X$ variable. If this assumption is appropriate, it would mean the larger $X_i$ values are, the larger $\sigma_i^2$ gets. If that turns out to be the case, heteroskedasticity is most likely to be present in the model. 

To test this, Goldfeld and Quandt provide the following steps:
\begin{itemize}
\item[Step 1:] Order or rank the observations according to the values of $X_i$ beginning with the lowest $X$ value;

\item[Step 2:] Omit $c$ central observations, where $c$ is specified a priori, and divide the remaining observations into two groups, each of $\frac{n-c}{2}$ observations;

\item[Step 3:] Fit separate OLS regressions to these two groups of observations and obtain the respective residual sum of squares $RSS_1$ and $RSS_2$, where $RSS_1$ represents the $RSS$ from the regression corresponding to the smaller $X_i$ values, i.e. small variance group, and $RSS_2$ to the larger $X_i$ values, i.e. the large variance group.

These $RSS$ each have $\displaystyle\frac{n-c}{2}-(k+1)$ degrees of freedom where $k$ is the number of parameters to be estimated, excluding the intercept - hence $+1$.

\item[Step 4:] Compute the following ratio:
\[
\lambda = \frac{\frac{RSS_2}{df}}{\frac{RSS_1}{df}}
\]
\end{itemize}

The main argument of this test is that if the assumption of homoskedasticity and $u_i$ are normally distributed both hold true, then $\lambda$ of equation (\ref{eq:GQTest1}) follows the $F$-distribution with $\displaystyle\frac{n-c}{k}-(k+1)$ degrees of freedom in both the numerator and denominator.

As usual, if the computed $\lambda$ which is equal to $F$-statistic, is greater than the critical $F$ value at the chosen level of significance, we can reject the null hypothesis of homoskedasticity.

\textbf{Why we ommit $c$ central observations?}
These observations are omitted to accentuate the difference between the small variance group, $RSS_1$, and the large variance group $RSS_2$. However, the \textit{power} of the test depends on how $c$ is chosen. Recall that \textit{power of a test} is measured by the probability of rejecting the null hypothesis when it is false, and it is calculated by $1-prob(Type\ II\ error)$.

Goldfeld and Quandt suggest $c=8$ for models with two-explanatory variables if $n=30$ and double if $n=60$.
\end{tcolorbox}
\end{description}

In this question $c=24$ and we order $LOTSIZE$ from small to large. To run the Goldfeld-Quandt test in R we can use the `gqtest()` function from the `lmtest` library:

```{r include=FALSE}
library(lmtest)
```

```{r}
gqtest(SQ2a_lm, order.by = property_df$LOTSIZE, fraction = 24, alternative="two.sided")
qf(0.975, 28, 28, lower.tail = TRUE) #critical F-value
```

In STATA there are more steps involved. First we need to order the data and removed the middle 24 observations before running regression on each:

```{stata}
/* load the data */
  quietly cd ..
  quietly import excel using Data/sup4.xls, sheet("heteroscedasticity") firstrow
 
/* Step 1: Order the data according to LOTSIZE values */  
  sort LOTSIZE
  
/* create an index on which we will impose our condition for splitting data */  
  gen index=_n

/* run the regressions on each splitted data */  
  reg PRICE BDRMS LOTSIZE SQRFT if index<33
  
  reg PRICE BDRMS LOTSIZE SQRFT if index>56
  
/* Derive F-stat by dividing RSS of each (since df cancel out) */
  display e(rss)/8.5839e+10
  
/* compute critical F value */
  display invfprob(28,28,0.025)

```

Both R and STATA give the same result that the $F$-statistic of $1.6275654$ is smaller than the critical $F$-value of $2.1299243$ which means we cannot reject the null of homoskedasticity. Therefore, it seems as though there is no heteroskedasticity according to the Goldfeld-Quandt test. However, the form of heteroskedasticity may be more complicated.




\bigskip\bigskip
***
\bigskip\bigskip

### (d) Test for heteroskedasticity by first estimating an equation that regresses the squared residuals from equation (\ref{eq:SQ2a}) on page \pageref{eq:SQ2a} against all of the independent variables used to estimate equation (\ref{eq:SQ2a}). (Calculate both F and LM versions of this test). Verify your results using the 'hettest' command in Stata. Compare these results with the results of the White Test in Stata.

\begin{description}
\item[Answer:]
Goldfeld-Quandt test depends not only on the number of observations we omit but also on identifying the correct $X$ variable that needs to be ordered. These limitations of this test can be avoided with \textit{Breusch-Pagan Test},\footnote{Breusch, T and Pagan A (1979) "A Simple Test for Heteroscedasticity and Random Coefficient Variation", \textit{Econometrica}, 47:1287-1294.} or BP test, which is also called \textit{Breusch-Pagan-Godfrey Test},\footnote{Godfrey L (1978) "Testing for Multiplicative Heteroscedasticity" \textit{Journal of Econometrics}, 8:227-236.} or BPG test.  

\begin{tcolorbox}[breakable, title=Breusch-Pagan / Breusch-Pagan-Godfrey Test, skin=enhancedlast]

\textbf{What are the reasoning and mechanics of the test?}\footnote{Gujarati and Porter (2009), Section 11.5; Wooldridge (2021), Section 8.3}

Consider the model
\[
Y = \beta_0 + \beta_1X_{1} + \beta_2X_{2} + \dots + \beta_kX_{k} + u
\]
and assume that $\mathbb{E}(u | X_1. X_2, \dots, X_k) = 0$ so that OLS is unbiased and consistent. 

The null hypothesis is that homoskedasticity holds and we require the data to tell us otherwise. That is, $\mathbb{H}_0: Var(u|X_1,X_2,\dots,X_k) = \sigma^2$; and since $Var(u|X_1,X_2,\dots,X_k) = \mathbb{E}(u^2|X_1,X_2,\dots,X_k)$ the null hypothesis can be expressed as:
\[
\mathbb{H}_0: Var(u|X_1,X_2,\dots,X_k) = \mathbb{E}(u^2|X_1,X_2,\dots,X_k) = \mathbb{E}(u^2) = \sigma^2.
\]
This shows that in order to test for violation of the homoskedasticity assumption we want to test whether $u^2$ is related in expected value to one or more of the explanatory variables. Therefore, if $\mathbb{H}_0$ is false, then the expected value of $u^2$ given the independent variables, i.e. $\mathbb{E}(u^2|X_1, X_2,\dots,X_k)$ can be any function of the $X_j$. A simple approach is to assume a linear function:
\[
u^2 = \gamma_0 + \gamma_1X_1 + \gamma_2X_2 + \dots + \gamma_kX_k + v
\]
The null hypothesis then becomes
\[
\mathbb{H}_0: \gamma_1 = \gamma_2 = \dots = \gamma_k = 0.
\]

Under the null hypothesis we can assume that the error $v$ is independent of $X_1, \dots X_k$. Then, either the $F$ or \textit{Lagrange Multiplier (LM)} statistics can be used to test for the overall significance of the independent variables in explaining $u^2$. Both statistics would have asymptotic justification, even though $u^2$ cannot be normally distributed.

$\hookrightarrow$ e.g. if $u$ is normally distributed then $\frac{u^2}{\sigma^2}$ is distributed $\chi_1^2$.

If we could observe the $u^2$ in the sample, then we could compute this statistic by running the OLS regression of $u^2$ on $X_1,\dots,X_k$ using all $n$ observations, which would give us the maximum likelihood (ML) of $\sigma^2$. 

Since we do not know $u$, we can instead estimate the equation:
\[
\hat{u}^2 = \gamma_0 + \gamma_1 X_1 + \gamma_2 X_2 + \dots + \gamma_k X_k + \epsilon
\]

and compute $F$ or $LM$ statistics for the joint significance of $X_1, \dots, X_k$. The $F$ and $LM$ statistic both depend on the $R$-squared value of this regression, $R_{\hat{u}^2}^2$.

The $F$-statistic for heteroskedasticity is
\[
F = \frac{\displaystyle\frac{R_{\hat{u}^2}^2}{k}}{\displaystyle\frac{1-R_{\hat{u}^2}^2}{n-(k+1)}}
\]
where $k$ is the number of regressors. This $F$ statistic has approximately an $F_{k, n-(k+1)}$ distribution under the null hypothesis of homoskedasticity.

The $LM$ statistic for heteroskedasticity is
\[
LM = n \times R_{\hat{u}^2}^2
\]
which is the $R$-squared of the error regression multiplied by the sample size. Under the null hypothesis, $LM$ is distributed asymptotically as $\chi_k^2$.

The $LM$ version of the test is called the \textbf{Breusch-Pagan test} for heteroskedasticity, or BP-test; though the $LM$-statistic form was suggested by Koenker (1981).\footnote{Koenker, R (1981) "A Note on Studentizing a Test for Heteroskedasticity", \textit{Journal of Econometrics} 17:107-112.}

\bigskip

\textbf{What is Lagrange Multiplier Statistic?}\footnote{Wooldridge (2021), Section 5.2a}

Consider again the multiple regression model with $k$ independent variables:
\[
Y = \beta_0 + \beta_1X_{1} + \beta_2X_{2} + \dots + \beta_kX_{k} + u
\]
We want to test whether, say, the last $q$ of these variables all have $0$ population parameters. The null hypothesis is therefore
\[
\mathbb{H}_0: \beta_{k-q+1} = \beta_{k-q+2} = \dots = \beta_k = 0, 
\]
which puts $q$ exclusion restrictions on the model. The alternatively hypothesis is that at least one of the parameters is different from $0$.

The LM statistic requires the estimation of the restricted model only. So we run the regression
\[
Y = \beta_0^{res} + \beta_1^{res} X_1 + \dots + \beta_{k-q}^{res} X_{k-q} + u^{res}
\]
where $u^{res}$ indicate that the residuals are from the restricted model. Note that this is a shorthand to indicate that we obtain restricted residual for each observation in the sample, but didn't use the $i$ subscript to avoid crowding of subscripts.

The idea is that if the omitted variables $X_{k-q+1}$ through $X_{k}$ truly have $0$ population coefficients, then $u^{res}$ should at least be approximately uncorrelated with each of these variables in the sample. In fact, it should be uncorrelated with all regressors because the omitted regressors in the restricted model are correlated with the regressors that appear in the restricted model.

This means, we run the regression of $u^{res}$ on $X_1, \dots, X_k$.

$\hookrightarrow$ this is an example of \textit{auxiliary regression} which is a regression used to compute a test statistic but whose coefficients are not of direct interest.

If the null hypothesis is true, the $R$-squared from this regression should be "close" to zero, subject to sampling error. This is because $u^{res}$ will be approximately uncorrelated with all the independent variables. 

What is interesting with this test is that, under the null hypothesis, the sample size multiplied by the $R$-squared from the auxiliary regression is distributed asymptotically as a chi-square random variable with $q$ degrees of freedom. That is, $n\times R_{\hat{u}^2}^2 \ \overset{a}{\sim} \ \chi_q^2$.

Because of its form, the $LM$ statistic is also referred to as the \textbf{n-R-squared statistic}.
\end{tcolorbox}

\end{description}

\bigskip

We can obtain the BP-statistic that has a $\chi_3^2$ distribution in R as follows:

```{r eval=FALSE}
bptest(SQ2a_lm)
```

In STATA, we can do the same using the `hettest` command:

```{stata}
/* load the data */
quietly cd ..
quietly import excel using Data/sup4.xls, sheet("heteroscedasticity") firstrow

/* run the regression */
quietly regress PRICE LOTSIZE SQRFT BDRMS

/* apply hettest where rhs mean right-hand-side */
  hettest, rhs fstat
  hettest, rhs iid

/* manual calculation to compare the results */
  predict u, r
  generate U2 = u^2
  quietly regress U2 LOTSIZE SQRFT BDRMS

/* display the F-statistic and LM-statistic */
  display e(F)
  display e(r2)*e(N)
```

it is often the case that $\chi^2$-tests have better properties but are harder to explain. So here we use the $F$ initially to give the intuition then point out which $\chi^2$-tests do roughly the same things. Based on the $p$-values we can reject the null hypothesis. The Breusch-Pagan test suggests the presence of heteroskedasticity.

However, the BP test assumes that the form of the heteroskedasticity is linear. To try out different forms of the relations, we can use the White test.

\begin{tcolorbox}[breakable, title=White Test, skin=enhancedlast]

\textbf{What are the reasoning and mechanics of the test?}\footnote{Gujarati and Porter (2009), Section 11.5; Wooldridge (2021), Section 8.3a}

Unlike Goldfeld-Quandt test, which requires reordering the observations with respect to the $X$ variable that supposedly caused heteroskedasticity, or the BP test, which is sensitive to the normality and linearity assumptions, the general heteroskedasticity test proposed by White (1980)\footnote{White H (1980) "A Heteroskedasticity Consistent Covariance Matrix Estimator and a Direct Test of Heteroskedasticity", \textit{Econometrica}, 48:817-818} does not rely on the normality assumption. 

White test uses the insight that the homoskedasticity assumption can be replaced with the weaker assumption that the squared error $\hat{u}_i^2$ is \textit{uncorrelated} with all the independent variables, $X_j$, the squares of the independent variables, $X_j^2$, and all the cross products, $X_jX_h$ for $j\neq h$. 

The test is explicitly intended to test for forms of heteroskedasticity that invalidate the usual OLS standard errors and test statistics. Consider a model with $k=3$ independent variables. The White test process is as follows:
\begin{itemize}
\item[Step 1:] Estimate $Y_i = \beta_0 + \beta_1 X_{1i} + \beta_2 X_{2i} + \beta_3 X_{3i} + u_i$ and obtain the residuals, $\hat{u}_i$;

\item[Step 2:] Obtain the $R_{\hat{u}^2}^2$ from following auxiliary regression:
\[
\hat{u}_i^2 = \gamma_0 + \gamma_1 X_{1i} + \gamma_2 X_{2i} + \gamma_3 X_{3i} + \gamma_4 X_{1i}^2 + \gamma_5 X_{2i}^2 + \gamma_6 X_{3i}^2 + \gamma_7 X_{1i}X_{2i} + \gamma_8 X_{1i}X_{3i} + \gamma_9 X_{2i}X_{3i} + \epsilon_i 
\]
That is, we are regressing the squared residuals from the original regression on the original variables, their squared values, and the cross products of the regressors. We can also introduce higher powers of regressors if necessary.

\item[Step 3:] Under the null hypothesis that there is no heteroskedasticity, $n\times R_{\hat{u}^2}^2 \ \overset{a}{\sim} \ \chi_{df}^2$. In this example we have $9$ regressors, so $df=9$. 

\item[Step 4:] If the $\chi_{df}^2$ value obtained is higher than the critical $\chi_{df}^2$ at the chosen level of significance, then this test would suggest a presence of heteroskedasticity. If it does not exceed the critical value, then we cannot reject $\mathbb{H}_0: \gamma_1 = \dots = \gamma_9 = 0$.
\end{itemize}

One final point is that this approach of White test uses many degrees of freedom. We can have a slightly different approach to White test that can conserve on degrees of freedom. To create the test, notice that the difference between White and BP tests is that the White test includes the squares and cross-products of the independent variables, whereas BP doesn't. We can preserve the spirit of the White test while conserving on degrees of freedom by using the OLS fitted values in a test for heteroskedasticity. 

Recall the fitted values are defined for each observation $i$ by
\[
\hat{Y}_i = \hat{\beta}_0 + \hat{\beta}_1 X_{1i} + \dots + \hat{\beta}_k X_{ki}
\]
These are just linear functions of the independent variables. If we square the fitted values, we get a particular function of all the squares and cross products of the independent variables. This suggests testing for heteroskedasticity by estimating the equation
\[
\hat{u}_i^2 = \eta_0 + \eta_1 \hat{Y}_i + \eta_2 \hat{Y}_i^2 + \epsilon
\]
where $\hat{Y}_i$ stand for fitted values. We can use $F$ or $LM$ statistic for the null hypothesis $\mathbb{H}_0: \eta_1=0, \eta_2=0$. This results in two restrictions in testing the null of homoskedasticity, regardless of the number of independent variables in the original model. This can be thought of as a special case of White test.
\end{tcolorbox}

We can run the White test in R manually as follows:
```{r eval=FALSE}
Ru2_SQ2 <- summary(lm(resid(SQ2a_lm) ~ fitted(SQ2a_lm) + I(fitted(SQ2a_lm)^2)))$r.squared
LM_SQ2 <- nrow(property_df)*Ru2_SQ2
p_value_SQ2 <- 1-pchisq(LM_SQ2,2)
p_value_SQ2
```

or we can use the `bptest()` function from `lmtest` package:
```{r eval=FALSE}
bptest(SQ2a_lm, ~ BDRMS + LOTSIZE + SQRFT 
       + I(BDRMS)^2 + I(LOTSIZE)^2 + I(SQRFT)^2 
       + BDRMS*LOTSIZE + BDRMS*SQRFT + SQRFT*LOTSIZE, 
       data = property_df)

#Special case of White test that conserves on degrees of freedom
bptest(SQ2a_lm, ~ fitted(SQ2a_lm) + poly(fitted(SQ2a_lm),2))
```

```{stata}
/* load the data */
quietly cd ..
quietly import excel using Data/sup4.xls, sheet("heteroscedasticity") firstrow

/* run the regression */
quietly regress PRICE LOTSIZE SQRFT BDRMS

/* manual calculation for White Test */
  predict u, r
  generate U2 = u^2
  generate B2 = BDRMS^2
  generate L2 = LOTSIZE^2
  generate S2 = SQRFT^2
  generate BL = BDRMS*LOTSIZE
  generate BS = BDRMS*SQRFT
  generate LS = LOTSIZE*SQRFT
  
  quietly regress U2 BDRMS LOTSIZE SQRFT B2 L2 S2 BL BS LS
  
/* calculate the chi-square statistic */
  display e(N)*e(r2)
```
or we can use the `imtest, white` command in STATA after the original regression:

```{stata}
/* load the data */
quietly cd ..
quietly import excel using Data/sup4.xls, sheet("heteroscedasticity") firstrow

/* run the regression */
quietly regress PRICE LOTSIZE SQRFT BDRMS

/* run the White test */
imtest, white
```

In all of these approaches we obtain a chi-squared value of $33.73$ with $0.001$ p-value. Thus we can reject the null hypothesis of the homoskedasticity.

However, there is sure to be lots of multicollinearity here so it is difficult to tell if there is a non-linear relationship with any of the variables. We can run individual regressions and see what we can find. For example, with $LOTSIZE$:

```{stata}
/* load the data */
quietly cd ..
quietly import excel using Data/sup4.xls, sheet("heteroscedasticity") firstrow

/* run the regression */
quietly regress PRICE LOTSIZE SQRFT BDRMS

/* obtain residual squareds and create LOTSIZE squared */
  predict u, r
  generate U2 = u^2
  generate LOTSIZE2 = LOTSIZE^2
  
/* regress residuals on lotsize for nonlinearity */
  regress U2 LOTSIZE LOTSIZE2
```
There appears to be a non-linear relationship to LOTSIZE, in which case the White test may be better than the Breusch-Pagan test.



***
\bigskip\bigskip

### (e) Researcher A decides to try 'scaling' to remove the heterosekdasticity and so reestimates the equation in part (a) by dividing equation (\ref{eq:SQ2a}) on page \pageref{eq:SQ2a} by $BDRMS$ and then, as an alternative, by $LOTSIZE$. Discuss the reasoning behind using such variable to scale in this way (what must the form of heteroskedasticity be in each case? - note the $1/X$ term in each regression you have to estimate!). Which gives the best results?

\begin{description}
\item[Answer:]
Recall in Question 2(b) we calculated White's robust standard errors. That method has some drawbacks, however. In addition to being a large-sample procedure, the estimators obtained using White's robust standard errors may not be so efficient compared to those obtained by transforming the data to reflect specific types of heteroskedasticity. 

To see this, consider the simple regression
\[
Y_i = \beta_0 + \beta_1 X_i + u_i
\]
and assume that the error variance is proportional to square of the explanatory variable, $X_i^2$:
\[
\mathbb{E}(u_i^2) = \sigma^2X_i^2.
\]
If this is the case, the original model can be transformed to yield a homoskedastic error variance as follows.

First, divide the original model by $X_i$ so that
\begin{align*}
\frac{Y_i}{X_i} 
  = \frac{\beta_0}{X_i} + \beta_1 \frac{X_i}{X_i} + \frac{u_i}{X_i} \\[6pt]
  = \frac{\beta_0}{X_i} + \beta_1 + v_i
\end{align*}
where $v_i = u_i / X_i$ is the transformed disturbance term. Then,
\[
\mathbb{E}(v_i^2) = \mathbb{E}\Bigg[\Big(\frac{u_i}{X_i}\Big)^2\Bigg] = \frac{1}{X_i^2}\mathbb{E}(u_i^2) = \frac{1}{X_i^2}\sigma^2X_i^2 = \sigma^2.
\]
That is, the variance of $v_i$ is homoskedastic and OLS can be applied to the transformed equation.

This is exactly what the question is asking us to do. In the first part we will estimate the following:
\[
\frac{PRICE_i}{BDRMS_i} = \beta_0\frac{1}{BDRMS_i} + \beta_1 \frac{LOTSIZE_i}{BDRMS_i} + \beta_2 \frac{SQRFT_i}{BDRMS_i} + \beta_3 + u_i\frac{1}{BDRMS_i} 
\]
\end{description}

In R we can run the transformed regression and run the BP and White tests as follows:

```{r eval=FALSE}
SQ2e_lm_bdrms <- lm(I(PRICE/BDRMS) ~ I(LOTSIZE/BDRMS) + I(SQRFT/BDRMS) + I(1/BDRMS), 
     data=property_df)
summary(SQ2e_lm_bdrms)

# Breusch-Pagan test
bptest(SQ2e_lm_bdrms)

# White test
bptest(SQ2e_lm_bdrms, ~ I(LOTSIZE/BDRMS) + I(SQRFT/BDRMS) + I(1/BDRMS)
       + I((LOTSIZE/BDRMS)^2) + I((SQRFT/BDRMS)^2) + I((1/BDRMS)^2)
       + I(LOTSIZE/BDRMS)*I(SQRFT/BDRMS) + I(LOTSIZE/BDRMS)*I(1/BDRMS)  
       + I(SQRFT/BDRMS)*I(1/BDRMS), data=property_df)

#Special case of White test that conserves on degrees of freedom
bptest(SQ2e_lm_bdrms, ~ fitted(SQ2e_lm_bdrms) + poly(fitted(SQ2e_lm_bdrms),2))
```

and in STATA as follows:
```{stata}
/* load the data */
  quietly cd ..
  quietly import excel using Data/sup4.xls, sheet("heteroscedasticity") firstrow

/* generate the transformations */
  generate PRBD = PRICE/BDRMS
  generate LTBD = LOTSIZE/BDRMS
  generate FTBD = SQRFT/BDRMS
  generate BD = 1/BDRMS

/* run the regression */
  regress PRBD LTBD FTBD BD
  
/* run the Breusch-Pagan test */
  hettest LTBD FTBD BD, iid
  
/* run the White test */
  imtest, white
```

From the chi-squared values and their associated p-values we cannot reject the null hypothesis of homoskedasticity.

Note that we can also obtain the same using the GLS, or weighted least squares, approach discussed in question 1(b). Here the weights are $1/BDRMS_i$. 

If we do the same transformation with $LOTSIZE$ we get:
```{stata}
* load the data */
  quietly cd ..
  quietly import excel using Data/sup4.xls, sheet("heteroscedasticity") firstrow

/* generate the transformations */
  generate PRLT = PRICE/LOTSIZE
  generate LT = 1/LOTSIZE
  generate FTLT = SQRFT/LOTSIZE
  generate BDLT = BDRMS/LOTSIZE

/* run the regression */
  regress PRLT LT FTLT BDLT
  
/* run the Breusch-Pagan test */
  hettest LT FTLT BDLT, iid
  
/* run the White test */
  imtest, white
```

Once again, from the chi-squared values and their associated p-values we cannot reject the null hypothesis of homoskedasticity.




\bigskip\bigskip
***
\bigskip\bigskip

### (f) Researcher B decides to pursue the following strategies to remove heteroskedasticity:
\begin{itemize}
\item[i)] \textbf{Use logged data for PRICE, LOTSIZE, SQRFT (N.B. don't drop BDRMS)}
\item[ii)] \textbf{Remove outliers (observations 42,73,76,77)}
\end{itemize}
\textbf{Discuss the reasoning behind each of these strategies, and the results obtained in each case.}

\begin{description}
\item[Answer (i):]
A log transformation such as $ln Y_i = \beta_0 + \beta_1 \ lnX_i + u_i$ often reduces heteroskedasticity because log transformation compresses the scales, reducing say a ten-fold difference between two values to a two-fold difference. This transformation, of course, would not be possible if some of the $Y$ and $X$ values are zero or negative. Though in this case, we can use $ln(Y_i + m)$ or $ln(X_i+m)$ where $m$ is a postive value large enough to make all the values of $Y$ or $X$ positive.

In this question, our transformation is as follows:
\[
ln(PRICE_i) = \beta_0 + \beta_1 \ ln(LOTSIZE_i) +\beta_2 \ ln(SQRFT_i) + \beta_3 \ BDRMS_i + u_i
\]

\end{description}

In R we can transform and obtain the BP and White tests as follows:

```{r eval=FALSE}
SQ2f_lm_log <- lm(log(PRICE) ~ log(LOTSIZE) + log(SQRFT) + BDRMS, data=property_df)
summary(SQ2f_lm_log)

# Breusch-Pagan test
bptest(SQ2f_lm_log)

# White test
bptest(SQ2f_lm_log, ~ log(LOTSIZE) + log(SQRFT) + BDRMS
       + I(log(LOTSIZE)^2) + I(log(SQRFT)^2) + I(BDRMS^2)
       + log(LOTSIZE)*log(SQRFT) + log(LOTSIZE)*BDRMS + log(SQRFT)*BDRMS, data=property_df)

#Special case of White test that conserves on degrees of freedom
bptest(SQ2f_lm_log, ~ fitted(SQ2f_lm_log) + poly(fitted(SQ2f_lm_log),2))
```

Similarly, in STATA:

```{stata}
/* load the data */
  quietly cd ..
  quietly import excel using Data/sup4.xls, sheet("heteroscedasticity") firstrow

/* generate the transformations */
  generate lnPR = ln(PRICE)
  generate lnLT = ln(LOTSIZE)
  generate lnFT = ln(SQRFT)

/* run the regression */
  regress lnPR lnLT lnFT BDRMS
  
/* run the Breusch-Pagan test */
  hettest lnLT lnFT BDRMS, iid
  
/* run the White test */
  imtest, white
```

From the chi-squared values and their associated p-values we fail to reject the null hypothesis of homoskedasticity.

\bigskip

\begin{description}
\item[Answer (ii):]
Heteroskedasticity can also arise as a result of the presence of outliers. An outlier "is an observation from a different population to that generating the remaining sample observations."\footnote{Gujarati and Porter (2009), Section 11.1} Whether those outliers are included in the regression or not can substantially alter the results of the regression analysis. This is because OLS gives equal weight to every observation in the sample. 

To see why these points were picked in the question first let's look at the scatter plot of $LOTSIZE$
\end{description}

```{r}
plot(SQ2a_lm$residuals^2 ~ property_df$LOTSIZE, 
     xlab="Lot Size ft-sq", ylab = "residuals squared")
text(SQ2a_lm$residuals^2 ~ property_df$LOTSIZE, label=property_df$Date)
```

Here we can see that the 42nd, 73rd, 76th, and 77th data points are outliers.

We can now remove these and run the regression in R:

```{r eval=FALSE}
# create a new dataframe with outliers removed
property_df_nooutl <- property_df[-c(42,73,76,77),]

# run the regression using this new dataframe
SQ2f_lm_nooutl <- lm(PRICE ~ LOTSIZE + SQRFT + BDRMS, data = property_df_nooutl)
summary(SQ2f_lm_nooutl)

# run the BP test
bptest(SQ2f_lm_nooutl)

# run the White test
bptest(SQ2f_lm_nooutl, ~ BDRMS + LOTSIZE + SQRFT 
       + I(BDRMS^2) + I(LOTSIZE^2) + I(SQRFT^2) 
       + BDRMS*LOTSIZE + BDRMS*SQRFT + SQRFT*LOTSIZE, 
       data = property_df_nooutl)

#Special case of White test that conserves on degrees of freedom
bptest(SQ2f_lm_nooutl, ~ fitted(SQ2f_lm_nooutl) + poly(fitted(SQ2f_lm_nooutl),2))
```

and in STATA

```{stata}
/* load the data */
  quietly cd ..
  quietly import excel using Data/sup4.xls, sheet("heteroscedasticity") firstrow

/* run the regression removing the outliers */    
  regress PRICE LOTSIZE BDRMS SQRFT if Date != 42 & Date != 73 & Date != 76 & Date != 77

  /* run the Breusch-Pagan test */  
  hettest LOTSIZE BDRMS SQRFT, iid
  
/* run the White test */
  imtest, white
```

From the chi-squared values and their associated p-values we fail to reject the null hypothesis of homoskedasticity.


\bigskip\bigskip
***
\bigskip\bigskip

### (g) Considering your results from parts (e) and (f), which strategy for removing heteroskedasticity do you believe to be the best and why?

\begin{description}
\item[Answer:]
Table below summarizes the results of the BP and White tests for each strategy:

\begin{center}
\begin{tabular}{|| c | c | c | c c c | ||}
\multicolumn{6}{r}{Heteroskedasticity Test} \\
 \hline
 &  F & Adj $R^2$ & & BP & White \\
\hline\hline
\multirow{2}{8em}{Scaling by BDRMS} & \multirow{2}{3em}{37.39} & \multirow{2}{3em}{0.557} & $\chi^2$ & 2.59 & 14.13 \\
 & & & $p$-value & 0.459 & 0.118 \\
\hline
\multirow{2}{8em}{Scaling by LOTSIZE} & \multirow{2}{3em}{480.77} & \multirow{2}{3em}{0.943} & $\chi^2$ & 5.96 & 8.74 \\
 & & & $p$ value & 0.113 & 0.462 \\
\hline
\multirow{2}{8em}{log transformation} & \multirow{2}{3em}{50.42} & \multirow{2}{3em}{0.63} & $\chi^2$ & 4.22 & 9.55 \\
 & & & $p$ value & 0.238 & 0.388 \\
\hline
\multirow{2}{8em}{remove outliers in LOTSIZE} & \multirow{2}{3em}{65.83} & \multirow{2}{3em}{0.701} & $\chi^2$ & 5.97 & 9.26 \\
 & & & $p$ value & 0.113 & 0.414 \\
\hline
\end{tabular}
\end{center}

Looking at the table, we can see that in all approaches we are able to tackle heteroskedasticity since in all of them we fail to reject the null hypothesis of homoskedasticity. However, the $F$ value and the adjusted $R^2$ is considerably higher for "scaling by $LOTSIZE$" strategy. This may be because the error variance is proportional to $LOTSIZE$ and there are outliers in the $LOTSIZE$ dimension. Scaling gets rid of both problems. 

\end{description}














\bigskip\bigskip\bigskip
\hdashrule[0.5ex]{\textwidth}{1pt}{3mm}
\bigskip\bigskip\bigskip

## QUESTION 3

### (a) Briefly discuss the problem of autocorrelation - why might such a problem arise and what problems follow from the use of OLS?

\begin{description}
  \item[Answer:] We will briefly discuss the nature of the problem, why it occurs, and what problems arise from use of OLS with serially correlated errors.
  
  \underline{What is the problem of autocorrelation?} 
  
  Classical linear regression model assumes autocorrelation does not exist in the disturbances. That is, it assumes $Cov(u_i,u_j | X_i, X_j) = 0, \ i\neq j$. What this means is that the disturbance term relating to any observation is not influenced by the disturbance term relating to any other observation. If there is such a dependence, then autocorrelation is present. That is, $\mathbb{E}(u_i,u_j)\neq 0$.
 
  \underline{Why autocorrelation occurs?} 
  
  There are several reasons as to why autocorrelation occurs including inertia, specification bias, cobweb phenomenon, lags, 'manipulation' of data, data transformation, and nonstationarity.\footnote{Gujarati and Porter (2009), Section 12.1}
    \begin{itemize}
      \item[inertia:] Most econometrics time series has inertia or sluggishness in that the variables exhibit cyclicality where successive observations are likely to be interdependent.
      \item[specification bias:] It may be the case that one of the variables of the true model is omitted or excluded from the model used. The omitted variable would then be part of the error or disturbance term, which would in turn reflect a systematic pattern.
      \item[cobweb phenomenon:] The supply of many agricultural commodities reflect the cobweb phenomenon where supply reacts to price with a lag of one time period because supply decisions take time to implement. Thus at the beginning of this year's planting of crops, farmers are influenced by the price prevailing last year.
      \item[lags:] In a time series regression, it is not uncommon to find that the dependent variable in the current variable also depends on the value of itself in the previous period. That is, $Y_t = \beta_0 + \beta_1 X_t + \beta_2 Y_{t-1} + u_t$. This is known as \textit{autoregression}. If we omit the lagged term in this equation, the resulting error term will reflect a systematic pattern due to the influence of the lagged variable on the dependent variable.
      \item['manipulation' of data:] Smoothing out the data, say by averaging three monthly observations to obtain quarterly figures, may itself lend to a systematic pattern in the disturbances, thereby introducing autocorrelation. Similarly, interpolation or extrapolating from data for the missing values, or any data 'massaging' techniques may also introduce autocorrelation.
      \item[data transformation:] Even if the level form $Y_t = \beta_1 + \beta_2X_t + u_t$ satisfies the OLS assumptions including no autocorrelation, in its first difference form $\Delta Y_i = \beta_2 \ \Delta X_i + v_i$ the error term $v_i = \Delta u_i$ is autocorrelated. Thus, certain transformations may induce autocorrelation.
      \item[nonstationarity:] A time series is stationary if its characteristics such as mean, variance, and covariance are time invariant. If, however, they do change over time, then the time series are nonstationary, and the error term will exhibit autocorrelation.
    \end{itemize}
  \underline{Problems in estimating with autocorrelated errors:} 
  
  Lets look at what happens in terms of unbiasedness, consistency, efficiency and goodness of fit.
    \begin{itemize}
      \item[Unbiasedness:] Unbiasedness assumes nothing about the serial correlation of errors. As long as the explanatory variables are strictly exogenous then the $\hat{\beta}_j$ are unbiased, regardless of the degree of serial correlation in the errors. This is analogous to the observation that heteroskedasticity in the errors does not cause bias in the $\hat{\beta}_j$.
      \item[consistency:] Even when the data are weakly dependent, the $\hat{\beta}_j$ are still consistent, though not necessarily unbiased if there is that dependency. Just like unbiasedness, this result does not depend on autocorrelation in the errors.
      \item[efficiency:] Since the Gauss-Markov requires both homoskedasticity and serially uncorrelated errors, presence of autocorrelation in errors would make the OLS no longer BLUE. Even more importantly, the usual OLS standard errors and test statistics are not valid, \textit{even asymptotically}.
    \end{itemize}
     
\begin{tcolorbox}[breakable, title=Why not BLUE?, skin=enhancedlast]
To see why OLS is no longer BLUE in the presence of serially correlated errors, consider the simple regression model $Y_t = \beta_0+\beta_1X_t + u_t$ and assume that the error, or disturbance, terms are generated by the following mechanism:
\[
u_t = \rho u_{t-1} + \varepsilon_t, \ \ -1 < \rho < 1
\]
where $\rho$ is the \textit{coefficient of autocovariance}, and $\varepsilon_t$ are uncorrelated random variables that satisfies the OLS assumptions:
\[
\mathbb{E}(\varepsilon_t) = 0 \ \ ; \ \ Var(\varepsilon_t) = \sigma^2 \ \ ; \ \ Cov(\varepsilon_t, \varepsilon_{t-s}) = 0 \ , \ s\neq 0.
\]
Thus the error term in period $t$ is equal to $\rho$ times its value in the preceding period plus a white noise error term. This scheme is known as a \textit{Markov first-order autoregressive scheme}, or just a \textit{first-order autoregressive scheme}, and usually denoted as \textbf{AR(1)}.

Also note that the population correlation coefficient between $u_t$ and $u_{t-1}$ is given by
\[
\rho 
= \frac{\mathbb{E}\Big( \big[ u_t - \mathbb{E}(u_t) \big] \big[ u_{t-1} - \mathbb{E}(u_{t-1}) \big] \Big) }{\sqrt{Var(u_t)}\sqrt{Var(u_{t-1})}} 
= \frac{\mathbb{E}(u_tu_{t-1})}{Var(u_{t-1})}
\]
the latter equality is due to $\mathbb{E}(u_t) = 0$ and $Var(u_t) = Var(u_{t-1})$ because of the homoskedasticity assumption. $\rho$ is also the slope coefficient in the regression of $u_t$ on $u_{t-1}$.

Under the AR(1) scheme we have 
\begin{align*}
Var(u_t) &= \mathbb{E}(u_t^2) = \frac{\sigma_\varepsilon^2}{1-\rho^2} \\[6pt]
Cov(u_t, u_{t-s}) &= \mathbb{E}(u_tu_{t-s}) = \rho^2\frac{\sigma_\varepsilon^2}{1-\rho^2} \\[6pt]
Corr(u_t, u_{t-s}) &= \rho^s
\end{align*}
Since $\rho$ is a constant between $-1$ and $1$, $Var(u_t)$ is still homoskedastic. Notice, however, $u_t$ is correlated not only with its immediate past value but its values several periods in the past. 

If $|\rho| = 1$ then the variances and covariances above are not defined. If $|\rho| < 1$ then the AR(1) process is \textit{stationary}, i.e. mean, variance, and covariance do not change over time. If $|\rho| < 1$ it is also the case that the value of the covariance will decline as we go into distant past.

Considering the simple regression model $Y_t = \beta_0+\beta_1X_t + u_t$ we know from previous supervisions that the OLS estimator of the slope coefficient is
\[
\hat{\beta}_1 = \frac{\sum(X_t - \bar{X})(Y_t - \bar{Y})}{\sum(X_t-\bar{X})^2}
\]
and its variance is
\[
Var(\hat{\beta}_1) = \frac{\sigma^2}{\sum(X_t - \bar{X})^2}
\]
Whereas under the AR(1) scheme, the variance of this estimator is
\begin{align*}
Var(\hat{\beta}_1)^{AR(1)} &= \frac{\sigma^2}{\sum(X_t - \bar{X})^2} \bigg[ 1 \ +\ 2\rho \frac{\sum(X_t - \bar{X})(X_{t-1} - \bar{X})}{\sum(X_t-\bar{X})^2} \ +\ 2\rho^2 \frac{\sum(X_t - \bar{X})(X_{t-2} - \bar{X})}{\sum(X_t-\bar{X})^2} \\
& \qquad + \dots + \ 2\rho^{n-1}\frac{(X_1 - \bar{X})(X_n - \bar{X})}{\sum(X_t-\bar{X})^2} \bigg]
\end{align*}
That is, the variance of $\hat{\beta}_1$ under an $AR(1)$ scheme is equal to its variance under the OLS times a term that depends on $\rho$ as well as the sample autocorrelations between the values taken by the regressor $X$ at various lags. In general, we cannot foretell whether $Var(\hat{\beta}_1)$ is greater or less than $Var(\hat{\beta}_1)^{AR(1)}$.

If we assume that the regressor $X$ also follows AR(1) scheme with a coefficient of autocorrelation, i.e. correlation between $X_t$ and $X_{t-1}$ given as
\[
r = \frac{\sum(X_t - \bar{X})(X_{t-1} - \bar{X})}{\sum(X_t-\bar{X})^2}
\]
then, $Var(\hat{\beta}_1)^{AR(1)}$ reduces to
\begin{align*}
Var(\hat{\beta}_1)^{AR(1)} 
  &= \frac{\sigma^2}{\sum(X_t - \bar{X})^2}\bigg( \frac{1+r\rho}{1-r\rho} \bigg) \\
  &= Var(\hat{\beta}_1)^{OLS}\bigg( \frac{1+r\rho}{1-r\rho} \bigg)
\end{align*}
If we continue to use the OLS estimator $\hat{\beta}_1$ and adjust the usual variance formula by taking account of the AR(1) scheme, $\hat{\beta}_1$ will still be linear and unbiased, but not efficient.

This finding is very similar to the finding that $\hat{\beta}_1$ is less efficient in the presence of heteroskedasticity.
\end{tcolorbox} 
\end{description}



\bigskip\bigskip
***
\bigskip\bigskip

### (b) If $Y_t = \alpha + \beta X_t + \varepsilon_t$ and $\varepsilon_t = \rho\varepsilon_{t-1} + v_t$, where $\mathbb{E}(\varepsilon_t) = 0, \ \rho\neq 0, \ Cov(\varepsilon_t,v_t)=0$ and $v_t \sim i.i.d. N(0, \sigma^2)$, show that:
\begin{itemize}
\item{i)} \textbf{if $\varepsilon_t$ is homoskedastic, $Var(\varepsilon_t) = \sigma^2/(1-\rho^2)$};
\item{ii)} \textbf{$Cov(\varepsilon_t, \varepsilon_{t-1}) \neq 0$};
\item{iii)} \textbf{$Corr(\varepsilon_t,\varepsilon_{t-1})=\rho$, where $Corr$ is the correlation coefficient.}
\end{itemize}

\begin{description}
\item[Answer (i):]
Under AR(1) we have $\varepsilon_t = \rho \varepsilon_{t-1} + v_t$ where $v_t$ is a white noise error term. Since $\varepsilon_t$ is homoskedastic, then
\[
\mathbb{E}(\varepsilon_t) = \rho\mathbb{E}(\varepsilon_{t-1}) + \mathbb{E}(v_t) = 0
\]
which means
\begin{align*}
\mathbb{E}(\varepsilon_t^2) = \rho^2\mathbb{E}(\varepsilon_{t-1}^2) + \mathbb{E}(v_t^2) \\
Var(\varepsilon_t) = \rho^2Var(\varepsilon_{t-1}) + Var(v_t)
\end{align*}
since $\varepsilon$s and $v$s are uncorrelated.

Because of homoskedasticity, notice that $Var(u_t) = Var(u_{t-1}) = \sigma^2$, and since $v_t \sim i.i.d. N(0, \sigma^2)$ its variance is $Var(v_t)= \sigma_v^2$. Plugging these back into the expression above we get
\begin{align*}
Var(\varepsilon_t) 
  &= \rho^2Var(\varepsilon_{t-1}) + Var(v_t) \\
  &= \rho^2Var(\varepsilon_{t}) + \sigma_v^2 \\
Var(\varepsilon_t) - \rho^2Var(\varepsilon_{t})  &= \sigma_v^2 \\
Var(\varepsilon_t)(1-\rho^2) &= \sigma_v^2 \\
Var(\varepsilon_t) &= \frac{\sigma_v^2}{1 - \rho^2}.
\end{align*}
as desired.

\bigskip

\item[Answer (ii):] 
To obtain the covariance, first we need to multiply the AR(1) scheme by $\varepsilon_{t-1}$ and then we can take the expectations of the resulting expression, since that is the covariance. Accordingly,
\begin{align*}
\varepsilon_t &= \rho\varepsilon_{t-1} + v_t \\
\varepsilon_t\varepsilon_{t-1} &= \rho\varepsilon_{t-1}^2 + v_t\varepsilon_{t-1} \\[6pt]
\mathbb{E}(\varepsilon_t\varepsilon_{t-1}) &= \mathbb{E}(\rho\varepsilon_{t-1}^2 + v_t\varepsilon_{t-1}) \\[6pt]
Cov(\varepsilon_t,\varepsilon_{t-1}) = \mathbb{E}(\varepsilon_t\varepsilon_{t-1}) &= \rho\mathbb{E}(\varepsilon_{t-1}^2) + \mathbb{E}(v_t\varepsilon_{t-1}) \\[6pt]
Cov(\varepsilon_t,\varepsilon_{t-1}) &= \rho\mathbb{E}(\varepsilon_{t-1}^2) \ \ \ \ \ \ \text{since } Cov(v_t,\varepsilon_{t-1})=0 \\[6pt]
Cov(\varepsilon_t,\varepsilon_{t-1}) &= \rho\frac{\sigma_v^2}{1 - \rho^2} \ \ \ \ \ \ \text{since } Var(\varepsilon_t) = \frac{\sigma_v^2}{1 - \rho^2}.
\end{align*}
We can then continue in this fashion for the further past periods:
\begin{align*}
Cov(\varepsilon_t,\varepsilon_{t-2}) &= \rho^2 \frac{\sigma_v^2}{1 - \rho^2} \\[6pt]
Cov(\varepsilon_t,\varepsilon_{t-3}) &= \rho^3 \frac{\sigma_v^2}{1 - \rho^2} \\
\vdots
\end{align*}

Since $|\rho|\neq 0$, then $Cov(\varepsilon_t,\varepsilon_{t-1})\neq 0$.

\bigskip

\item[Answer (iii):] 
Since the correlation coefficient is the ratio of covariance to variance, we get
\[
Corr(\varepsilon_t, \varepsilon_{t-1}) = \frac{Cov(\varepsilon_t,\varepsilon_{t-1})}{Var(\varepsilon_t)} = \frac{\rho\displaystyle\frac{\sigma_v^2}{1 - \rho^2}}{\displaystyle\frac{\sigma_v^2}{1 - \rho^2}} = \rho
\]
and in future periods a similar approach would yield $Corr(\varepsilon_t, \varepsilon_{t-2}) = \rho^2, \ Corr(\varepsilon_t, \varepsilon_{t-3}) = \rho^3, \dots$
\end{description}




\bigskip\bigskip
***
\bigskip\bigskip

### (c) Using the data from the worksheet 'Autocorrelation' estimate the following
\begin{equation}\label{eq:SQ3c}
gprice = \beta_0 + \beta_1 \ gwage_t + u_t
\end{equation}

\begin{description}
\item[Answer:]
The question is asking for us to regress "growth of price" on "growth of wages". First, lets import the data and look at its summary.
\end{description}

In R:

```{r results = 'hide'}
wage_autocorr_df <- read_excel("../Data/sup4.xls", sheet = "autocorrelation")
str(wage_autocorr_df)
```

and in STATA via the command `describe`.

```{stata eval = !knitr::is_latex_output()}
/* load the data and describe it*/
  quietly cd ..
  quietly import excel using Data/sup4.xls, sheet("autocorrelation") firstrow

  describe
```

In R we see that the all the variables that start with $g$, i.e. all the growth columns, are coded as characters and in STATA the same are called strings. The data also seems to have missing values so we need to remove those as well.

In R we can do this with

```{r warning=FALSE, results='hide'}
# convert all characters to numeric
wage_autocorr_df <- wage_autocorr_df %>% mutate_if(is.character, as.numeric)

# remove N/As from `gprice` and `gwage` only
wage_autocorr_df <- wage_autocorr_df[!with(wage_autocorr_df, 
                                                   is.na(gprice) & is.na(gwage)),]

# look at the structure of the data again to confirm
str(wage_autocorr_df)
```

and we see that the number of observations for `gprice` and `gwage` now have reduced to 285 each.

The same can be done in STATA with

```{stata eval = !knitr::is_latex_output()}
/* load the data*/
  quietly cd ..
  quietly import excel using Data/sup4.xls, sheet("autocorrelation") firstrow
  
/* change from string to numeric, and replace N/As */
  quietly destring, replace
```

Now we can run the regression. In R

```{r}
SQ3c_lm <- lm(gprice ~ gwage, data = wage_autocorr_df)
summary(SQ3c_lm)
```

and in STATA

```{stata eval = !knitr::is_latex_output()}
/* load the data*/
  quietly cd ..
  quietly import excel using Data/sup4.xls, sheet("autocorrelation") firstrow
  
/* change from string to numeric, and replace N/As */
  quietly destring, replace
  
/* run the regression */
  regress gprice gwage
```


### (d) Test for serial correlation by (i) computing the Durbin-Watson $d$ statistic, (ii) running a regression of $\hat{u}_t$ on $\hat{u}_{t-1}$ using residuals from part (c), (iii) using the Breusch-Godfrey test in Stata, (iv) using the alternative Durbin test in Stata (durbinalt). Discuss the relative advantages of each method.

\begin{description}
\item[Answer (i):]
A commonly used test for AR(1) autocorrelation is the Durbin-Watson $d$ test\footnote{Durbin, J and Watson G S (1951) "Testing for Serial Correlation in Least-Squares Regression" \textit{Biometrika} 38:159-177.} which is based on the OLS residuals. 

\begin{tcolorbox}[breakable, title=Durbin-Watson $d$ test, skin=enhancedlast]

The $d$ statistic is the ratio of the sum of squared differences between successive residuals to the RSS:
\[
DW = d = \frac{\displaystyle\sum_{i=2}^n(\hat{u}_t - \hat{u}_{t-1})^2}{\sum_{i=1}^n\hat{u_t}^2}
\]
Note that the number of observations in the numerator of the $d$ statistic is one less than the numerator, i.e. $n-1$, because one observation is lost in taking successive differences.

\bigskip

\underline{Test procedure:}

Unlike the $t, F$ or $\chi^2$ tests, there is no unique critical value against which we would compare the $d$ statistic for rejection decision. This is because, it is difficult to derive probability distribution of $d$ statistic since it depends on $\hat{u}$ which in turn depends on the $X$s.

To understand the decision process first expand the $d$ statistic to get
\[
d = \frac{\displaystyle\sum_{i=2}^n\hat{u}_t^2 + \displaystyle\sum_{i=2}^n\hat{u}_{t-1}^2 - 2\displaystyle\sum_{i=2}^n\hat{u}_t\hat{u}_{t-1}}{\displaystyle\sum_{i=1}^n\hat{u}_t^2}
\]

One thing to notice is that $\sum\hat{u}_t^2$ and $\sum\hat{u}_{t-1}^2$ differ in only one observation, so they are approximately equal. That is, $\sum\hat{u}_t^2 \approx \sum\hat{u}_{t-1}^2$. We can use this to rewrite the expression for $d$ statistic:
\[
d \approx 1 + 1 - 2\left( \frac{\displaystyle\sum_{i=2}^n\hat{u}_t\hat{u}_{t-1}}{\displaystyle\sum_{i=1}^n\hat{u}_t^2} \right) = 2\left(1 - \frac{\displaystyle\sum_{i=2}^n\hat{u}_t\hat{u}_{t-1}}{\displaystyle\sum_{i=1}^n\hat{u}_t^2} \right)
\]
Also recall that in part (a) when discussing why OLS with autocorrelation is not BLUE, we noted that the definition of population correlation between $u_t$ and $u_{t-1}$ is
\[
\rho 
= \frac{\mathbb{E}\Big( \big[ u_t - \mathbb{E}(u_t) \big] \big[ u_{t-1} - \mathbb{E}(u_{t-1}) \big] \Big) }{\sqrt{Var(u_t)}\sqrt{Var(u_{t-1})}} 
= \frac{\mathbb{E}(u_tu_{t-1})}{Var(u_{t-1})}
\]
the latter equality is due to $\mathbb{E}(u_t) = 0$ and $Var(u_t) = Var(u_{t-1})$ because of the homoskedasticity assumption. $\rho$ is also the slope coefficient in the regression of $u_t$ on $u_{t-1}$.

If we define $\hat{\rho}$ as
\[
\hat{\rho} = \frac{\displaystyle\sum_{i=2}^n\hat{u}_t\hat{u}_{t-1}}{\displaystyle\sum_{i=1}^n\hat{u}_t^2}
\]
then the $d$ statistic can be expressed as
\[
d \approx 2(1-\hat{\rho})
\]
Since $-1 \leq \hat{\rho} \leq 1$, this means the bounds of $d$ are
\[
0 \leq d \approx 2(1-\hat{\rho}) \leq 4.
\]
That is, any estimated $d$ statistic has to be between $0$ and $4$. We can use this for decision heuristics:
\begin{itemize}
\item If there are no autocorrelation in the residuals then $\hat{\rho} = 0$ and $d \approx 2$. Therefore, if $d=2$ we can assume that there is no first-order autocorrelation;
\item If there is perfect positive autocorrelation in the residuals then $\hat{\rho}=1$ and $d \approx 0$. Therefore, the closer $d$ is to $0$, the greater evidence of positive serial correlation.
\item If there is perfect negative autocorrelation in the residuals then $\hat{\rho}=-1$ and $d \approx 4$. Therefore, the closer $d$ is to $4$, the greater evidence of negative serial correlation.
\end{itemize}
Durbin-Watson then derived a lower bound $d_L$ and an upper bound $d_U$ within these limits of $0$ and $4$ such that if the computed $d$ statistic lies outside of these bounds a decision can be made regarding the presence of negative or positive serial correlation as below:
\bigskip
\begin{center}
\begin{tabular}{| l | c | c |}
\hline
If & Decision & Null Hypothesis \\
\hline\hline
$0 < d < d_L$ & Reject & No positive autocorrelation \\
$d_L \leq d \leq d_U$ & No decision & No positive autocorrelation \\
$4-d_L < d < 4$ & Reject & No negative autocorrelation \\
$4-d_u \leq d \leq 4-d_L$ & No decision & No negative autocorrelation \\
$d_U < d < 4 - d_U$ & Do not reject & No autocorrelation \\
\hline
\end{tabular}
\end{center}

The upper and lower bounds depend on the number of observations, $n$, and the number of explanatory variables only. They do not depend on the values they take.

Accordingly, the steps for Durbin-Watson test is as follows:
\begin{enumerate}
\item Run the OLS and obtain the residuals;
\item Calculate $d$;
\item For the given sample size $n$ and the number of explanatory variables, $k$, obtain the critical values $d_L$ and $d_U$ from the manually published tables.
\item Follow the decision rules given in the table above.
\end{enumerate}

\bigskip

\underline{Assumptions of $d$ statistic:}
\begin{enumerate}
\item The regression model includes the intercept term;
\item The explanatory variables, i.e. the $X$s, are nonstochastic, or fixed in repeated sampling;
\item The disturbances are generated by AR(1) scheme, i.e. it cannot be used to detect higher order autoregressive schemes;
\item The error term $u_t$ is assumed to be normally and i.i.d. distributed;
\item The regression model does not include lagged value(s) of the dependent variable as one of the explanatory variables, i.e. the test cannot be used for autoregressive models;
\item There are no missing observations in the data.
\end{enumerate}

\bigskip

\underline{Drawbacks of DW Test:}
\begin{itemize}
\item It is only applicable when the above assumptions hold true;
\item It cannot be used for testing for higher-autocorrelations than first-order;
\item It contains zones of indecision where presence or absence of aotocorrelation cannot be conclusively determined; though the indecision zone narrows as the sample size increases;
\item If a regression model contains lagged values of the regressand, the $d$ value in such cases is often around 2, which would suggest that there is no first-order autocorrelation in such models. Thus, there is a built-in bias against discovering first-order autocorrelation in such models.

\end{itemize}

\underline{Modified $d$ test:}
In many situations it has been found that the upper limit $d_U$ is approximately the true significance limit. So if $d$ lies in the indecision zone, we can use the following \textit{modified $d$ test} with the given level of significance $\alpha$:
\begin{itemize}
\item $\mathbb{H}_0:\ \rho=0$ vs. $\mathbb{H}_1:\ \rho>0$
\subitem Reject $\mathbb{H}_0$ at $\alpha$ if $d<d_U$. Thus, there is statistically significant positive autocorrelation.
\item $\mathbb{H}_0:\ \rho=0$ vs. $\mathbb{H}_1:\ \rho<0$
\subitem Reject $\mathbb{H}_0$ at $\alpha$ if $(4-d)<d_U$. Thus, there is statistically significant negative autocorrelation.
\item $\mathbb{H}_0:\ \rho=0$ vs. $\mathbb{H}_1:\ \rho=0$
\subitem Reject $\mathbb{H}_0$ at $2\alpha$ level if $d<d_U$ or $(4-d)<d_U$. Thus, there is statistically significant autocorrelation, positive or negative.
\end{itemize}
\end{tcolorbox}

\bigskip

We have already completed step 1 above in part (c) sn we can now move to step 2 and compute the Durbin-Watson statistic.

For this, in R we use `durbinWatsonTest()` function from the `car` package, or `dwtest` from the `lmtest` package:

\end{description}

```{r}
car::durbinWatsonTest(SQ3c_lm)
dwtest(SQ3c_lm)
```



which gives us $d=0.8032315$ with a $p$-value of essentially $0$. 

Step 3 involves looking up the Durbin-Watson critical value table. It is difficult to find exact $n=285$ but we can approximate. For $n=280, k=2$ we see that the critical values are $d_L = 1.7969$ and $d_U=1.81123$, and for $n=290, k=2$ we see that the critical values are $d_L = 1.80053$ and $d_U=1.81436$. The values for $n=285,k=2$ will be somewhere between these. In any case, Since $d=0.8032315$ is closer to 0 and lower than either of the $d_L$, i.e. $0 < d < d_L$, we reject the null hypothesis and conclude that there seems to be a positive autocorrelation. 


The same calculation in STATA can be done via the `estat dwatson` command

```{stata eval = !knitr::is_latex_output()}
/* load the data*/
  quietly cd ..
  quietly import excel using Data/sup4.xls, sheet("autocorrelation") firstrow

/* change from string to numeric, and replace N/As */
  quietly destring, replace
  
/* run the regression */
  quietly regress gprice gwage
  
/* run Durbin-Watson test */
  estat dwatson
```



\bigskip

\begin{description}
\item[Answer (ii):]
The second part of the question is asking us to regress the residuals to their one-lagged values.
\end{description}

In R:

```{r}
SQ3d_ii_lm <- lm(SQ3c_lm$residuals ~ lag(SQ3c_lm$residuals, n=1))
summary(SQ3d_ii_lm)
```

Since $d = 2(1 - \hat{\rho})$ and since $\hat{\rho}$ is the slope of the regression coefficient of $u_t$ on $u_{t-1}$ - here $0.5945888$ - we can calculate the $d$ statistic as follows:

```{r}
# rho is the slope coefficient
rho <- SQ3d_ii_lm$coefficients[2]

# calculate d
2*(1 - rho)
```

Here we get a very similar $d$ value of $0.8108223$ to $0.8032315$ obtained in part (i), and would conclude the same way as we did in part (i) - reject the null hypothesis of no positive autocorrelation.


\bigskip

\begin{description}
\item[Answer (iii):] 
To avoid some of the pitfalls of the Durbin-Watson $d$ test of autocorrelation, a general test has been developed by Breusch-Godfrey.\footnote{Breusch, T S (1978) "Testing for Autocorrelation in Dynamic Linear Models", \textit{Australian Economic Papers} 17:334-355 and Godfrey, L G (1978) "Testing Against General Autoregressive and Moving Average Error Models When the Regressor Includes Lagged Dependent Variables", \textit{Econometrica} 46:1293-1302.} The BG test can also be used if we think $gwage$ is not strictly exogenous - a condition for the DW test. 

\begin{tcolorbox}[breakable, title=Breusch-Godfrey (BG) test, skin=enhancedlast]

Also known as Lagrange multiplier (LM) test, the BG test is a general test in the sense that it allows for
\begin{itemize}
\item nonstochastic regressors, such as the lagged values of the dependent variable;
\item higher order autoregressive schemes such as AR(2), AR(3) etc; and
\item simple or higher order \textit{moving averages} of white noise errors terms; e.g. in regression $Y_t = \beta_1 + \beta_2 X_t + u_t$, the error term can be represented as $u_t = \varepsilon_t + \eta_1\varepsilon_{t-1} + \eta_2\varepsilon_{t-2} + \dots + \eta_p\varepsilon_{t-p}$, which represents a $p$-period moving average of the white noise error term $\varepsilon_t$.
\end{itemize}

\bigskip

\underline{Test procedure}

Although the discussion can be extended to multiple regression, consider the simple regression model $Y_t = \beta_0 + \beta_1 X_t + u_t$ with the error term following the $p^{th}$-order autoregressive, AR($p$),\footnote{if $p=1$ then this test is called \textit{Durbin's $M$ test}.} scheme:
\[
u_t = \rho_1u_{t-1} + \rho_2u_{t-2} + \dots + \rho_pu_{t-p} + \beta_1X_{1t} + \beta_2X_{2t} +\dots+ \beta_kX_{kt} + \varepsilon_t
\]
where $\varepsilon_t$ is the white noise error term. The null hypothesis is that there is no autocorrelation of any order:
\[
\mathbb{H}_0: \rho_1 = \rho_2 = \dots = \rho_p = 0
\]
The BG or LM test is carried out via the following steps:
\begin{enumerate}
\item Run the OLS and obtain the residuals;
\item Regress the residuals, $\hat{u}_t$, on the original $X_t$ and on its lagged values $\hat{u}_{t-1}, \hat{u}_{t-2}, \dots, \hat{u}_{t-p}$. If there are more than one $X$ variable in the original model, then include them in this auxiliary regression also;
\item Obtain $R^2$ from this auxiliary regression;
\item Since this auxiliary regression of the residuals have $(n-p)$ observations given that $p$ of them are used up in the model, Breusch and Godfrey have shown that $R^2$ of this regression multiplied by the sample size $(n-p)$ asymptotically follows the chi-square distribution with $p$ degrees of freedom. Therefore, if $(n-p)R^2$ exceeds the critical $\chi_p^2$ at the chosen level of significance, we reject the null hypothesis and conclude that at least one $\rho$ is statistically significantly different from zero.
\[
(n-p)R^2 \sim \chi_p^2
\]
\end{enumerate}


\underline{Drawbacks:}

One drawback of the BG test is that the length of lag, $p$, cannot be specified a priori, which means some experimentation with $p$ happens. Sometimes, Akaike and Schwarz information criteria can be used to select the lag length.

Another drawback is that the test assumes that the variance of disturbance is homoskedastic, i.e. $Var(u_t)=\sigma^2$.
\end{tcolorbox}

\bigskip

To conduct the Breusch-Godfrey test for this question, we can follow these steps. step 1 has aleady been completed in part (i), so we will continue from step 2 onwards. 

This can be done in R via `bgtest()` function in `lmtest` library:
\end{description}

```{r}
bgtest(SQ3c_lm, order=1, data = wage_autocorr_df)
```

which gives us a $\chi^2$ statistic of $105.4$ with a $p$ value of essentially $0$. The critical value at $\alpha=0.05$ is $\chi_1^2 = 3.84146$. Since our test statistic exceeds that, we can reject the null hypothesis and conclude that $\rho_1 \neq 0$ at 95% significance.

The same can be done in STATA using the `bgodfrey` command:

```{stata eval = !knitr::is_latex_output()}
/* load the data*/
  quietly cd ..
  quietly import excel using Data/sup4.xls, sheet("autocorrelation") firstrow
  
/* change from string to numeric, and replace N/As */
  quietly destring, replace
  
/* run the regression */
  quietly regress gprice gwage
  
/* set variable `t` in the spreadsheet as time variable */
  quietly tsset t

/* run the Breusch-Godfrey test */  
  bgodfrey
```



\bigskip

\begin{description}
\item[Answer (iv):] 
This final part of the question asks us to run the alternative Durbin test. The box below discusses the test briefly.

\begin{tcolorbox}[breakable, title=Durbin's alternative test, skin=enhancedlast]

Durbin (1970)\footnote{Durbin, J (1970) \textit{Testing for serial correlation in least-squares regressions when some of the regressors are lagged dependent variables.} Econometrica 38: 410–421} suggested an alternative test for models with lagged dependent variables. When lagged dependent variables are included among the regressors, the past values of the error term are correlated with those lagged variables at time $t$, implying that they are not strictly exogenous regressors. The inclusion of covariates that are not strictly exogeneous causes $d$ statistic to be biased toward the acceptance of the null hypothesis.

Durbin's alternative test is in fact a Lagrange multiplier (LM) test but it is most easily computed with a Wald test on the coefficients of the lagged residuals in an auxiliary OLS regression. The auxiliary OLS regression regresses the residuals on their lags and all the explanatory variables in the original regression.

Consider the linear regression model
\[
Y_t = \beta_1 X_{1t} + \dots + \beta_k X_{kt} + u_t
\]
where the covariates $X_1$ through $X_k$ are not assumed to be strictly exogeneous\footnote{Note that when there are only strictly exogeneous regressors and $p=1$, then this test is asymptotically equivalent to Durbin-Watson test.} and $u_t$ is assumed to be i.i.d. with finite variance. The process is also assumed to be stationary.

\underline{Test procedure}
\begin{enumerate}
\item Run the OLS and obtain the residuals;
\item Run the auxiliary regression whereby the residuals $\hat{u}_t$ are regressed on its lagged values and on the other regressors:
\[
\hat{u}_t = \eta_1\hat{u}_{t-1}+\dots+\eta_p\hat{u}_{t-p}+\beta_1X_{1t}+\dots+\beta_kX_{kt} + \varepsilon_t
\]
\item Durbin's alternative test is obtained by performing a Wald test on the null hypothesis that $\mathbb{H}_0:\eta_1 = \dots = \eta_p = 0$ 
\end{enumerate}

Durbin's alternative test and and the Breusch-Godfrey test are asymptotically equivalent.
\end{tcolorbox}

In STATA we can run this test as follows:
\end{description}


```{stata}
/* load the data*/
  quietly cd ..
  quietly import excel using Data/sup4.xls, sheet("autocorrelation") firstrow
  
/* change from string to numeric, and replace N/As */
  quietly destring, replace
  
/* run the regression */
  quietly regress gprice gwage

/* set variable `t` in the spreadsheet as time variable */
  quietly tsset t

/* run Durbin's alternative test */  
  estat durbinalt
```

which gives us a $\chi^2$ value of $165.492$.

The table below compares the results of each of the test:

\begin{center}
\begin{tabular}{|| c | c | c ||}
\hline
Test & Statistic & Result \\
\hline\hline
Durbin Watson  & $d=0.803$ & Reject null \\
Manual calculation of DW  & $d=0.810$ & Reject null \\
Breusch Godfrey & $\chi^2=105.39$ & Reject null \\
Durbin's alternative & $\chi^2=165.49$ & Reject null \\
\hline
\end{tabular}
\end{center}

Given that $X$s are not exogeneous, BG test is more appropriate here. However, it $F$-tests all slopes including all $X$s which is not really that interesting. So it may be preferable to use Durbin's alternative test instead, which is almost identical to the BG test but it does not test whether all the slope coefficients are zero, only whether the coefficients of the lagged residuals are zero.




\bigskip\bigskip
***
\bigskip\bigskip

### (e) Estimate the model using the Cochrane-Orcutt (two step) method. Check that these results are the same as those obtained by using the `prais` command in Stata (with the `corc` and `twostep` options). Does any autocorrelation remain?

\begin{description}
\item[Answer:]
This question is looking at the ways in which we can deal with autocorrelation, especially the lack of efficiency of the OLS estimators. Consider a simple regression model $Y_t = \beta_0 + \beta_1 X_t + u_t$ with the errors following the AR(1) process
\[
u_t = \rho u_{t-1}+\varepsilon_t
\]
Since there is autocorrelation in the disturbance term, we can transform the equation to try to eliminate the problem. We can do this in such a way that the error term of our transformed equation is the white noise $\varepsilon_t$. To obtain that, the error term of the transformed equation then has to equal to $u_t-\rho u_{t-1}$. Therefore, for $t \geq 2$ we can lag the equation by one period, multiply it by $\rho$, and subtract it from the original equation.
\begin{align*}
\rho Y_{t-1} &= \beta_0 \rho + \beta_1 \rho X_{t-1} + \rho u_{t-1} \\
Y_t - \rho Y_{t-1} &= \beta_0 - \beta_0 \rho + \beta_1 X_t - \beta_1 \rho X_{t-1} + u_t - \rho u_{t-1} \\
Y_t - \rho Y_{t-1} &= \beta_0(1 - \rho) + \beta_1 (X_t - \rho X_{t-1}) + \varepsilon_t \\
Y_t^* &= \beta_0^* + \beta_1 X_t^* + \varepsilon_t
\end{align*}
where $Y_t^* = Y_t - \rho Y_{t-1} \ , \ X_t^* = X_t - \rho X_{t-1} \ , \ \beta_0^* = \beta_0(1-\rho)$, and $-1 < \rho < 1$. With the white noise error, the model is now free from autocorrelation, as desired.

With multiple regressors with $u_t$ following an AR(1) process, the transformed model can be expressed as
\[
Y_t^* = \beta_0^* + \beta_1 X_{1t}^* + \beta_2 X_{2t}^* + \dots + \beta_k X_{kt}^* + \varepsilon_t.
\]
Since now the error term satisfies the Gauss-Markov assumptions which means that if we knew $\rho$, we could estimate $\beta_0*$ and $\beta_1$ by regressing $Y_t^*$ on $X_t^*$. In effect, running $Y_t^* = \beta_0^* + \beta_1 X_t^* + \varepsilon_t$ is tantamount to using generalized least squares (GLS) discussed in the Supplementary Questions 1(b) above, where it was posited that GLS is the application of OLS to a transformed model that satisfies the Gauss-Markov assumptions.

This equation, $Y_t^* = \beta_0^* + \beta_1 X_t^* + \varepsilon_t$, is known as \textit{generalized difference equation}, or \textit{quasi-differenced equation}. 

\begin{tcolorbox}[breakable, title=Prais-Winsten transformation, skin=enhancedlast]

Notice that in the differencing procedure above we loose one observation because the lagged variables are not defined for the first observation.This loss of observation may not be a too serious in large samples but can make considerable difference in the results of small samples. However, the OLS estimators from $Y_t^* = \beta_0^* + \beta_1 X_t^* + \varepsilon_t$ is not quite BLUE because the first period is not used.

A quick and straight-forward fix for this is to write the equation for $t=1$ as
\[
Y_1 = \beta_0 + \beta_1 X_1 + u_1.
\]
This is a fix because the error term of the first observation, $u_1$, is uncorrelated with the white noise error $\varepsilon_t$. Therefore we can add this equation to $Y_t^* = \beta_0^* + \beta_1 X_t^* + \varepsilon_t$. 

However, this introduces a problem. With the inclusion of the first observation, the error variances will not be homoskedastic. This is because the variance of $u_t$ is
\[
Var(u_t) = \frac{\sigma_\varepsilon^2}{1-\rho^2}
\]
as derived in Supplementary Question 3(b). Since $|\rho|<1$, 
\[
Var(u_t) = \frac{\sigma_\varepsilon^2}{1-\rho^2} > \sigma_\varepsilon^2 = Var(\varepsilon_t)
\]
In order to equate the two variances therefore, we need to adjust the equation of the first observation by $\sqrt{1-\rho^2}$ whereby
\[
\sqrt{1-\rho^2} \ Y_1 = \beta_0 \sqrt{1-\rho^2} + \beta_1 \sqrt{1-\rho^2} \  X_1 + \sqrt{1-\rho^2} \ u_1
\]
so that the error variance of this equation becomes
\[
Var(\sqrt{1-\rho^2} \ u_1) = (1-\rho^2)Var(u_t) = (1-\rho^2) \frac{\sigma_\varepsilon^2}{1-\rho^2} = \sigma_\varepsilon^2 = Var(\varepsilon_t).
\]
Adding this adjusted equation for first observation to $Y_t^* = \beta_0^* + \beta_1 X_t^* + \varepsilon_t$ then yields OLS estimators that are BLUE.
\end{tcolorbox}

\bigskip

One of the points that was made above was that if we knew $\rho$, we could estimate $\beta_0*$ and $\beta_1$ by regressing $Y_t^*$ on $X_t^*$. Cochrane-Orcutt method is a way to estimate $\rho$.

\bigskip

\begin{tcolorbox}[breakable, title=Cochrane-Orcutt Two-Step Procedure, skin=enhancedlast]

Early regression applications tended to use the Cochrane-Orcutt iterative process to provide an estimate for $\rho$. One advantage of this process is that it can be used to estimate not only AR(1) scheme but higher order autoregressive schemes such as $\hat{u}_t = \hat{\rho}_1\hat{u}_{t-1} + \hat{\rho}\hat{u}_{t-2} + v_t$, which is AR(2).

The procedure for this method is as follows:
\begin{enumerate}
\item Run the OLS on the untransformed model;
\item Regress $\hat{u}_t = \rho\hat{u}_{t-1} + \varepsilon_t$ to obtain an estimate of $\rho$;
\item Use this estimation, $\hat{\rho}$, to fit $Y_t^* = \beta_0^* + \beta_1 X_t^* + \varepsilon_t$.    
\end{enumerate}

By using two linear regressions, we can obtain a BLUE estimate.
\end{tcolorbox}

\bigskip

For this question, therefore we need to transform the following model:
\begin{align*}
{gprice}_t^* &= \beta_0^* + \beta_1 {gwage}_t^* + \varepsilon_t  \\
{gprice}_t - \hat{\rho} \ {gprice}_{t-1} &= \beta_0 (1-\hat{\rho}) + \beta_2 ({gwage}_t - \hat{\rho} \ {gwage}_{t-1}) + u_t - \hat{\rho}u_{t-1}
\end{align*}

In supplementary question 3(d)(ii) above we already obtained an estimate for $\rho$ as $0.5945888$. We can use this to transform the model and run the regression.
\end{description}

In R:

```{r}
# transform the data
wage_autocorr_df <- wage_autocorr_df %>%
  mutate(gprice_star = gprice - rho*lag(gprice, 1),
         gwage_star = gwage - rho*lag(gwage,1),
         beta0_star = 1-rho)

# next we need to adjust this for the first observation
wage_autocorr_df$gprice_star[wage_autocorr_df$t == 2] <- 
  sqrt((1- rho^2))*wage_autocorr_df$gprice[wage_autocorr_df$t == 2]

wage_autocorr_df$gwage_star[wage_autocorr_df$t == 2] <- 
  sqrt((1- rho^2))*wage_autocorr_df$gwage[wage_autocorr_df$t == 2]

wage_autocorr_df$beta0_star[wage_autocorr_df$t == 2] <- 
  sqrt((1- rho^2))

# run the regression on the transformed variables
SQ3e_lm <- lm(gprice_star ~ gwage_star + beta0_star + 0, data = wage_autocorr_df)
summary(SQ3e_lm)

# or alternatively use `prais_winsten()` formula from `prais` package
library(prais)
prais_winsten(gprice ~ gwage, data = wage_autocorr_df, index = "t", twostep = TRUE)
```

and in STATA:

```{stata eval = !knitr::is_latex_output()}
/* load the data*/
  quietly cd ..
  quietly import excel using Data/sup4.xls, sheet("autocorrelation") firstrow
  
/* change from string to numeric, and replace N/As */
  quietly destring, replace
  
/* transform the variables */
  quietly generate gprice_star = gprice - 0.5945888 * gprice_1
  quietly generate gwage_star = gwage - 0.5945888 * gwage_1
  quietly generate beta0_star = 1 - 0.5945888
  
/* adjust the first observation */
  quietly replace gprice_star=((1 - 0.5945888^2)^(1/2))*gprice if t==2
  quietly replace gwage_star=((1 - 0.5945888^2)^(1/2))*gwage if t==2
  quietly replace beta0_star=((1 - 0.5945888^2)^(1/2)) if t==2
    
/* run the regression */
  regress gprice_star gwage_star beta0_star, noc
```

Which can instead be done automatically with the `prais` command in STATA:

```{stata}
/* load the data*/
  quietly cd ..
  quietly import excel using Data/sup4.xls, sheet("autocorrelation") firstrow
  
/* change from string to numeric, and replace N/As */
  quietly destring, replace
  
/* set the time index with volumn "t" */
  quietly tsset t
  
/* run the `prais` command */  
  prais gprice gwage, twostep
```

and to obtain the Cochrane-Orcutt Two-Step Procedure:
```{stata}
/* load the data*/
  quietly cd ..
  quietly import excel using Data/sup4.xls, sheet("autocorrelation") firstrow
  
/* change from string to numeric, and replace N/As */
  quietly destring, replace
  
/* set the time index with volumn "t" */
  quietly tsset t
  
/* run the `prais` command */  
  prais gprice gwage, corc twostep
```


Thus, we obtain $\beta_1 = 0.0388568$ and $\beta_0(1 - 0.5945888) = 0.0018121$, which means $\beta_0 = 0.00446978$.

Notice that the transformed DW $d$ statistic is $2.194615$. The critical values are $d_L = 2.1887$ and $d_U = 2.203$ which means this falls within the "No decision" rule.

To see if we reduced autocorrelation we can first regress the residuals from the transformed model's regression against their 1-lagged values, i.e. $\varepsilon_t = \gamma_0 + \gamma_1 \varepsilon_{t-1} + v_t$:

```{r}
summary(lm(SQ3e_lm$residuals ~ lag(SQ3e_lm$residuals, 1)))
```

The estimate of the coefficient of the lagged value, $\gamma_1$, does not seem to be significant since it is significant only at $\alpha=0.05$. We can check if this is also the case for higher-order lags, i.e. $\varepsilon_t = \gamma_0 + \gamma_1 \varepsilon_{t-1} + \gamma_2 \varepsilon_{t-2} + \gamma_3 \varepsilon_{t-3} + v_t$:

```{r}
summary(lm(SQ3e_lm$residuals ~ lag(SQ3e_lm$residuals, 1) 
           + lag(SQ3e_lm$residuals, 2) + lag(SQ3e_lm$residuals, 3)))
```

We see that the second and third lags, $\gamma_2$ and $\gamma_3$, are significant at $\alpha=0.01$. which suggests there may be AR(2) and AR(3) process in the error terms. We can also cheeck this via Breusch-Godfrey test:

```{r}
bgtest(SQ3e_lm, order = 1, data = property_autocorr_df)
bgtest(SQ3e_lm, order = 2, data = property_autocorr_df)
bgtest(SQ3e_lm, order = 3, data = property_autocorr_df)
```

which gives a $\chi^2$ values of $8.8743$ and $16.137$ for lags 2 and 3, with $p$ values suggesting significance at least at $\alpha=0.05$ level.

So based on the above tests it is possible that some autocorrelation may still be present.

The STATA equivalent of above commands are:

```{stata  eval = !knitr::is_latex_output()}
/* load the data*/
  quietly cd ..
  quietly import excel using Data/sup4.xls, sheet("autocorrelation") firstrow
  
/* change from string to numeric, and replace N/As */
  quietly destring, replace
  
/* transform the variables */
  quietly generate gprice_star = gprice - 0.5945888 * gprice_1
  quietly generate gwage_star = gwage - 0.5945888 * gwage_1
  
/* run the regression */
  quietly regress gprice_star gwage_star
  
/* regress residuals against its lags */
  quietly tsset t
  predict U, residuals 
  regress U L.U L2.U L3.U
  
/* Run Breusch-Godfrey manually */  
  display e(N)*e(r2)
  
/* run regression with residuals and regressors */
  regress gprice_star gwage_star L.U L2.U L3.U 
  
/* Run Breusch-Godfrey */
  bgodfrey, lags (1 2 3)
```




\bigskip\bigskip
***
\bigskip\bigskip

### (f) Use the 'ac' command to graph the autocorrelations of the residuals from equation (\ref{eq:SQ3c}) on page \pageref{eq:SQ3c} that you used in part (c), and comment on the relevance of this graph for the results above.

```{stata  eval = !knitr::is_latex_output()}
/* load the data*/
  quietly cd ..
  quietly import excel using Data/sup4.xls, sheet("autocorrelation") firstrow
  
/* change from string to numeric, and replace N/As */
  quietly destring, replace
  
/* transform the variables */
  generate gprice_star = gprice - 0.5945888 * gprice_1
  generate gwage_star = gwage - 0.5945888 * gwage_1
  
/* run the regression */
  quietly regress gprice_star gwage_star
  
/* regress residuals against its lags */
  quietly tsset t
  predict U, residuals
  
/* generate autocorrelations and obtain the graph */
  quietly corrgram U, lags(40)
  ac U, lags(40)

```

and in R we use the `Acf()` function from the `forecast` package:

```{r message=FALSE}
library(forecast)
acf(SQ3c_lm$residuals)
```

Which shows that there is a lot of autocorrelation present in the residuals of equation \ref{eq:SQ3c}, and much of it is of a higher order than the ones we have attempted to deal with so far.

Note that the dashed lines are the Bartlett's standard errors for MA(q) 95% confidence intervals.



\bigskip\bigskip
***
\bigskip\bigskip

### (g) Now estimate the following distributed lag model:
\begin{equation}\label{eq:SQ3g}
{gprice}_t = \beta_0 + \beta_1 {gwage}_t + \beta_2 {gwage}_{t-1} + \dots + \beta_13 {gwage}_{t-12} + \varepsilon_t
\end{equation}
\textbf{Comment on your results. Is there autocorrelation in this model?}

\begin{description}
\item[Answer:]
The spreadsheet has these lagged values as columns already. So we just need to regress them.
\end{description}

In R:

```{r}
SQ3g_lm <- lm(gprice ~ gwage + gwage_1 + gwage_2 + gwage_3 + gwage_4 + gwage_5 
              + gwage_6 + gwage_7 + gwage_8 + gwage_9 + gwage_10 + gwage_11 
              + gwage_12, data = property_autocorr_df)
Acf(SQ3g_lm$residuals)
```

We can see that the autocorrelation is reduced but not eliminated, especially in lower lags. 

As before, we can also use Breusch Godfrey to check for autocorrelation:

```{r}
bgtest(SQ3g_lm)
```

with $chi^2=70.241$ and a $p$ value that is practically zero, we would reject the null hypothesis that there is no autocorrelation in the residuals.



\bigskip\bigskip
***
\bigskip\bigskip

### (h) Run model (\ref{eq:SQ3g}) on page \pageref{eq:SQ3g} again, this time including the residuals from this equation as lagged values. Experiment with the number of lagged terms you include. How many lagged terms are required to make the autocorrelation disappear?

```{r eval = FALSE}
SQ3h_lm <- update(SQ3g_lm, . ~ . + SQ3g_lm_residuals + lag(SQ3g_lm_residuals, 1))
Acf(SQ3h_lm$residuals)
```

```{stata}
  quietly cd ..
  quietly import excel using Data/sup4.xls, sheet("autocorrelation") firstrow
  quietly destring, replace
  quietly tsset t
  quietly regress gprice gwage gwage_1 gwage_2 gwage_3 gwage_4 gwage_5 gwage_6 gwage_7 gwage_8 gwage_9 gwage_10 gwage_11 gwage_12
  
  predict E, residuals
  
/* run the regression again with its residuals and their lags */
  
  regress gprice gwage gwage_1 gwage_2 gwage_3 gwage_4 gwage_5 gwage_6 gwage_7 gwage_8 gwage_9 gwage_10 gwage_11 gwage_12 L.E L2.E 
    
/* Breusch Godfrey */
  bgodfrey, lags (1 2 3 4)
    
/* Durbin's alternative test */
  estat durbinalt, lags (1 2 3 4) 
```

Notice that the $\chi^2$ statistics are getting larger for higher-order lags but this is mainly because of degrees of freedom being used up and not because they are becoming less significant (notice the $p$-values are increasing too.)

Overall, we used residuals and its one lag as well for the model. 


\bigskip\bigskip
***
\bigskip\bigskip

### (i) Using this last specification estimate the long-run elasticity between prices and wages and test that it is unity.

```{stata}
 quietly cd ..
  quietly import excel using Data/sup4.xls, sheet("autocorrelation") firstrow
  quietly destring, replace
  quietly tsset t
  quietly regress gprice gwage gwage_1 gwage_2 gwage_3 gwage_4 gwage_5 gwage_6 gwage_7 gwage_8 gwage_9 gwage_10 gwage_11 gwage_12
  predict E, residuals
  
/* run the regression again with its residuals and their lags */
  quietly regress gprice gwage gwage_1 gwage_2 gwage_3 gwage_4 gwage_5 gwage_6 gwage_7 gwage_8 gwage_9 gwage_10 gwage_11 gwage_12 L.E L2.E 
  
/* obtain the linear combination of parameter estimates */
lincom gwage + gwage_1 + gwage_2 + gwage_3 + gwage_4 + gwage_5 + gwage_6 + gwage_7 + gwage_8 + gwage_9 + gwage_10 + gwage_11 + gwage_12
  
/* carry out a Wald test that linear combination equals to 1 */
    test gwage + gwage_1 + gwage_2 + gwage_3 + gwage_4 + gwage_5 + gwage_6 + gwage_7 + gwage_8 + gwage_9 + gwage_10 + gwage_11 + gwage_12 = 1 
```

or alternatively we can carry out a $t$-test on the coefficient which should be the square root of the Wald's F statistic above:

```{r}
(1.139599-1)/0.0940975
((1.139599-1)/0.0940975)^2
```


In either approach, we cannot reject the null hypothesis that the long run elasticity between prices and wages is unity.
