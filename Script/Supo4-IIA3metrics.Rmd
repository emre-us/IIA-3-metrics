---
title: "IIA-3 Econometrics: Supervision 4"
author: "Emre Usenmez"
date: "Christmas Break 2024"
output: pdf_document
header-includes: 
  - \usepackage{amsmath, tcolorbox, dashrule, booktabs, fancyhdr, multirow}
  - \tcbuselibrary{listings,most}
  - \allowdisplaybreaks
---

<!-- This comment will not be displayed in the output. Below change to CSS style is to ensure the blocktexts are in the same form size as the rest of the text.-->

```{css style settings, echo = FALSE} 
blockquote {
    padding: 10px 20px;
    margin: 0 0 20px;
    font-size: 14px;
    border-left: 5px solid #eee;
}
```
\pagestyle{fancy}
\fancyhead[L]{2024-25 Part IIA Paper 3}
\fancyhead[R]{Supervision 4 Solutions}
\fancyfoot[L]{University of Cambridge}
\fancyfoot[R]{Emre Usenmez}

\small Very grateful to Dr Oleg Kitov and Dr Clive Lawson for the very informative stylized answers to previous iterations of the supervision questions.
\normalsize


# FACULTY QUESTIONS

\bigskip\bigskip
 

## QUESTION 1

\textbf{Consider the following bivariate linear regression}
\[
y = \alpha + T\beta + u
\]
\textbf{where $T$ is a binary treatment regressor, $\alpha$ and $\beta$ are unknown parameters, and $u$ is an error term.}


\bigskip

### (a) Describe in two sentences an empirical, real-life example where such an equation might arise.

\begin{description}
\item[Answer:] 

We can think of $T$ as "graduated from university" and $y$ as "annual earning after 10 years of graduation."

\end{description}

\bigskip\bigskip
***
\bigskip\bigskip

### (b) Why might $u$ be heteroskedastic in your example.

\begin{description}
\item[Answer:] 

The variance of earnings will likely to be smaller across people who did not graduate from a university compared to those who did it. This may be because those who did not go to university are less likely to be in the professions such as lawyers or doctors, and more likely to be in lower-paying jobs, or unemployed, or out of labor force.

\end{description}


\bigskip\bigskip
***
\bigskip\bigskip

### (c) Why might T be endogenous in your example?

\begin{description}
\item[Answer:] 

Broadly, variables that are correlated with the error term are called \textit{endogeneous variables}, and those that are uncorrelated with the error term are called \textit{exogeneous variables}.\footnote{See Chapter 12: Instrumental Variables Regression p.428 in Stock J H, and Watson M W (2020) Introduction to Econometrics, $4^{th}$ Global Ed, Pearson; and Section 8.5: Instrumental Variables in Dougherty C (2016) Introduction to Econometrics $5^{th}$ ed, OUP in addition to Chapter 9: More on Specification and Data Issues in Wooldridge J M (2021) Introductory Econometrics: A Modern Approach, $7^{th}$ ed, Cengage}

Thus the question is asking us to consider some of the reasons as to why $T$ might be correlated with the error term. There are certainly nonnegligible number of high earners who either never went to a university or dropped out. There may be omitted variable or even simultaneity is possible.

Let's consider what the implications of of endogeneity are for the OLS estimator of $\beta$.

Variable $T$ would be endogenous if $\mathbb{E}(u|T) \neq 0$. Endogeneity would imply that $Cov(T,u)\neq 0$. 

We can first look at whether it is biased. For that, we need to use the law of iterated expectations whereby
\[
\mathbb{E}(\hat{\beta}^{OLS}) = \mathbb{E}\Big[\mathbb{E}(\hat{\beta}^{OLS}\ |\ T_1,\dots,T_n)\Big]
\]
The OLS estimator of $\beta$ would be:
\begin{align*}
\mathbb{E}(\hat{\beta}^{OLS} | T_1,\dots,T_n) 
  &= \mathbb{E}\Bigg( \frac{\widehat{Cov}(T_i,Y_i)}{\widehat{Var}(T_i)} \ \Bigg{|}\ T_1,\dots,T_n \Bigg) = \mathbb{E} \Big( \frac{\hat{\sigma}_{TY}}{\hat{\sigma}_{TT}} \ \bigg{|}\ T_1,\dots,T_n \Big) \\[4pt]
  &= \mathbb{E}\left( \frac{\displaystyle\sum_{i=1}^n(T_i-\bar{T})(Y_i-\bar{Y})}{\displaystyle\sum_{i=1}^n(T-\bar{T})^2}\ \Bigg{|}\  T_1,\dots,T_n \right) \\[4pt]
  &= \mathbb{E}\left( \frac{\displaystyle\sum_{i=1}^n(T_i-\bar{T})\big((\alpha + \beta T_i + u_i) - (\alpha + \beta \bar{T} + \bar{u})\big)}{\displaystyle\sum_{i=1}^n(T-\bar{T})^2} \ \Bigg{|}\ T_1,\dots,T_n \right)\\[4pt]
  &= \mathbb{E}\left( \frac{\displaystyle\sum_{i=1}^n(T_i-\bar{T})\big(\beta(T_i-\bar{T}) + u_i - \bar{u}\big)}{\displaystyle\sum_{i=1}^n(T-\bar{T})^2}  \ \Bigg{|}\ T_1,\dots,T_n \right) \\[4pt]
  &= \mathbb{E}\left( \frac{\displaystyle\sum_{i=1}^n \beta(T_i-\bar{T})^2 + \displaystyle\sum_{i=1}^n(T_i-\bar{T})(u_i - \bar{u})}{\displaystyle\sum_{i=1}^n(T-\bar{T})^2} \ \Bigg{|}\ T_1,\dots,T_n \right) \\[4pt]
  &= \mathbb{E}\left( \beta + \frac{\displaystyle\sum_{i=1}^n(T_i-\bar{T})(u_i-\bar{u})}{\displaystyle\sum_{i=1}^n(T_i-\bar{T})^2} \ \Bigg{|}\ T_1,\dots,T_n \right) \\[4pt]
  &= \mathbb{E}\left( \beta + \frac{\displaystyle\sum_{i=1}^n(T_i-\bar{T})u_i - \bar{u}\displaystyle\sum_{i=1}^n(T_i-\bar{T})}{\displaystyle\sum_{i=1}^n(T_i-\bar{T})^2} \ \Bigg{|}\ T_1,\dots,T_n \right) \\[4pt]
  &= \mathbb{E}\left(\beta + \frac{\displaystyle\sum_{i=1}^n(T_i-\bar{T})u_i - \bar{u}\left(\sum_{i=1}^nT_i-n\bar{T}\right)}{\displaystyle\sum_{i=1}^n(T_i-\bar{T})^2} \ \Bigg{|}\ T_1,\dots,T_n \right) \\[4pt]
  &= \mathbb{E}\left( \beta + \frac{\displaystyle\sum_{i=1}^n(T_i-\bar{T})u_i - \bar{u}\left(n\bar{T}-n\bar{T})\right)}{\displaystyle\sum_{i=1}^n(T_i-\bar{T})^2} \ \Bigg{|}\ T_1,\dots,T_n \right) \\[4pt]
  &= \mathbb{E}\left( \beta + \frac{\displaystyle\sum_{i=1}^n(T_i-\bar{T})u_i}{\displaystyle\sum_{i=1}^n(T_i-\bar{T})^2} \ \Bigg{|}\ T_1,\dots,T_n \right) \\[4pt]
  &= \beta + \frac{\displaystyle\sum_{i=1}^n(T_i-\bar{T})\ \mathbb{E}(u_i \ |\ T_1,\dots,T_n)}{\mathbb{E}\left( \displaystyle\sum_{i=1}^n(T_i-\bar{T})^2 \ \Big{|}\ T_1,\dots,T_n \right)} \\[4pt]
\end{align*}
Notice that since $\mathbb{E}(u|T) \neq 0$, the numerator of this last expression is also nonzero. That is, $\sum_{i=1}^n(T_i-\bar{T})\ \mathbb{E}(u_i \ |\ T_1,\dots,T_n) \neq 0$. Therefore the expectation of this expectation is also not equal to $\beta$:
\[
\mathbb{E}(\hat{\beta}^{OLS}) = \mathbb{E}\Big[\mathbb{E}(\hat{\beta}^{OLS}\ |\ T_1,\dots,T_n)\Big] = \mathbb{E}\left[ \mathbb{E}\left( \beta + \frac{\displaystyle\sum_{i=1}^n(T_i-\bar{T})u_i}{\displaystyle\sum_{i=1}^n(T_i-\bar{T})^2} \ \Bigg{|}\ T_1,\dots,T_n \right] \right) \neq \beta
\]
which means the OLS estimator is \textit{not} unbiased.

We can also check for consistency by examining the probability limit of this expression as $n$ tends towards infinity. For that, we can rewrite the OLS estimator as:
\[
\hat{\beta}^{OLS} = \beta + \frac{\displaystyle\frac{1}{n}\displaystyle\sum_{i=1}^n(T_i-\bar{T})u_i}{\displaystyle\frac{1}{n}\displaystyle\sum_{i=1}^n(T_i-\bar{T})^2} 
\]
Using the law of large numbers, we can see that as $n \to \infty$
\begin{align*}
\frac{1}{n}\sum_{i=1}^n(T_i-\bar{T})u_i &\overset{p}{\to} \mathbb{E}\big[(T_i - \bar{T})u_i\big] = Cov(T_i,u_i)\neq 0 \\
\text{and}\\
\frac{1}{n}\sum_{i=1}^n(T_i-\bar{T})^2 &\overset{p}{\to} \mathbb{E}\Big[(T_i-\bar{T})^2\Big] = Var(T_i) = \sigma^2_T < \infty
\end{align*}
Note that $Var(T_i) = \sigma^2_T < \infty$ is an additional assumption.

Since $Cov(T_i,u_i)\neq 0$, the OLS estimator as $n \to \infty$ (using Slutsky's theorem):
\[
\hat{\beta}^{OLS} \overset{p}{\to}\beta+\frac{Cov(T_i,u_i)}{Var(T_i)}\neq \beta
\]
which means the OLS estimator is not only biased but also inconsistent for $\beta$.
\end{description}

\bigskip\bigskip
***
\bigskip\bigskip

### (d) Suppose a single instrument $z$ is available. Show that the population coefficient $\beta$ satisfies
\[
\beta = \frac{Cov(z,y)}{Cov(z,T)}
\]
\textbf{where $Cov(z,y)$ and $Cov(z,T)$ denote, respectively, the population covariance between $z$ and $y$, and $z$ and $T$. How can you use this information to obtain a consistent estimate of $\beta$?}
\begin{description}
\item[Answer:]
Instrument $z$ needs to satisfy the following conditions:
\begin{itemize}
\item \textit{Instrument relevance}: $z$ must have non-trivial explanatory power for $T$, namely $Cov(z,T)\neq 0$.
\item \textit{Instrument exogeneity}: $z$ must affect $Y$ only through its influence on $T$ and not in any other way. That is, $z$ must be exogenous with respect to $u$ in regression $y = \alpha + \beta T + u$. Formally, $\mathbb{E}(u|z)=0$. This is why it is said "z is exogenous in $y = \alpha + \beta T + u$. Exogeneity of instrument $z$ implies that $Cov(z, u)=0$.
\end{itemize}

In the context of omitted variables, instrument exogeneity means that $z$ should be uncorrelated with the omitted variables, i.e. $Cov(z,u)=0$, and $z$ should be related, positively or negatively, to the endogeneous explanatory variable $T$, i.e. $Cov(z,T)\neq 0$.\footnote{see Section 15-1: Omitted Variables in a Simple Regression Model in Wooldridge J M (2021) Introductory Econometrics: A Modern Approach, $7^{th}$ ed, Cengage}

The underlying reasoning is that if an instrument is relevant, then variation in that instrument $z$ is related to variation in $T$, and if it is also exogeneous, then that part of the variation of $T$ captured by $z$ is exogeneous. Therefore, an instrument that is relevant and exogeneous can capture movements in $T$ that are exogeneous. This exogeneous variation can in turn be used to estimate the population coefficient $\beta$.\footnote{see Section 12.1: The IV Estimator with a Single Regressor and a Single Instrument in Stock and Watson (2020, $4^{th}$ ed.).}  

These conditions serve to \textit{identify} the parameter $\beta$. In this context, \textit{identification of a parameter} means that we can write $\beta$ in terms of population moments that can be estimated using a sample of data. 

To write $\beta$ in terms of population covariances we use $y = \alpha + \beta T + u$:
\[
Cov(z,y) = Cov(z, \ \alpha+\beta T + u) = \beta Cov(z,T) + Cov(z,u)
\]

Since instrument exogeneity condition assumes that $Cov(z,u)=0$ then $Cov(z,y)=\beta Cov(z,T)$. Rearranging this gives:
\[
\beta=\frac{Cov(z,y)}{Cov(z,T)}
\]
as desired. Notice that this only holds if instrument relevance also holds, since this expression would fail if $Cov(z,T)=0$. What this expression is telling us is that $\beta$ is identified by the ratio of population covariance between $z$ and $y$ to population covariance between $z$ and $T$. 

Given a random sample, we estimate the population quantities by the sample analogs:
\[
\hat{\beta}^{IV} = \frac{\displaystyle\frac{1}{n}\displaystyle\sum_{i=1}^n(z_i-\bar{z})(y_i-\bar{y})}{\displaystyle\frac{1}{n}\displaystyle\sum_{i=1}^n(z_i-\bar{z})(T_i-\bar{T})} = \frac{\displaystyle\sum_{i=1}^n(z_i-\bar{z})(y_i-\bar{y})}{\displaystyle\sum_{i=1}^n(z_i-\bar{z})(T_i-\bar{T})}.
\]
With a sample data on $T, y,$ and $z$ we can obtain the IV estimator above. The IV estimator for the intercept $\alpha$ is $\alpha = \bar{y} - \hat{\beta}^{IV}\bar{T}$. Also notice that when $z=T$, we get the OLS estimator of $\beta$. That is, when $T$ is exogeneous, it can used as its own IV, and the IV estimator is then identical to the OLS estimator.

A similar set of steps we used in part (c) will show that IV estimator is consistent for $\beta$. That is, $\underset{n\to \infty}{plim}(\hat{\beta}) = \beta$.

Note that, an important feature of IV estimator is that when $T$ and $u$ are in fact correlated, and thus instrumental variables estimation is actually needed, it is essentially \underline{never unbiased}. This means, in small samples, the IV estimator can have a substantial bias, which is one reason why large samples are preferred. 
\end{description}

\bigskip\bigskip
***
\bigskip\bigskip

### (e) Can you give an example of an instrument in your example? Argue why it might be a sensible IV.
\begin{description}
\item[Answer:]
Distance from nearest college can be an example of an instrument, where $z=1$ if individual lived near college and $0$ otherwise. This may be violated for a number of reasons, though; for e.g. if wealthy parents choose to live near college. This would mean that $z$ is correlated with unobserved factors that also affect wage, our $y$. For any example, exogeneity and relevance conditions need to be checked.
\end{description}




\bigskip\bigskip\bigskip
\hdashrule[0.5ex]{\textwidth}{1pt}{3mm}
\bigskip\bigskip\bigskip


## QUESTION 2

\textbf{Consider the following wage equation that explicitly recognizes that ability affects $log(wage)$}
\[
log(wage) = \alpha + \beta_1 educ + \beta_2 ability + u
\]
\textbf{The above model shows explicitly that we would like to hold ability fixed when measuring the returns on education. Assuming that the primary interest is in obtaining a reliable estimate of the slope parameters $\beta_1$, and that there is no direct measurement for ability, explain how you would do this using a method based upon a proxy variable and an IV estimator. In doing so evaluate the following statement:}

"\textbf{\textit{whilst $IQ$ is a good candidate as a proxy for variable for ability, it is not a good instrumental variable for $educ$.}}"

\begin{description}
\item[Answer:] 
This question is essentially aiming to ensure the students understand the difference between proxy variable and instrumental variable.
\begin{itemize}
\item[proxy variable] refers to an \textit{observed} variable that is correlated with but not identical to the \textit{unobserved} variable.
\item[instrumental variable] refers to a variable that does not appear in the regression, uncorrelated with the error in the equation, and partially correlated with the endogenous explanatory variable in an equation where such endogenous explanatory variable exists.
\end{itemize}


\underline{Proxy Variable:}

Notice in this question $educ$ is observed but $ability$ is unobserved, and we would not even know how to interpret it's coefficient $\beta_2$ since 'ability' itself is a vague concept. We can instead use intelligence quotient, or $IQ$, as a proxy for ability as long as $IQ$ is correlated with ability. This is captured by the following simple regression:
\[
ability = \delta_0 + \delta_2IQ + v_2
\]
where $v_2$ is an error due to the fact that $ability$ and $IQ$ are not exactly related. The parameter $\delta_2$ measures the relationship between $ability$ and $IQ$. If $\delta_2=0$ then $IQ$ is not a suitable proxy for $ability$.

Note that the intercept $\delta_0$ allows $ability$ and $IQ$ to be measured on different scales and thus can be positive or negative. That is, the unobserved $ability$ is not required to have the same average value as $IQ$ in the population.

To use $IQ$ to get unbiased, or at least consistent, estimators for $\beta_1$, the coefficient of $educ$, we would regress $log(wage)$ on $educ$ and $IQ$. This is called \textit{the plug-in solution to the omitted variables problem} since we plug-in $IQ$ for $ability$ before running the OLS. However, since $IQ$ and $educ$ are not the same, we need to check if this does give consistent estimator for $\beta_1$. 

For the plug-in solution to provide consistent estimator for $\beta_1$ the following two assumptions need to hold true:
\begin{itemize}
\item The error $u$ is uncorrelated with $educ$ and $ability$ as well as $IQ$. That is, $\mathbb{E}(u|educ,ability,IQ)=0$. What this means is that $IQ$ is irrelevant in the population model which is true by definition since $IQ$ is a proxy for $ability$, it is $ability$ that directly affects $log(wage)$ not $IQ$.
\item The error $v_2$ is uncorrelated with $educ$ and $IQ$. For $v_2$ to be a uncorrelated with $educ$, $IQ$ needs to be a 'good' proxy for $ability$. 
\end{itemize}

What is meant by 'good' proxy in this sense is that
\[
\mathbb{E}(ability\ |\ educ,IQ) = \mathbb{E}(ability\ |\ IQ) = \delta_0 + \delta_2IQ.
\]
Here, the first equality, which is the most important one, says that once $IQ$ is controlled for, the expected value of $ability$ does not depend on $educ$. In other words, $ability$ has zero correlation with $educ$ once $IQ$ is partialled out. Thus the average level of $ability$ only changes with $IQ$ and not with $educ$.

To see why these two assumptions are enough for the plug-in solution to work, we can rewrite the $log(wage)$ equation in the question as:
\begin{align*}
log(wage) 
  &= \alpha + \beta_1 educ + \beta_2 ability + u \\
  &= \alpha + \beta_1 educ + \beta_2 (\delta_0 + \delta_2IQ + v_2) + u \\
  &= (\alpha + \beta_2\delta_0) + \beta_1 educ + \beta_2\delta_2IQ + u + \beta_2v_2 \\
  &= (\alpha + \beta_2\delta_0) + \beta_1 educ + \beta_2\delta_2IQ + \epsilon \\
  &= \gamma_0 + \beta_1 educ + \gamma_2 IQ + \epsilon.
\end{align*}

Notice that the composite error $\epsilon$ depends on both the error in the model of interest in the question, $u$, and on the error in the proxy variable equation, $v_2$. Since both $u$ and $v_2$ have zero mean and each is uncorralated with $educ$ and $IQ$, $\epsilon$ also has zero mean and is uncorralted with $educ$ and $IQ$.

So when we regress $log(wage)$ on $educ$ and $IQ$, we will \underline{not} get unbiased estimators of $\alpha$ and $\beta_2$. Instead, we will get unbiased, or at least consistent, estimators of $\gamma_0, \beta_1,$ and $\gamma_2$. The important thing is that we get good estimators of $\beta_1$. 

In most cases, the estimate of $\gamma_2$ is more interesting than an estimate of $\beta_2$ anyway, since $\gamma_2$ measures the return to $log(wage)$ given one more point on $IQ$ score.


\begin{tcolorbox}[breakable, title=Bias and Multicollinearity when using a proxy, skin=enhancedlast]
\textbf{When using a proxy variable can still lead to bias?}

If the two assumptions above are not satisfied, then using a proxy variable can lead to a bias. To see this, suppose now that ability is not only related to $IQ$ but also to $educ$:
\[
ability = \delta_0 + \delta_1 educ + \delta_2 IQ + v_3
\]
where the error $v_3$ has a zero mean and uncorrelated with $educ$ and $IQ$. In the proxy variable discussion above, it was essentially assumed that $\delta_1=0$. We can re-write $log(wage)$ with this plug-in solution:
\begin{align*}
log(wage) 
  &= \alpha + \beta_1 educ + \beta_2 ability + u \\
  &= \alpha + \beta_1 educ + \beta_2 (\delta_0 + \delta_1 educ + \delta_2 IQ + v_3) + u \\
  &= (\alpha + \beta_2\delta_0) + (\beta_1 + \beta_2\delta_1) educ + \beta_2\delta_2 IQ + u + \beta_2v_3 \\
\end{align*}
Since the error term $u+\beta_2v_3$ has zero mean and is uncorrelated with $educ$ and $IQ$, we have $plim(\hat{\beta}_1)=\beta_1 + \beta_2\delta_1$. If $educ$ is partially and positively correlated with $ability$, i.e. $\delta_1 > 0$, and if the coefficient of $ability$ is positively correlated with $log(wage$, i.e. $\beta_2 > 0$, then $plim(\hat{\beta}_1)=\beta_1 + \beta_2\delta_1 > \beta_1$ giving us an upward bias. That is, in this case where $IQ$ is not a good proxy for $ability$ but we still use it, then we'd still be getting an upward bias for the coefficient of $educ$.

Having said that, the bias is likely to be smaller than if we ignored the problem of omitted ability entirely.

\bigskip

\textbf{What about multicollinearity?}

Even if $IQ$ is a good proxy for $ability$, using it in a regression that includes $educ$ can exacerbate the multicollinearity problem, which, in turn, is likely to lead a less precise estimate of the coefficient for $educ$, i.e. $\beta_1$.

However, notice that
\begin{itemize}
\item inclusion of $IQ$ in the regression means that the part of $ability$ explained by $IQ$ is removed from the error term, reducing the error variance. This is likely to be reflected in a smaller standard error of the regression, though that reduction may not happen because of degrees of freedom adjustment.

\item if we want to have a less bias for $\beta_1$, ie, the estimator of the coefficient for $educ$, then we have to live with increased multicollinearity. This is an important point. Since $educ$ and $ability$ are thought to be correlated, and if we could include $ability$ in the regression, then there would be inevitable multicollinearity caused by the correlation between these two variables. Since $IQ$ is a proxy for $ability$, $educ$ and $IQ$ are also correlated, and a similar argument ensues.
\end{itemize}
\end{tcolorbox}

\bigskip

\underline{Instrumental Variable}

Suppose now that the proxy variable does not have the required properties for a consistent estimator of $\beta_1$. Then we put $ability$ in the error term since it is unobserved and we don't have a proxy for it. This leaves us with:
\[
log(wage) = \beta_0 + \beta_1 educ + \epsilon
\]
where $\epsilon$ contains $ability$. If $ability$ and $educ$ are correlated, then we have a biased and inconsistent estimate of $\beta_1$.

However, we can still use this equation as the basis for estimation as long as we can find an instrumental variable for $educ$. For this we can introduce an \textit{instrumental variable} $z$ which satisfies the "instrument relevance", i.e. $Cov(z, educ)\neq 0$, and "instrument exogeneity", i.e. $Cov(z,\epsilon)=0$ conditions as discussed in Question 1(d).

Note that we cannot really test for "instrument exogeneity" assumption and need to consider economic behavior in order to maintain the $Cov(z, \epsilon)=0$ assumption. At times, there may be an observable proxy for some factor contained in $\epsilon$ and we can check if $z$ and the proxy variable are more or less uncorrelated. And, of course, as discussed above, if we have a good proxy then we would add that variable to the equation and estimate the expanded form by OLS. 

This is exactly where we see a tension between a good proxy vs a good instrumental variable. For $IQ$ to be a good proxy, it needs to be as highly correlated with $ability$ as possible. Yet for $IQ$ to be a good instrumental variable, it needs to be uncorrelated with $ability$ since $ability$ is contained in $\epsilon$ and a good instrumental variable should not covary with the error term. That is, a good instrumental variable should affect $log(wage)$ only through its influence on $educ$ and not in any other way.

Thus, in this question, although $IQ$ is a good candidate as a proxy variable for $ability$, it is not a good instrumental variable for $educ$.

\end{description}








\bigskip\bigskip\bigskip
\hdashrule[0.5ex]{\textwidth}{1pt}{3mm}
\bigskip\bigskip\bigskip


## QUESTION 3
\textbf{The following regression explores the relationship between television watching and childhood obesity, using a cross-section of US children. The variables are:}

\begin{tabular}{llccc}
Name   & Description & Minimum & Maximum & Mean \\ 
\midrule
tvyest & hours of TV watched yesterday & 0  & 6  & 3.14 \\
black  & dummy, 1 if black             & 0  & 1  & 0.31 \\
hisp   & dummy, 1 if hispanic          & 0  & 1  & 0.36 \\
ageyrs & age in years                  & 5  & 16 & 9.4  \\
bmi    & child's Body Mass Index       & 11 & 55 & 19   \\
dadbmi & father's BMI                  & 11 & 58 & 26   \\
mombmi & mother's BMI                  & 14 & 56 & 26   \\
\midrule
\end{tabular}

```{r include=FALSE}
libraries <- c("haven",       # to import/export SPSS, STATA, SAS files
               "readxl",      # to import/export Excel files   
               "tidyverse",   # for tidy data
               "Statamarkdown", # for using STATA commands in R
               "ivreg", #for regressions with instrumental variables
               "mnormt", #for creating bivariate normal distributions
               "kableExtra", # for creating nice tables in R
               "rstatix")     # converts stats functions to a tidyverse-friendly format

invisible(lapply(libraries, library, character.only=TRUE))  # will load the libraries

```

\bigskip

The output from a 2SLS regression appears below:


```{stata echo=FALSE}
* Change the working directory to access the data file:
    quietly cd .. 
* Load the data:
    use Data/obesekids.dta

* Run the instrumental variables regression
    ivregress 2sls tvyest black hisp ageyrs (bmi = dadbmi mombmi)

```

\textbf{Now answer the following questions.}

### (a) Why might an OLS regression of $tvyest$ on the child's BMI give us inconsistent estimates of the causal effect of BMI on TV watching?

\begin{description}
\item[Answer:]
Recall that correlation between the error term and any of the regressors generally causes all of the OLS estimators to be inconsistent. In fact, if the error term is correlated with any of the independent variables, then OLS is both biased and inconsistent. This means any bias persists even as the sample size grows.

Here, if we only regress $tvyest$ on $bmi$ then inevitably all the omitted variables would be contained in the error term and they would be correlated with $bmi$, which would give us inconsistent estimates of the causal effect of $bmi$ on tv watching.
\end{description}


\bigskip\bigskip
***
\bigskip\bigskip

### (b) Interpret the coefficient $0.73$ on $black$.
\begin{description}
\item[Answer:]
The coefficient implies that holding other variables constant, black children watched on average about 0.73 hours more tv than non-black children.
\end{description}


\bigskip\bigskip
***
\bigskip\bigskip

### (c) Can you state a reason why we may doubt the validity of the 2SLS estimates reported above?
\begin{description}
\item[Answer:]
In the least, the 2SLS estimation method have the following assumptions:

\begin{itemize}
\item the error term of the structural equation is uncorrelated with each of the exogenous explanatory variables
\item there exists at least one exogenous variable that is partially correlated with the endogenous variable in the structural equation but itself is not in the structural equation to ensure consistency
\item the structural error term cannot depend on any of the exogeneous variables, i.e. homoskedasticity assumption. This ensures the 2SLS standard errors and $t$-statistics to be asymptotically valid. 
\end{itemize}

\item Violation of any one of these assumptions would make us doubt the validity of the 2SLS estimates reported above.
\end{description}















































\pagebreak

# SUPPLEMENTARY QUESTIONS

\bigskip\bigskip

## QUESTION 1

\textbf{Consider the simple regression model:}
\begin{equation} \label{eq:SQ1}
Y_i = \alpha + \beta X_i + \varepsilon_i, \ \ i = 1,2,\dots,m
\end{equation}
\textbf{where $Y_i$ is the mean expenditure on alcohol in group $i$ and $X_i$ is the mean income of group $i$. Each group $i$ has $N_i$ members and the model satisfies all the classical assumptions except that the variance of $\varepsilon_i$ is equal to $\sigma^2/N_i$.}


### (a) What are the statistical properties of the OLS estimates of $\alpha$ nad $\beta$ in this case?

\begin{description}
\item[Answer:] 
Recall that when demonstrating unbiasedness and consistency of OLS estimators, homoskedasticity assumption did not play any role. That is, if the variance of the unobserved error is not constant, i.e. heteroskedastic, it does not impact whether an estimator is unbiased or consistent. Similarly, the interpretation of the goodness-of-fit measures, $R^2$ and $\bar{R}^2$, are also unaffected by the presence of heteroskedasticity.

The problem with the presence of heteroskedasticity is that the estimators of the variances are biased. Since the OLS standard errors are based on these variances, they are no longer valid for constructing confidence intervals and $t$-statistics. In this situation the OLS $t$-statistics do not have $t$ distributions and the problem is not resolved by increasing the sample size. Similarly, $F$-statistics are not longer $F$-distributed. Finally, the OLS is no longer BLUE as it is no longer asymptotically efficient.

Recall that the OLS estimator is
\[
\hat{\beta} = \beta + \frac{\displaystyle\sum_{i=1}^m (X_i - \bar{X})\varepsilon_i}{\displaystyle\sum_{i=1}^m (X_i - \bar{X})^2} = \beta + \frac{\displaystyle\sum_{i=1}^m (X_i - \bar{X})\varepsilon_i}{{SST}_X^2} 
\]

and its variance when homoskedasticity is present is
\begin{align*}
Var(\hat{\beta})  
  &= Var \left( \beta + \frac{\displaystyle\sum_{i=1}^m (X_i - \bar{X})\varepsilon_i}{\displaystyle\sum_{i=1}^m (X_i - \bar{X})^2} \right) \\[6pt] 
  &= Var \left(\frac{\displaystyle\sum_{i=1}^m (X_i - \bar{X})\varepsilon_i}{\displaystyle\sum_{i=1}^m (X_i - \bar{X})^2} \right) \ \ \ \ \text{since $\beta$ is constant} \\[6pt] 
  &= \left(\frac{1}{\displaystyle\sum_{i=1}^m (X_i - \bar{X})^2} \right)^2 \ Var\Big( \displaystyle\sum_{i=1}^m (X_i - \bar{X})\varepsilon_i \Big)  \ \ \ \ \text{since we are conditioning on $X_i$, $SST_X$ is nonrandom}   \\[6pt]
  &= \Big( \frac{1}{SST_X} \Big)^2 \Big( \displaystyle\sum_{i=1}^m (X_i - \bar{X})^2 \ Var(\varepsilon_i) \Big) \ \ \ \ \text{since we are conditioning on $X_i$, $X_i - \bar{X}$ is nonrandom} \\[6pt]
  &= \Big( \frac{1}{SST_X} \Big)^2 \Big( \displaystyle\sum_{i=1}^m (X_i - \bar{X})^2\sigma_\varepsilon^2 \Big)  \ \ \ \ \text{since $Var(\varepsilon_i) = \sigma^2$ for all $i$ when homoskedastic} \\[6pt]
  &= \sigma_\varepsilon^2 \Big( \frac{1}{SST_X}\Big)^2 SST_X \\[6pt]
  &= \frac{\sigma_\varepsilon^2}{SST_X} 
  = \frac{\sigma_\varepsilon^2}{\displaystyle\sum_{i=1}^m(X_i - \bar{X})^2}
\end{align*}

and its variance when heteroskedasticity is present is
\[
Var(\hat{\beta}) 
  = \Big( \frac{1}{SST_X} \Big)^2 \Big( \displaystyle\sum_{i=1}^m (X_i - \bar{X})^2\sigma_i^2 \Big) 
  = \frac{\displaystyle\sum_{i=1}^m (X_i - \bar{X})^2\sigma_i^2}{\Big( \displaystyle\sum_{i=1}^m(X_i - \bar{X})^2 \Big)^2}.
\]

\underline{Spherical Errors}

We assume homoskedasticity and no autocorrelation in estimating the variance of OLS estimates. That is, we assume that all errors have the same variance $\sigma^2$ and that there is no correlation across errors. If these hold true, then we have \textit{spherical errors}, or that the error term follows a \textit{spherical distribution}. This is represented in matrix form as follows:
\begin{equation*}
\mathbb{E}(\vec{u}\vec{u}^T | \mathbf{X}) = 
\begin{bmatrix}
\sigma^2 & 0 & \dots & 0 \\
0 & \sigma^2 & \dots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \dots & \sigma^2
\end{bmatrix}
= \sigma^2 \mathbf{I}
\end{equation*}

To see why this is called a spherical distribution lets look at a special case of two dimensions, i.e. circular distribution, as opposed to three dimensions for spherical distribution. Consider two random errors, $u_i$ and $u_j$ which are graphed below as density plots and contour plots, the latter of which shows what you'd see when you look straight down from the top of the density plot.

The shapes of these plots depend on the variances and covariances of these two random errors. If $u_i$ and $u_j$ are homoskedastic and they are not autocorrelated, then the contour lines will be circles. If there were three random error variables $u_i$, $u_j$, and $u_k$ then we would have four-dimensional density plot and the contours would form a sphere. If there were more than three random error variables then the contours would form a hyper-sphere. This is why the errors are spherically distributed.

What we are plotting is therefore:
\begin{equation*}
\mathbb{E}
\begin{pmatrix}
u_i \\
u_j
\end{pmatrix}
=
\begin{pmatrix}
0 \\
0
\end{pmatrix}
\ \ \ \ ; \ \ \ \
Var
\begin{pmatrix}
u_i \\
u_j
\end{pmatrix}
=
\begin{pmatrix}
1 & 0 \\
0 & 1
\end{pmatrix}
\end{equation*}

where errors are homoskedastic and there is no autocorrelation.

\end{description}


```{r echo=FALSE}
#build bivariate normal distribution
x <- seq(-3,3,0.25)
y <- seq(-3,3,0.25)
mu <- c(0,0)
sigma <- matrix(c(1,0,0,1), nrow = 2)
f <- function(x,y) dmnorm(cbind(x,y),mean = mu, varcov = sigma)
z <- outer(x,y,f)

#plot side by side
par(mfrow = c(1,2))
#density plot
persp(x,y,z, main="Density",  xlab = expression(error[i]), ylab=expression(error[j]), zlab=expression('density'))
#contour plot
contour(x,y,z, main="Contour", axes=TRUE, xlab=expression(error[i]),ylab=expression(error[j]))


```


If on the other hand, heterskedasticity is present then we loose the symmetry of the joint density plot and get a more elliptic contours. Suppose now the variance-covariance matrix is as follows:
\begin{equation*}
Var
\begin{pmatrix}
u_i \\
u_j
\end{pmatrix}
=
\begin{pmatrix}
0.25 & 0 \\
0 & 2
\end{pmatrix}
\end{equation*}

```{r echo=FALSE}
#build bivariate normal distribution
x <- seq(-3,3,0.25)
y <- seq(-3,3,0.25)
mu <- c(0,0)
sigma <- matrix(c(0.25,0,0,2), nrow = 2)
f <- function(x,y) dmnorm(cbind(x,y),mean = mu, varcov = sigma)
z <- outer(x,y,f)

#plot side by side
par(mfrow = c(1,2))
#density plot
persp(x,y,z, main="Density",  xlab = expression(error[i]), ylab=expression(error[j]), zlab=expression('density'))
#contour plot
contour(x,y,z, main="Contour", axes=TRUE, xlab=expression(error[i]),ylab=expression(error[j]))

?contour
```

Similarly, we would also get an elliptic contours if the errors are homoskedastic but there is autocorrelation. The slope of the main axis of the ellipse would depend on the sign of the correlation between the errors. Suppose now the variance-covariance matrix is as follows:
\begin{equation*}
Var
\begin{pmatrix}
u_i \\
u_j
\end{pmatrix}
=
\begin{pmatrix}
0.1 & -0.5 \\
-0.5 & 1
\end{pmatrix}
\end{equation*}

```{r echo=FALSE}
#build bivariate normal distribution
x <- seq(-3,3,0.25)
y <- seq(-3,3,0.25)
mu <- c(0,0)
sigma <- matrix(c(1,-0.5,-0.5,1), nrow = 2)
f <- function(x,y) dmnorm(cbind(x,y),mean = mu, varcov = sigma)
z <- outer(x,y,f)

#plot side by side
par(mfrow = c(1,2))
#density plot
persp(x,y,z, main="Density",  xlab = expression(error[i]), ylab=expression(error[j]), zlab=expression('density'))
#contour plot
contour(x,y,z, main="Contour", axes=TRUE, xlab=expression(error[i]),ylab=expression(error[j]))


```



\bigskip\bigskip
***
\bigskip\bigskip


### (b) How should equation \ref{eq:SQ1} on page \pageref{eq:SQ1} be transformed so that the OLS estimates of $\alpha$ and $\beta$ are BLUE?
\begin{description}
\item[Answer:] The variances of the error terms are given in the question, thus \textit{known}. We can therefore estimate using the generalized least squares (GLS) estimators for correcting heteroskedasticity where we minimize a \textit{weighted sum of squared residuals.}

$\hookrightarrow$ For remedial measures when $\sigma_i^2$ is unknown, see Question 2 below.

 
\[
Var(\varepsilon_i) = \frac{\sigma^2}{N_i} = \sigma_i^2
\]
So we transform equation \ref{eq:SQ1} on page \pageref{eq:SQ1} by dividing it with theses known standard deviations, $\sigma_i$:
\[
\frac{Y_i}{\sigma_i} = \frac{\alpha}{\sigma_i} + \beta \frac{X_i}{\sigma_i} + \frac{\varepsilon_i}{\sigma_i}
\]
so that
\begin{align*}
Var(\frac{\varepsilon_i}{\sigma_i}) 
  &= \mathbb{E}\Bigg[ \bigg( \frac{\varepsilon_i}{\sigma_i} \bigg)^2 \Bigg] - \Bigg[ \mathbb{E}\bigg( \frac{\varepsilon_i}{\sigma_i} \bigg) \Bigg]^2 \\[6pt]
  &= \mathbb{E}\Bigg[ \bigg( \frac{\varepsilon_i}{\sigma_i} \bigg)^2 \Bigg] \ \ \ \ \text{since } \mathbb{E}\bigg( \frac{\varepsilon_i}{\sigma_i} \bigg) = 0 \\[6pt]
  &= \frac{1}{\sigma_i^2}\mathbb{E}(\varepsilon_i^2) \ \ \ \ \text{since $\sigma_i^2$ is known, thus it is a collection of constants} \\[6pt] 
  &= \frac{1}{\sigma_i^2}\sigma_i^2 = 1
\end{align*}
which is a constant. This means, the variance of the transformed disturbance term $\frac{\varepsilon_i}{\sigma_i}$ is now homoskedastic. Since all the other assumptions of classical model still hold true, this means that if we apply OLS method to the transformed model, we will get estimators that are BLUE.

Thus, GLS is OLS on the transformed variables that satisfy the standard least-squares assumptions. The estimators that are obtained these way are GLS estimators which are BLUE.
\end{description}




\bigskip\bigskip
***
\bigskip\bigskip

### (c) Derive $\hat{\alpha}$ in terms of $\hat{\beta}$ in this case.

\begin{description}
\item[Answer:]
In this case, what we want is a transformation of the equation \ref{eq:SQ1} on page \pageref{eq:SQ1} in such a way that the variance of the transformed error, $Var(\varepsilon_i*)$, is constant $\sigma^2$. 

For this, we can work backwards. We know that $Var(\varepsilon_i) = \mathbb{E}(\varepsilon_i^2) = \frac{\sigma^2}{N_i}$ so if the transformation resulted in $Var(\varepsilon_i^*)=N_i\mathbb{E}(\varepsilon_i^2)$ then it would equal to constant $\sigma^2$. Fro that to happen, we can set $\varepsilon_i^* = \varepsilon_i\sqrt{N_i}$, so that
\[
Var(\varepsilon_i^*) = \mathbb{E}\big((\varepsilon_i^*)^2\big) - \big[\mathbb{E}(\varepsilon_i^*)\big]^2 = \mathbb{E}\big((\varepsilon_i^*)^2\big) = \mathbb{E}\Big((\varepsilon_i\sqrt{N_i})^2\Big) = N_i\mathbb{E}(\varepsilon_i^2) = N_i\frac{\sigma^2}{N_i} = \sigma^2
\]
as desired.

Thus using the weighting of $\sqrt{N_i}$ the sample regression function becomes:
\[
Y_i\sqrt{N_i} = \alpha\sqrt{N_i} + \beta \sqrt{N_i}X_i + \varepsilon_i\sqrt{N_i} \\
Y_i^* = \alpha^* + \beta^* X_i + \varepsilon^*
\]
In general, to obtain the estimators for the coefficients, the weighted least-squares method minimizes the weighted residual sum of squares:
\[
\sum w_i\hat{\varepsilon}_i^2 = \sum w_i(Y_i - \hat{\alpha}^* - \hat{\beta}^* X_i)^2
\]
where $\alpha^*$ and $\beta^*$ are the weighted least squares estimators. Differentiating these with respect to $\hat{\alpha}^*$ and $\hat{\beta}^*$ gives us:
\begin{align*}
\frac{\partial}{\partial\hat{\alpha}^*}\sum w_i\hat{\varepsilon}_i^2 &= 2 \sum w_i(Y_i - \hat{\alpha}^* - \hat{\beta}^* X_i)(-1) \\[6pt]
\frac{\partial}{\partial\hat{\beta}^*}\sum w_i\hat{\varepsilon}_i^2 &= 2 \sum w_i(Y_i - \hat{\alpha}^* - \hat{\beta}^* X_i)(-X_i)
\end{align*}
Setting these equal to $0$ gives us:
\begin{align*}
\sum w_iY_i &= \hat{\alpha}^*\sum w_i + \hat{\beta}^* \sum w_iX_i \\
\sum w_iX_iY_i &= \hat{\alpha}^*\sum w_iX_i + \hat{\beta}^*\sum w_iX_i^2
\end{align*}
Solving these simultaneously, we get:
\begin{align*}
\hat{\alpha}^* 
  &= \frac{\sum w_iY_i}{\sum w_i} - \hat{\beta}^* \frac{\sum w_iX_i}{\sum w_i} \\[6pt]
  &= \bar{Y}^* - \hat{\beta}^*\bar{X}^* \\[18pt]
\hat{\beta}^*
  &= \frac{(\sum w_i)(\sum w_iX_iY_i) - (\sum w_iX_i)(\sum w_iY_i)}{(\sum w_i)(\sum w_iX_i^2) - (\sum w_iX_i)^2}
\end{align*}
Notice that in this question $w_i = N_i$ and not $\sqrt{N_i}$.  
\end{description}










\bigskip\bigskip\bigskip
\hdashrule[0.5ex]{\textwidth}{1pt}{3mm}
\bigskip\bigskip\bigskip

## QUESTION 2
\textbf{Using the Heteroskedasticity worksheet in sup4.xls}

Load the data in R:
```{r, eval=FALSE}
property_df <- read_excel("../Data/sup4.xls")

# You can use any of the following to examine data frame (df): 
# `dim()`: for its dimensions, by row and column
# `str()`: for its structure
# `summary()`: for summary statistics on its columns
# `colnames()`: for the name of each column
# `head()`: for the first 6 rows of the data frame
# `tail()`: for the last 6 rows of the data frame
# `View()`: for a spreadsheet-like display of the entire data frame
```


\bigskip\bigskip


### (a) Estimate the following and comment on your results:
\begin{equation} \label{eq:SQ2a}
PRICE_t = \beta_0 + \beta_1LOTSIZE_t + \beta_2SQRFT_t + \beta_3BDRMS_t + u_t
\end{equation}

In R run the following:
```{r, eval=FALSE}
SQ2a_lm <- lm(PRICE ~ BDRMS + LOTSIZE + SQRFT, data = property_df)
print(summary(SQ2a_lm), digits=7)
```

and in STATA run the following:
```{stata}
/* load the data */
quietly cd ..
quietly import excel using Data/sup4.xls, ///
  sheet("heteroscedasticity") firstrow

/* `firstrow` indicates that the first row contains the variable names */
/* `describe` command would give basic information about the data set */

/* run the regression */
regress PRICE LOTSIZE SQRFT BDRMS
```

We see that the $F$-stat is high at $57.46$ with its $p$ value being $0$. We also see that both $LOTSIZE$ and $SQRFT$ are significant with $t$-values $3.22$ and $9.28$ with near $0$, or $0$, $p$-values, respectively. On the other hand, $BDRMS$ look insignificant with $t$-value at $1.54$, though it may perhaps be due to multicollinearity.

To check for heteroskedasticity, usually the first thing to do is to plot the residuals against the estimated values of the independent variable as an amalgamation of all the dependent variables.

In R we do this with the following:
```{r eval=FALSE}
# the following will provide four important plots that are usually needed
# since there are four graphs, we want to display in 2x2 format first then plot
par(mfrow = c(2,2))
plot(SQ2a_lm)

# if it is only the residuals vs fitted that we are interested, then
plot(SQ2a_lm, which=1)
# or
plot(fitted(SQ2a_lm), resid(SQ2a_lm))
# we can also add a horizontal line at 0
abline(0,0)

# to make this look nicer, we can  also use `autoplot` command from `ggfortify` library
library(ggfortify)
autoplot(SQ2a_lm)
```

In STATA we instead use the following:
```{stata, eval=FALSE}
/* plot residuals against fitted values */
rvfplot, yline(0)
```

In either case we get the following plot:

```{r, echo=FALSE}
property_df <- read_excel("../Data/sup4.xls")
SQ2a_lm <- lm(PRICE ~ BDRMS + LOTSIZE + SQRFT, data = property_df)
plot(SQ2a_lm, which=1)
abline(0,0)
```

There seems to be a downward trend which can suggest heteroskedasticity but it is difficult to tell, as it could be due to outliers.



\bigskip\bigskip
***
\bigskip\bigskip

### (b) Calculate robust standard errors for the equation \ref{eq:SQ2a} specified on page \pageref{eq:SQ2a} and compare your results.

\begin{description}
\item[Answer:]
White (1980)\footnote{White, H (1980) "A Heteroscedasticity Consistent Covariance Matrix Estimator and a Direct Test of Heteroscedasticity", \textit{Econometrica}, 48:817-828. Though the possibility of such heterskedasticity-robust standard errors were previously discussed by Eicker (1967) and Huber (1967) and so sometimes these are also called \textit{White-Huber-Eicker standard errors.} See Eicker, F (1967) "Limit Theorems for Regressions with Unequal and Dependent Errors", \textit{Proceedings of the Fifth Berkeley Symposium on Mathematical Statistics nd Probability} 1:59-82, and Huber, P J (1967) "The Behavior of Maximum Likelihood Estimates under Nonstandard Conditions",\textit{Proceedings of the Fifth Berkeley Symposium on Mathematical Statistics nd Probability} 1:221-233.} has shown that asymptotically consistent estimates of variances and covariances of OLS estimators can be obtained even if there is heteroskedasticity present so that asymptotically valid statistical inferences can be made about the true paramter values. White's heteroskedasticity-corrected standard errors are also known as \textit{robust standard errors}. 

\begin{tcolorbox}[breakable, title=White's robust standard errors, skin=enhancedlast]

\textbf{How do we get heteroskedasticity-consistent variances and standard errors?}\footnote{Gujarati and Porter (2009), Appendix 11A.4; Wooldridge (2021), Section 8.2}

Consider the simple regression model:
\[
Y_i = \beta_0 + \beta_1 X_1 + u_i
\]
where $Var(u_i)=\sigma_i$; that is, it is heteroskedastic. In Question 1 part (a) we have shown that 
\begin{equation}\label{eq:WhiteVar1}
Var(\hat{\beta_1}) = \frac{\displaystyle\sum_{i=1}^n (X_i - \bar{X})^2\sigma_i^2}{\Big( \displaystyle\sum_{i=1}^n(X_i - \bar{X})^2 \Big)^2}
\end{equation}
Since $\sigma_i^2$ are not directly observable, White argues for using the squared residual of each $i$, $\hat{u}_i^2$, instead and estimating the variance of the estimator via:
\begin{equation}\label{eq:WhiteVar2}
Var(\hat{\beta}_1) = \frac{\displaystyle\sum_{i=1}^n (X_i - \bar{X})^2\hat{u}_i^2}{\Big( \displaystyle\sum_{i=1}^n(X_i - \bar{X})^2 \Big)^2}
\end{equation}
White has shown that when this equation \ref{eq:WhiteVar2} is multiplied by the sample size $n$, it converges in probability to $\frac{\mathbb{E}[(X_i - \mu_X)^2u_i^2]}{(\sigma_X^2)^2}$ which is the probability limit of equation \ref{eq:WhiteVar1} multiplied by $n$, and where $\mu_X$ is the expected value of $X$, and $\sigma_X^2$ is the population variance of $X$. Thus, the law of large numbers and the central limit theorem are key in establishing these convergences, which are necessary for justifying the use of standard errors to construct confidence intervals and $t$-statistics.

$\hookrightarrow$ One can first obtain the residuals from the usual OLS regression and then calculate the variance using equation \ref{eq:WhiteVar2}. Statistical software do this automatically.

This can be extended to $k$-variable regression model
\[
Y_i = \beta_0 + \beta_1 X_{1i} + \beta_2 X_{2i} + \dots + \beta_kX_{ki} + u_i
\]
The variance of any partial regression coefficient, say $\hat{\beta}_j$ is then obtained via
\begin{equation}\label{eq:WhiteVar3}
Var(\hat{\beta}_j) 
  = \frac{\displaystyle\sum_{i=1}^n \hat{v}_{ji}^2 \hat{u}_i^2}{\Big( \displaystyle\sum_{i=1}^n \hat{v}_{ji}^2 \Big)^2}
  = \frac{\displaystyle\sum_{i=1}^n \hat{v}_{ji}^2 \hat{u}_i^2}{RSS_j^2}
\end{equation}
where $\hat{v}_{ji}$ denotes the $i^{th}$ residual from regressing $X_j$ on all other independent variables, and $RSS_j$ is the residual sum of squares from this regression. 

The square root of this expression in equation \ref{eq:WhiteVar3} is called \textbf{heteroskedasticity-robust standard error} for $\hat{\beta}_j$.

Also note that, sometimes the equation \ref{eq:WhiteVar3} is adjusted for degrees of freedom by multiplying it with $frac{n}{n-(k+1)}$ before taking the square root. This is because if $\hat{u}_i$ were the same for all $i$ then we would get the usual OLS standard errors. Since all forms of this equation has asymptotic justification, and they are asymptotically equivalent, no one form is unanimously preferred over others. Usually, we use whatever form the software we work with uses. 
\end{tcolorbox}

\end{description}

We can now calculate this in R as follows:

```{r eval=FALSE}
#we need two additional libraries for this:
library(lmtest) #for `coeftest` function
library(sandwich) #for `vcovHC` function

coeftest(SQ2a_lm, vcov = vcovHC(SQ2a_lm, "HC1")) 
#the default in vcovHC is "HC3" but to get the exact result as STATA we use "HC1"
```

Similarly, we can calculate this in STATA as follows:
```{stata eval=FALSE}
/* run the regression with additional `robust` command */

regress PRICE LOTSIZE SQRFT BDRMS, robust
```

However, to present the "robust" and "nonrobust" results side by side in a table, we can use the following set of commands instead:

```{stata}
/* load the data */
  quietly cd ..
  quietly import excel using Data/sup4.xls, ///
    sheet("heteroscedasticity") firstrow

/* run the regression with `robust` command */
  quietly regress PRICE LOTSIZE SQRFT BDRMS, robust

/* store the estimates under the heading "robust" */
  estimates store robust
  
/* run the regression for nonrobust */
  quietly regress PRICE LOTSIZE SQRFT BDRMS 
  
/* store the estimates under the heading "nonrobust" */
  estimates store nonrobust
  
/* create the table for robust and nonrobust estimates of beta, s.e., and t-values */
  estimates table robust nonrobust, b se t

```

Notice that all the $t$-values are lower for each variable and in the case of $LOTSIZE$ this reduction means it is no longer significant. This is at least suggestive that $LOTSIZE$ may be a major source of the heteroskedasticity.




\bigskip\bigskip
***
\bigskip\bigskip

### (c) Using the specification in equation \ref{eq:SQ2a} on page \pageref{eq:SQ2a}, conduct a Goldfeld-Quandt test for heteroskedasticity in the $LOTSIZE$ dimension (exclude the middle 24 observations).

\begin{description}
\item[Answer:]
The Goldfeld-Quandt\footnote{Goldfeld S, and Quandt R E (1972) \textit{Nonlinear Methods in Econometrics}, North Holland Publishing Company, Amsterdam.} test is applicable when we assume that the heteroskedastic variance, $\sigma_i^2$, is positively related to \textit{one} of the explanatory variables in the regression model. In this question we are assuming that the heteroskedastic variance is related to $LOTSIZE$.

\begin{tcolorbox}[breakable, title=Goldfeld-Quandt Test, skin=enhancedlast]

\textbf{What are the reasoning and mechanics of the test?}\footnote{Gujarati and Porter (2009), Section 11.5}

Consider the simple regression model:
\[
Y_i = \beta_0 + \beta_1 X_1 + u_i
\]
and suppose $\sigma_i^2$ is monotonically related to $X_i$. One plausible assumption of this is 
\begin{equation}\label{eq:GQTest1}
\sigma_i^2 = \sigma^2X_i^2.
\end{equation}
What this assumption says is that $\sigma_i^2$ is proportional to the square of the $X$ variable. If this assumption is appropriate, it would mean the larger $X_i$ values are, the larger $\sigma_i^2$ gets. If that turns out to be the case, heteroskedasticity is most likely to be present in the model. 

To test this, Goldfeld and Quandt provide the following steps:
\begin{itemize}
\item[Step 1:] Order or rank the observations according to the values of $X_i$ beginning with the lowest $X$ value;

\item[Step 2:] Omit $c$ central observations, where $c$ is specified a priori, and divide the remaining onservations into two groups, each of $\frac{n-c}{2}$ observations;

\item[Step 3:] Fit separate OLS regressions to these two groups of observations and obtain the respective residual sum of squares $RSS_1$ and $RSS_2$, where $RSS_1$ represents the $RSS$ from the regression corresponding to the smaller $X_i$ values, i.e. small variance group, and $RSS_2$ to the larger $X_i$ values, i.e. the large variance group.

These $RSS$ each have $\displaystyle\frac{n-c}{2}-(k+1)$ degrees of freedom where $k$ is the number of parameters to be estimated, excluding the intercept - hence $+1$.

\item[Step 4:] Compute the following ratio:
\[
\lambda = \frac{\frac{RSS_2}{df}}{\frac{RSS_1}{df}}
\]
\end{itemize}

The main argument of this test is that if the assumption of homoskedasticity and $u_i$ are normally distributed both hold true, then $\lambda$ of equation \ref{eq:GQTest1} follows the $F$-distribution with $\displaystyle\frac{n-c}{k}-(k+1)$ degrees of freedom in both the numerator and denominator.

As usual, if the computed $\lambda$ which is equal to $F$-statistic, is greater than the critical $F$ value at the chosen level of significance, we can reject the null hypothesis of homoskedasticity.

\textbf{Why we ommit $c$ central observations?}
These observations are omitted to accentuate the difference between the small variance group, $RSS_1$, and the large variance group $RSS_2$. However, the \textit{power} of the test depends on how $c$ is chosen. Recall that \textit{power of a test} is measured by the probability of rejecting the null hypothesis when it is false, and it is calculated by $1-prob(Type II error)$.

Goldfeld and Quandt suggest $c=8$ for models with two-explanatory variables if $n=30$ and double if $n=60$.
\end{tcolorbox}
\end{description}

In this question $c=24$ and we order $LOTSIZE$ from small to large. To run the Goldfeld-Quandt test in R we can use the `gqtest()` function from the `lmtest` library:

```{r include=FALSE}
library(lmtest)
```

```{r}
gqtest(SQ2a_lm, order.by = property_df$LOTSIZE, fraction = 24, alternative="two.sided")
qf(0.975, 28, 28, lower.tail = TRUE) #critical F-value
```

In STATA there are more steps involved. First we need to order the data and removed the middle 24 observations before running regression on each:

```{stata}
/* load the data */
  quietly cd ..
  quietly import excel using Data/sup4.xls, ///
    sheet("heteroscedasticity") firstrow
 
/* Step 1: Order the data according to LOTSIZE values */  
  sort LOTSIZE
  
/* create an index on which we will impose our condition for splitting data */  
  gen index=_n

/* run the regressions on each splitted data */  
  reg PRICE BDRMS LOTSIZE SQRFT if index<33
  
  reg PRICE BDRMS LOTSIZE SQRFT if index>56
  
/* Derive F-stat by dividing RSS of each (since df cancel out) */
  display e(rss)/8.5839e+10
  
/* compute critical F value */
  display invfprob(28,28,0.025)

```

Both R and STATA give the same result that the $F$-statistic of $1.6275654$ is smaller than the critical $F$-value of $2.1299243$ which means we cannot reject the null of homoskedasticity. Therefore, it seems as though there is no heteroskedasticity according to the Goldfeld-Quandt test. However, the form of heteroskedasticity may be more complicated.




\bigskip\bigskip
***
\bigskip\bigskip

### (d) Test for heteroskedasticity by first estimating an equation that regresses the squared residuals from equation \ref{eq:SQ2a} on page \pageref{eq:SQ2a} against all of the independent variables used to estimate equation \ref{eq:SQ2a}. (Calculate both F and LM versions of this test). Verify your results using the 'hettest' command in Stata. Compare these results with the results of the White Test in Stata.

\begin{description}
\item[Answer:]
Goldfeld-Quandt test depends not only on the number of observations we omit but also on identifying the correct $X$ variable that needs to be ordered. These limitations of this test can be avoided with \textit{Breusch-Pagan Test},\footnote{Breusch, T and Pagan A (1979) "A Simple Test for Heteroscedasticity and Random Coefficient Variation", \textit{Econometrica}, 47:1287-1294.} or BP test, which is also called \textit{Breusch-Pagan-Godfrey Test},\footnote{Godfrey L (1978) "Testing for Multiplicative Heteroscedasticity" \textit{Journal of Econometrics}, 8:227-236.} or BPG test.  

\begin{tcolorbox}[breakable, title=Breusch-Pagan / Breusch-Pagan-Godfrey Test, skin=enhancedlast]

\textbf{What are the reasoning and mechanics of the test?}\footnote{Gujarati and Porter (2009), Section 11.5; Wooldridge (2021), Section 8.3}
Consider the model
\[
Y = \beta_0 + \beta_1X_{1} + \beta_2X_{2} + \dots + \beta_kX_{k} + u
\]
and assume that $\mathbb{E}(u | X_1. X_2, \dots, X_k) = 0$ so that OLS is unbiased and consistent. 

The null hypothesis is that homoskedasticity holds and we require the data to tell us otherwise. That is, $\mathbb{H}_0: Var(u|X_1,X_2,\dots,X_k) = \sigma^2$; and since $Var(u|X_1,X_2,\dots,X_k) = \mathbb{E}(u^2|X_1,X_2,\dots,X_k)$ the null hypothesis can be expressed as:
\[
\mathbb{H}_0: Var(u|X_1,X_2,\dots,X_k) = \mathbb{E}(u^2|X_1,X_2,\dots,X_k) = \mathbb{E}(u^2) = \sigma^2.
\]
This shows that in order to test for violation of the homoskedasticity assumption we want to test whether $u^2$ is related in expected value to one or more of the explanatory variables. Therefore, if $\mathbb{H}_0$ is false, then the expected value of $u^2$ given the independent variables, i.e. $\mathbb{E}(u^2|X_1, X_2,\dots,X_k)$ can be any function of the $X_j$. A simple approach is to assume a linear function:
\[
u^2 = \gamma_0 + \gamma_1X_1 + \gamma_2X_2 + \dots + \gamma_kX_k + v
\]
The null hypothesis then becomes
\[
\mathbb{H}_0: \gamma_1 = \gamma_2 = \dots = \gamma_k = 0.
\]

Under the null hypothesis we can assume that the error $v$ is independent of $X_1, \dots X_k$. Then, either the $F$ or $Lagrange Multiplier (LM)$ statistics can be used to test for the overall significance of the independent variables in explaining $u^2$. Both statistics would have asymptotic justification, even though $u^2$ cannot be normally distributed.

$\hookrightarrow$ e.g. if $u$ is normally distributed then $\frac{u^2}{\sigma^2}$ is distributed $\chi_1^2$.

If we could observe the $u^2$ in the sample, then we could compute this statistic by running the OLS regression of $u^2$ on $X_1,\dots,X_k$ using all $n$ observations, which would give us the maximum likelihood (ML) of $\sigma^2$ (as opposed to . Since we do not know $u$, we can instead estimate the equation:
\[
\hat{u}^2 = \gamma_0 + \gamma_1 X_1 + \gamma_2 X_2 + \dots + \gamma_k X_k + \epsilon
\]

and compute $F$ or $LM$ statistics for the joint significance of $X_1, \dots, X_k$. The $F$ and $LM$ statistic both depend on the $R$-squared value of this regression, $R_{\hat{u}^2}^2$.

The $F$-statistic for heteroskedasticity is
\[
F = \frac{\displaystyle\frac{R_{\hat{u}^2}^2}{k}}{\displaystyle\frac{1-R_{\hat{u}^2}^2}{n-(k+1)}}
\]
where $k$ is the number of regressors. This $F$ statistic has approximately an $F_{k, n-(k+1)}$ distribution under the null hypothesis of homoskedasticity.

The $LM$ statistic for heteroskedasticity is
\[
LM = n \times R_{\hat{u}^2}^2
\]
which is the $R$-squared of the error regression multiplied by the sample size. Under the null hypothesis, $LM$ is distributed asymptotically as $\chi_k^2$.

The $LM$ version of the test is called the \textbf{Breusch-Pagan test} for heteroskedasticity, or BP-test; though the $LM$-statistic form was suggested by Koenker (1981).\footnote{Koenker, R (1981) "A Note on Studentizing a Test for Heteroskedasticity", \textit{Journal of Econometrics} 17:107-112.}

\bigskip

\textbf{What is Lagrange Multiplier Statistic?}\footnote{Wooldridge (2021), Section 5.2a}
Consider again the multiple regression model with $k$ independent variables:
\[
Y = \beta_0 + \beta_1X_{1} + \beta_2X_{2} + \dots + \beta_kX_{k} + u
\]
We want to test whether, say, the last $q$ of these variables all have $0$ population parameters. The null hypothesis is therefore
\[
\mathbb{H}_0: \beta_{k-q+1} = \beta_{k-q+2} = \dots = \beta_k = 0, 
\]
which puts $q$ exclusion restrictions on the model. The alternatively hypothesis is that at least one of the parameters is dofferent from $0$.

The LM statistic requires the estimation of the restricted model only. So we run the regression
\[
Y = \beta_0^{res} + \beta_1^{res} X_1 + \dots + \beta_{k-q}^{res} X_{k-q} + u^{res}
\]
where $u^{res}$ indicate that the residuals are from the restricted model. Note that this is a shorthand to indicate that we obtain restricted residual for each observation in the sample, but didn't use the $i$ subscript to avoid crowding of subscripts.

The idea is that if the omitted variables $X_{k-q+1}$ through $X_{k}$ truly have $0$ population coefficients, then $u^{res}$ should at least be approximately uncorrelated with each of these variables in the sample. In fact, it should be uncorrelated with all regressors because the omitted regressors in the restricted model are correlated with the regressors that appear in the restricted model.

This means, we run the regression of $u^{res}$ on $X_1, \dots, X_k$.

$\hookrightarrow$ this is an example of \textit{auxiliary regression} which is a regression used to compute a test statistic but whose coefficients are not of direct interest.

If the null hypothesis is true, the $R$-squared from this regression should be "close" to zero, subject to sampling error. This is because $u^{res}$ will be approximately uncorrelated with all the independent variables. What is interesting with this test is that, under the null hypothesis, the sample size multiplied by the $R$-squared from the auxiliary regression is distributed asymptotically as a chi-square random variable with $q$ degrees of freedom. That is, $n\times R_{\hat{u}^2}^2 \ \overset{a}{\sim} \ \chi_q^2$.

Because of its form, the $LM$ statistic is also referred to as the \textbf{n-R-squared statistic}.
\end{tcolorbox}

\end{description}

\bigskip

We can obtain the BP-statistic that has a $Chi_3^2$ distribution in R as follows:

```{r eval=FALSE}
bptest(SQ2a_lm)
```

In STATA, we can do the same using the `hettest()` command:

```{stata}
/* load the data */
quietly cd ..
quietly import excel using Data/sup4.xls, ///
  sheet("heteroscedasticity") firstrow

/* run the regression */
quietly regress PRICE LOTSIZE SQRFT BDRMS

/* apply hettest where rhs mean right-hand-side */
  hettest, rhs fstat
  hettest, rhs iid

/* manual calculation to compare the results */
  predict u, r
  generate U2 = u^2
  quietly regress U2 LOTSIZE SQRFT BDRMS

/* display the F-statistic and LM-statistic */
  display e(F)
  display e(r2)*e(N)
```

it is often the case that $\chi^2$-tests have better properties but are harder to explain. So here we use the $F$ initially to give the intuition then point out which $\chi^2$-tests do roughly the same things. Based on the $p$-values we can reject the null hypothesis. The Breusch-Pagan test suggests the presence of heteroskedasticity.

However, the BP test assumes that the form of the heteroskedasticity is linear. To try out different forms of the relations, we can use the White test.

\begin{tcolorbox}[breakable, title=White Test, skin=enhancedlast]

\textbf{What are the reasoning and mechanics of the test?}\footnote{Gujarati and Porter (2009), Section 11.5; Wooldridge (2021), Section 8.3a}

Unlike Goldfeld-Quandt test, which requires reordering the observations with respect to the $X$ variable that supposedly caused heteroskedasticity, or the BP test, which is sensitive to the normality and linearity assumptions, the general heteroskedasticity test proposed by White (1980)\footnote{White H (1980) "A Heteroskedasticity Consistent Covariance Matrix Estimator and a Direct Test of Heteroskedasticity", \textit{Econometrica}, 48:817-818} does not rely on the normality assumption. 

White test uses the insight that the homoskedasticity assumption can be replaced with the weaker assumption that the squared error $\hat{u}_i^2$ is \textit{uncorrelated} with all the independent variables, $X_j$, the squares of the independent variables, $X_j^2$, and all the cross products, $X_jX_h$ for $j\neq h$. 

The test is explicitly intended to test for forms of heteroskedasticity that invalidate the usual OLS standard errors and test statistics. Consider a model with $k=3$ independent variables. The White test process is as follows:
\begin{itemize}
\item[Step 1:] Estimate $Y_i = \beta_0 + \beta_1 X_{1i} + \beta_2 X_{2i} + \beta_3 X_{3i} + u_i$ and obtain the residuals, $\hat{u}_i$;

\item[Step 2:] Obtain the $R_{\hat{u}^2}^2$ from following auxiliary regression:
\[
\hat{u}_i^2 = \gamma_0 + \gamma_1 X_{1i} + \gamma_2 X_{2i} + \gamma_3 X_{3i} + \gamma_4 X_{1i}^2 + \gamma_5 X_{2i}^2 + \gamma_6 X_{3i}^2 + \gamma_7 X_{1i}X_{2i} + \gamma_8 X_{1i}X_{3i} + \gamma_9 X_{2i}X_{3i} + \epsilon_i 
\]
That is, we are regressing the squared residuals from the original regression on the original variables, their squared values, and the cross products of the regressors. We can also introduce higher powers of regressors if necessary.

\item[Step 3:] Under the null hypothesis that there is no heteroskedasticity, $n\times R_{\hat{u}^2}^2 \ \overset{a}{\sim} \ \chi_{df}^2$. In this example we have $9$ regressors, so $df=9$. 

\item[Step 4:] If the $\chi_{df}^2$ value obtained is higher than the critical $\chi_{df}^2$ at the chosen level of significance, then this test would suggest a presence of heteroskedasticity. If it does not exceed the critical value, then we cannot reject $\mathbb{H}_0: \gamma_1 = \dots = \gamma_9 = 0$.
\end{itemize}

One final point is that this approach of White test uses many degrees of freedom. We can have a slightly different approach to White test that can conserve on degrees of freedom. To create the test, notice that the difference between White and BP tests is that the White test includes the squares and cross-products of the independent variables, whereas BP doesn't. We can preserve the spirit of the White test while conserving on degrees of freedom by using the OLS fitted values in a test for heteroskedasticity. 

Recall the fitted values are defined for each observation $i$ by
\[
\hat{Y}_i = \hat{\beta}_0 + \hat{\beta}_1 X_{1i} + \dots + \hat{\beta}_k X_{ki}
\]
These are just linear functions of the independent variables. If we square the fitted values, we get a particular function of all the squares and cross products of the independent variables. This suggests testing for heteroskedasticity by estimating the equation
\[
\hat{u}_i^2 = \eta_0 + \eta_1 \hat{Y}_i + \eta_2 \hat{Y}_i^2 + \epsilon
\]
where $\hat{Y}_i$ stand for fitted values. We can use $F$ or $LM$ statistic for the null hypothesis $\mathbb{H}_0: \eta_1=0, \eta_2=0$. This results in two restrictions in testing the null og homoskedasticity, regardless of the number of independent variables in the original model. This can be thought of as a special case of White test.
\end{tcolorbox}

We can run the White test in R manually as follows:
```{r eval=FALSE}
Ru2_SQ2 <- summary(lm(resid(SQ2a_lm) ~ fitted(SQ2a_lm) + I(fitted(SQ2a_lm)^2)))$r.squared
LM_SQ2 <- nrow(property_df)*Ru2_SQ2
p_value_SQ2 <- 1-pchisq(LM_SQ2,2)
p_value_SQ2
```

or we can use the `bptest()` function from `lmtest` package:
```{r eval=FALSE}
bptest(SQ2a_lm, ~ BDRMS + LOTSIZE + SQRFT 
       + I(BDRMS)^2 + I(LOTSIZE)^2 + I(SQRFT)^2 
       + BDRMS*LOTSIZE + BDRMS*SQRFT + SQRFT*LOTSIZE, 
       data = property_df)

#Special case of White test that conserves on degrees of freedom
bptest(SQ2a_lm, ~ fitted(SQ2a_lm) + poly(fitted(SQ2a_lm),2))
```

```{stata}
/* load the data */
quietly cd ..
quietly import excel using Data/sup4.xls, ///
  sheet("heteroscedasticity") firstrow

/* run the regression */
quietly regress PRICE LOTSIZE SQRFT BDRMS

/* manual calculation for White Test */
  predict u, r
  generate U2 = u^2
  generate B2 = BDRMS^2
  generate L2 = LOTSIZE^2
  generate S2 = SQRFT^2
  generate BL = BDRMS*LOTSIZE
  generate BS = BDRMS*SQRFT
  generate LS = LOTSIZE*SQRFT
  
  quietly regress U2 BDRMS LOTSIZE SQRFT B2 L2 S2 BL BS LS
  
/* calculate the chi-square statistic */
  display e(N)*e(r2)
```
or we can use the `imtest, white` command in STATA after the original regression:

```{stata}
/* load the data */
quietly cd ..
quietly import excel using Data/sup4.xls, ///
  sheet("heteroscedasticity") firstrow

/* run the regression */
quietly regress PRICE LOTSIZE SQRFT BDRMS

/* run the White test */
imtest, white
```

In all of these approaches we obtain a chi-squared value of $33.73$ with $0.001$ p-value. Thus we can reject the null hypothesis of the homoskedasticity.

However, there is sure to be lots of multicollinearity here so it is difficult to tell if there is a non-linear relationship with any of the variables. We can run individual regressions and see what we can find. For example, with $LOTSIZE$:

```{stata}
/* load the data */
quietly cd ..
quietly import excel using Data/sup4.xls, ///
  sheet("heteroscedasticity") firstrow

/* run the regression */
quietly regress PRICE LOTSIZE SQRFT BDRMS

/* obtain residual squareds and create LOTSIZE squared */
  predict u, r
  generate U2 = u^2
  generate LOTSIZE2 = LOTSIZE^2
  
/* regress residuals on lotsize for nonlinearity */
  regress U2 LOTSIZE LOTSIZE2
```
There appears to be a non-linear relationship to LOTSIZE, in which case the White test may be better than the Breusch-Pagan test.



***
\bigskip\bigskip

### (e) Researcher A decides to try 'scaling' to remove the heterosekdasticity and so reestimates the equation in part (a) by dividing \ref{eq:SQ2a} on page \pageref{eq:SQ2a} by $BDRMS$ and then, as an alternative, by $LOTSIZE$. Discuss the reasoning behind using such variable to scale in this way (what must the form of heteroskedasticity be in each case? - note the $1/X$ term in each regression you have to estimate!). Which gives the best results?

\begin{description}
\item[Answer:]
Recall in Question 2(b) we calculated White's robust standard errors. That method has some drawbacks, however. In addition to being a large-sample procedure, the estimators obtained using White's robust standard errors may not be so efficient compared to those obtained by transforming the data to reflect specific types of heteroskedasticity. 

To see this, consider the simple regression
\[
Y_i = \beta_0 + \beta_1 X_i + u_i
\]
and assume that the error variance is proportional to square of the explanatory variable, $X_i^2$:
\[
\mathbb{E}(u_i^2) = \sigma^2X_i^2.
\]
If this is the case, the original model can be transformed to yield a homoskedastic error variance as follows.

First, divide the original model by $X_i$ so that
\begin{align*}
\frac{Y_i}{X_i} 
  = \frac{\beta_0}{X_i} + \beta_1 \frac{X_i}{X_i} + \frac{u_i}{X_i} \\[6pt]
  = \frac{\beta_0}{X_i} + \beta_1 + v_i
\end{align*}
where $v_i = u_i / X_i$ is the transformed disturbance term. Then,
\[
\mathbb{E}(v_i^2) = \mathbb{E}\Bigg[\Big(\frac{u_i}{X_i}\Big)^2\Bigg] = \frac{1}{X_i^2}\mathbb{E}(u_i^2) = \frac{1}{X_i^2}\sigma^2X_i^2 = \sigma^2.
\]
That is, the variance of $v_i$ is homoskedastic and OLS can be applied to the transformed equation.

This is exactly what the question is asking us to do. In the first part we will estimate the following:
\[
\frac{PRICE_i}{BDRMS_i} = \beta_0\frac{1}{BDRMS_i} + \beta_1 \frac{LOTSIZE_i}{BDRMS_i} + \beta_2 \frac{SQRFT_i}{BDRMS_i} + \beta_3 + u_i\frac{1}{BDRMS_i} 
\]
\end{description}

In R we can run the transformed regression and run the BP and White tests as follows:

```{r eval=FALSE}
SQ2e_lm_bdrms <- lm(I(PRICE/BDRMS) ~ I(LOTSIZE/BDRMS) + I(SQRFT/BDRMS) + I(1/BDRMS), 
     data=property_df)
summary(SQ2e_lm_bdrms)

# Breusch-Pagan test
bptest(SQ2e_lm_bdrms)

# White test
bptest(SQ2e_lm_bdrms, ~ I(LOTSIZE/BDRMS) + I(SQRFT/BDRMS) + I(1/BDRMS)
       + I((LOTSIZE/BDRMS)^2) + I((SQRFT/BDRMS)^2) + I((1/BDRMS)^2)
       + I(LOTSIZE/BDRMS)*I(SQRFT/BDRMS) + I(LOTSIZE/BDRMS)*I(1/BDRMS)  
       + I(SQRFT/BDRMS)*I(1/BDRMS), data=property_df)

#Special case of White test that conserves on degrees of freedom
bptest(SQ2e_lm_bdrms, ~ fitted(SQ2e_lm_bdrms) + poly(fitted(SQ2e_lm_bdrms),2))
```

and in STATA as follows:
```{stata}
/* load the data */
  quietly cd ..
  quietly import excel using Data/sup4.xls, ///
    sheet("heteroscedasticity") firstrow

/* generate the transformations */
  generate PRBD = PRICE/BDRMS
  generate LTBD = LOTSIZE/BDRMS
  generate FTBD = SQRFT/BDRMS
  generate BD = 1/BDRMS

/* run the regression */
  regress PRBD LTBD FTBD BD
  
/* run the Breusch-Pagan test */
  hettest LTBD FTBD BD, iid
  
/* run the White test */
  imtest, white
```

From the chi-squared values and their associated p-values we cannot reject the null hypothesis of homoskedasticity.

Note that we can also obtain the same using the GLS, or weighted least squares, approach discussed in question 1(b). Here the weights are $1/BDRMS_i$. 

If we do the same transformation with $LOTSIZE$ we get:
```{stata}
* load the data */
  quietly cd ..
  quietly import excel using Data/sup4.xls, ///
    sheet("heteroscedasticity") firstrow

/* generate the transformations */
  generate PRLT = PRICE/LOTSIZE
  generate LT = 1/LOTSIZE
  generate FTLT = SQRFT/LOTSIZE
  generate BDLT = BDRMS/LOTSIZE

/* run the regression */
  regress PRLT LT FTLT BDLT
  
/* run the Breusch-Pagan test */
  hettest LT FTLT BDLT, iid
  
/* run the White test */
  imtest, white
```

Once again, from the chi-squared values and their associated p-values we cannot reject the null hypothesis of homoskedasticity.




\bigskip\bigskip
***
\bigskip\bigskip

### (f) Researcher B decides to pursue the following strategies to remove heteroskedasticity:
\begin{itemize}
\item[i)] \textbf{Use logged data for PRICE, LOTSIZE, SQRFT (N.B. don't drop BDRMS)}
\item[ii)] \textbf{Remove outliers (observations 42,73,76,77)}
\end{itemize}
\textbf{Discuss the reasoning behind each of these strategies, and the results obtained in each case.}

\begin{description}
\item[Answer (i):]
A log transformation such as $ln Y_i = \beta_0 + \beta_1 \ lnX_i + u_i$ often reduces heteroskedasticity because log transformation compresses the scales, reducing say a ten-fold difference between two values to a two-fold difference. This transformation, of course, would not be possible if some of the $Y$ and $X$ values are zero or negative. Though in this case, we can use $ln(Y_i + m)$ or $ln(X_i+m)$ where $m$ is a postive value large enough to make all the values of $Y$ or $X$ positive.

In this question, our transformation is as follows:
\[
ln(PRICE_i) = \beta_0 + \beta_1 \ ln(LOTSIZE_i) +\beta_2 \ ln(SQRFT_i) + \beta_3 \ BDRMS_i + u_i
\]

\end{description}

In R we can transform and obtaint he BP nad White tests as follows:

```{r eval=FALSE}
SQ2f_lm_log <- lm(log(PRICE) ~ log(LOTSIZE) + log(SQRFT) + BDRMS, data=property_df)
summary(SQ2f_lm_log)

# Breusch-Pagan test
bptest(SQ2f_lm_log)

# White test
bptest(SQ2f_lm_log, ~ log(LOTSIZE) + log(SQRFT) + BDRMS
       + I(log(LOTSIZE)^2) + I(log(SQRFT)^2) + I(BDRMS^2)
       + log(LOTSIZE)*log(SQRFT) + log(LOTSIZE)*BDRMS + log(SQRFT)*BDRMS, data=property_df)

#Special case of White test that conserves on degrees of freedom
bptest(SQ2f_lm_log, ~ fitted(SQ2f_lm_log) + poly(fitted(SQ2f_lm_log),2))
```

Similarly, in STATA:

```{stata}
/* load the data */
  quietly cd ..
  quietly import excel using Data/sup4.xls, ///
    sheet("heteroscedasticity") firstrow

/* generate the transformations */
  generate lnPR = ln(PRICE)
  generate lnLT = ln(LOTSIZE)
  generate lnFT = ln(SQRFT)

/* run the regression */
  regress lnPR lnLT lnFT BDRMS
  
/* run the Breusch-Pagan test */
  hettest lnLT lnFT BDRMS, iid
  
/* run the White test */
  imtest, white
```

From the chi-squared values and their associated p-values we fail to reject the null hypothesis of homoskedasticity.

\bigskip

\begin{description}
\item[Answer (ii):]
Heteroskedasticity can also arise as a result of the presence of outliers. An outlier "is an observation from a different population to that generating the remaining sample observations."\footnote{Gujarati and Porter (2009), Section 11.1} Whether those outliers are included in the regression or not can substantially alter the results of the regression analysis. This is because OLS gives equal weight to every observation in the sample. 

To see why these points were picked in the question first let's look at the scatter plot of $LOTSIZE$
\end{description}

```{r}
plot(SQ2a_lm$residuals^2 ~ property_df$LOTSIZE, 
     xlab="Lot Size ft-sq", ylab = "residuals squared")
text(SQ2a_lm$residuals^2 ~ property_df$LOTSIZE, label=property_df$Date)
```

Here we can see that the 42nd, 73rd, 76th, and 77th data points are outliers.

We can now remove these and run the regression in R:

```{r eval=FALSE}
# create a new dataframe with outliers removed
property_df_nooutl <- property_df[-c(42,73,76,77),]

# run the regression using this new dataframe
SQ2f_lm_nooutl <- lm(PRICE ~ LOTSIZE + SQRFT + BDRMS, data = property_df_nooutl)
summary(SQ2f_lm_nooutl)

# run the BP test
bptest(SQ2f_lm_nooutl)

# run the White test
bptest(SQ2f_lm_nooutl, ~ BDRMS + LOTSIZE + SQRFT 
       + I(BDRMS^2) + I(LOTSIZE^2) + I(SQRFT^2) 
       + BDRMS*LOTSIZE + BDRMS*SQRFT + SQRFT*LOTSIZE, 
       data = property_df_nooutl)

#Special case of White test that conserves on degrees of freedom
bptest(SQ2f_lm_nooutl, ~ fitted(SQ2f_lm_nooutl) + poly(fitted(SQ2f_lm_nooutl),2))
```

and in STATA

```{stata}
/* load the data */
  quietly cd ..
  quietly import excel using Data/sup4.xls, ///
    sheet("heteroscedasticity") firstrow

/* run the regression removing the outliers */    
  regress PRICE LOTSIZE BDRMS SQRFT if Date != 42 & Date != 73 & Date != 76 & Date != 77

  /* run the Breusch-Pagan test */  
  hettest LOTSIZE BDRMS SQRFT, iid
  
/* run the White test */
  imtest, white
```

From the chi-squared values and their associated p-values we fail to reject the null hypothesis of homoskedasticity.


\bigskip\bigskip
***
\bigskip\bigskip

### (g) Considering your results from parts (e) and (f), which strategy for removing heteroskedasticity do you believe to be the best and why?

\begin{description}
\item[Answer:]
Table below summarizes the results of the BP and White tests for each strategy:

\begin{center}
\begin{tabular}{|| c | c | c | c c c | ||}
\multicolumn{6}{r}{Heteroskedasticity Test} \\
 \hline
 &  F & Adj $R^2$ & & BP & White \\
\hline\hline
\multirow{2}{8em}{Scaling by BDRMS} & \multirow{2}{3em}{37.39} & \multirow{2}{3em}{0.557} & $\chi^2$ & 2.59 & 14.13 \\
 & & & $p$-value & 0.459 & 0.118 \\
\hline
\multirow{2}{8em}{Scaling by LOTSIZE} & \multirow{2}{3em}{480.77} & \multirow{2}{3em}{0.943} & $\chi^2$ & 5.96 & 8.74 \\
 & & & $p$ value & 0.113 & 0.462 \\
\hline
\multirow{2}{8em}{log transformation} & \multirow{2}{3em}{50.42} & \multirow{2}{3em}{0.63} & $\chi^2$ & 4.22 & 9.55 \\
 & & & $p$ value & 0.238 & 0.388 \\
\hline
\multirow{2}{8em}{remove outliers in LOTSIZE} & \multirow{2}{3em}{65.83} & \multirow{2}{3em}{0.701} & $\chi^2$ & 5.97 & 9.26 \\
 & & & $p$ value & 0.113 & 0.414 \\
\hline
\end{tabular}
\end{center}

Looking at the table, we can see that in all approaches we are able to tackle heteroskedasticity since in all of them we fail to reject the null hypothesis of homoskedasticity. However, the $F$ value and the adjusted $R^2$ is considerably higher for "scaling by $LOTSIZE$" strategy. This may be because the error variance is proportional to $LOTSIZE$ and there are outliers in the $LOTSIZE$ dimension. Scaling gets rid of both problems. 

\end{description}














\bigskip\bigskip\bigskip
\hdashrule[0.5ex]{\textwidth}{1pt}{3mm}
\bigskip\bigskip\bigskip

## QUESTION 3

### (a) Briefly discuss the problem of autocorrelation - why might such a problem arise and what problems follow from the use of OLS?

\begin{description}
  \item[Answer:] We will briefly discuss the nature of the problem, why it occurs, and what problems arise from use of OLS with serially correlated errors.
  \underline{What is the problem of autocorrelation?} 
  
  Classical linear regression model assumes autocorrelation does not exist in the disturbances. That is, it assumes $Cov(u_i,u_j | X_i, X_j) = 0, \ i\neq j$. What this means is that the disturbance term relating to any observation is not influenced by the disturbance term relating to any other observation. If there is such a dependence, then autocorrelation is present. That is, $\mathbb{E}(u_i,u_j)\neq 0$.
  \underline{Why autocorrelation occurs?} 
  
  There are several reasons as to why autocorrelation occurs including inertia, specification bias, cobweb phenomenon, lags, 'manipulation' of data, data transformation, and nonstationarity.\footnote{Gujarati and Porter (2009), Section 12.1}
    \begin{itemize}
      \item[inertia:] Most econometrics time series has inertia or sluggishness in that the variables exhibit cyclicality where successive observations are likely to be interdependent.
      \item[specification bias:] It may be the case that one of the variables of the true model is omitted or excluded from the model used. The omitted variable would then be part of the error or disturbance term, which would in turn reflect a systematic patternd.
      \item[cobweb phenomenon:] The supply of many agricultural commodities reflect the cobweb phenomenon where supply reacts to price with a lag of one time period because supply decisions take time to implement. Thus at the beginning of this year's planting of crops, farmers are influenced by the price prevailing last year.
      \item[lags:] In a time series regression, it is not uncommon to find that the dependent variable in the current variable also depends on the value of itself in the previous period. That is, $Y_t = \beta_0 + \beta_1 X_t + \beta_2 Y_{t-1} + u_t$. This is known as \textit{autoregression}. If we omit the lagged term in this equation, the resulting error term will reflect a systematic pattern due to the influence of the lagged variable on the dependent variable.
      \item['manipulation' of data:] Smoothing out the data, say by averaging three monthly observations to obtain quarterly figures, may itself lend to a systematic pattern in the disturbances, thereby introducing autocorrelation. Similarly, interpolation or extrapolating from data for the missing values, or any data 'massaging' techniques may also introduce autocorrelation.
      \item[data transformation:] Even if the level form $Y_t = \beta_1 + \beta_2X_t + u_t$ satisfies the OLS assumptions including no autocorrelation, in its first difference form $\Delta Y_i = \beta_2 \ \Delta X_i + v_i$ the error term $v_i = \Delta u_i$ is autocorrelated. Thus, certain transformations may induce autocorrelation.
      \item[nonstationarity:] A time series is stationary if its characteristics such as mean, variance, and covariance are time invariant. If, however, they do change over time, then the time series are nonstationary, and the error term will exhibit autocorrelation.
    \end{itemize}
  \underline{Problems in estimating with autocorrelated errors:} 
  
  Lets look at what happens in terms of unbiasedness, consistency, efficiency and goodness of fit.
    \begin{itemize}
      \item[Unbiasedness:] Unbiasedness assumes nothing about the serial correlation of errors. As long as the explanatory variables are strictly exogenous then the $\hat{\beta}_j$ are unbiased, regardless of the degree of serial correlation in the errors. This is analogous to the observation that heteroskedasticity in the errors does not cause bias in the $\hat{\beta}_j$.
      \item[consistency:] Even when the data are weakly dependent, the $\hat{\beta}_j$ are still consistent, though not necessarily unbiased if there is that dependency. Just like unbiasedness, this result does not depend on autocorrelation in the errors.
      \item[efficiency:] Since the Gauss-Markov requires both homoskedasticity and serially uncorrelated errors, presence of autocorrelation in errors would make the OLS no longer BLUE. Even more importantly, the usual OLS standard errors and test statistics are not valid, \textit{even asymptotically}.
    \end{itemize}
     
\begin{tcolorbox}[breakable, title=Why not BLUE?, skin=enhancedlast]
To see why OLS is no longer BLUE in the presence of serially correlated errors, consider the simple regression model $Y_t = \beta_0+\beta_1X_t + u_t$ and assume that the error, or disturbance, terms are generated by the following mechanism:
\[
u_t = \rho u_{t-1} + \varepsilon_t, \ \ -1 < \rho < 1
\]
where $\rho$ is the \textit{coefficient of autocovariance}, and $\varepsilon_t$ are uncorrelated random variables that satisfies the OLS assumptions:
\[
\mathbb{E}(\varepsilon_t) = 0 \ \ ; \ \ Var(\varepsilon_t) = \sigma^2 \ \ ; \ \ Cov(\varepsilon_t, \varepsilon_{t-s}) = 0 \ , \ s\neq 0.
\]
Thus the error term in period $t$ is equal to $\rho$ times its value in the preceding period plus a white noise error term. This scheme is known as a \textit{Markov first-order autoregressive scheme}, or just a \textit{first-order autoregressive scheme}, and usually denoted as \textbf{AR(1)}.

Also note that the population correlation coefficient between $u_t$ and $u_{t-1}$ is given by
\[
\rho 
= \frac{\mathbb{E}\Big( \big[ u_t - \mathbb{E}(u_t) \big] \big[ u_{t-1} - \mathbb{E}(u_{t-1}) \big] \Big) }{\sqrt{Var(u_t)}\sqrt{Var(u_{t-1})}} 
= \frac{\mathbb{E}(u_tu_{t-1})}{Var(u_{t-1})}
\]
the latter equality is due to $\mathbb{E}(u_t) = 0$ and $Var(u_t) = Var(u_{t-1})$ because of the homoskedasticity assumption. $\rho$ is also the slope coefficient in the regression of $u_t$ on $u_{t-1}$.

Under the AR(1) scheme we have 
\begin{align*}
Var(u_t) &= \mathbb{E}(u_t^2) = \frac{\sigma_\varepsilon^2}{1-\rho^2} \\[6pt]
Cov(u_t, u_{t-s}) &= \mathbb{E}(u_tu_{t-s}) = \rho^2\frac{\sigma_\varepsilon^2}{1-\rho^2} \\[6pt]
Corr(u_t, u_{t-s}) &= \rho^s
\end{align*}
Since $\rho$ is a constant between $-1$ and $1$, $Var(u_t)$ is still homoskedastic. Notice, however, $u_t$ is correlated not only with its immediate past value but its values several periods in the past. 

If $|\rho| = 1$ then the variances and covariances above are not defined. If $|\rho| < 1$ then the AR(1) process is \textit{stationary}, i.e. mean, variance, and covariance do not change over time. If $|\rho| < 1$ it is also the case that the value of the covariance will decline as we go into distant past.

Considering the simple regression model $Y_t = \beta_0+\beta_1X_t + u_t$ we know from previous supervisions that the OLS estimator of the slope coefficient is
\[
\hat{\beta}_1 = \frac{\sum(X_t - \bar{X})(Y_t - \bar{Y})}{\sum(X_t-\bar{X})^2}
\]
and its variance is
\[
Var(\hat{\beta}_1) = \frac{\sigma^2}{\sum(X_t - \bar{X})^2}
\]
Whereas under the AR(1) scheme, the variance of this estimator is
\begin{align*}
Var(\hat{\beta}_1)^{AR(1)} &= \frac{\sigma^2}{\sum(X_t - \bar{X})^2} \bigg[ 1 \ +\ 2\rho \frac{\sum(X_t - \bar{X})(X_{t-1} - \bar{X})}{\sum(X_t-\bar{X})^2} \ +\ 2\rho^2 \frac{\sum(X_t - \bar{X})(X_{t-2} - \bar{X})}{\sum(X_t-\bar{X})^2} \\
& \qquad + \dots + \ 2\rho^{n-1}\frac{(X_1 - \bar{X})(X_n - \bar{X})}{\sum(X_t-\bar{X})^2} \bigg]
\end{align*}
That is, the variance of $\hat{\beta}_1$ under an $AR(1)$ scheme is equal to its variance under the OLS times a term that depends on $\rho$ as well as the sample autocorrelations between the values taken by the regressor $X$ at various lags. In general, we cannot foretell whether $Var(\hat{\beta}_1)$ is greater or less than $Var(\hat{\beta}_1)^{AR(1)}$.

If we assume that the regressor $X$ also follows AR(1) scheme with a coefficient of autocorrelation, i.e. correlation between $X_t$ and $X_{t-1}$ given as
\[
r = \frac{\sum(X_t - \bar{X})(X_{t-1} - \bar{X})}{\sum(X_t-\bar{X})^2}
\]
then, $Var(\hat{\beta}_1)^{AR(1)}$ reduces to
\begin{align*}
Var(\hat{\beta}_1)^{AR(1)} 
  &= \frac{\sigma^2}{\sum(X_t - \bar{X})^2}\bigg( \frac{1+r\rho}{1-r\rho} \bigg) \\
  &= Var(\hat{\beta}_1)^{OLS}\bigg( \frac{1+r\rho}{1-r\rho} \bigg)
\end{align*}
If we continue to use the OLS estimator $\hat{\beta}_1$ and adjust the usual variance formula by taking account of the AR(1) scheme, $\hat{\beta}_1$ will still be linear and unbiased, but not efficient.

This finding is very similar to the finding that $\hat{\beta}_1$ is less efficient in the presence of heteroskedasticity.
\end{tcolorbox} 
\end{description}



\bigskip\bigskip
***
\bigskip\bigskip

### (b) If $Y_t = \alpha + \beta X_t + \varepsilon_t$ and $\varepsilon_t = \rho\varepsilon_{t-1} + v_t$, where $\mathbb{E}(\varepsilon_t) = 0, \ \rho\neq 0, \ Cov(\varepsilon_t,v_t)=0$ and $v_t \sim i.i.d. N(0, \sigma^2)$, show that:
\begin{itemize}
\item{i)} \textbf{if $\varepsilon_t$ is homoskedastic, $Var(\varepsilon_t) = \sigma^2/(1-\rho^2)$};
\item{ii)} \textbf{$Cov(\varepsilon_t, \varepsilon_{t-1}) \neq 0$};
\item{iii)} \textbf{$Corr(\varepsilon_t,\varepsilon_{t-1})=\rho$, where $Corr$ is the correlation coefficient.}
\end{itemize}

\begin{description}
\item[Answer (i):]
Under AR(1) we have $\varepsilon_t = \rho \varepsilon_{t-1} + v_t$ where $v_t$ is a white noise error term. Since $\varepsilon_t$ is homoskedastic, then
\[
\mathbb{E}(\varepsilon_t) = \rho\mathbb{E}(\varepsilon_{t-1}) + \mathbb{E}(v_t) = 0
\]
which means
\begin{align*}
\mathbb{E}(\varepsilon_t^2) = \rho^2\mathbb{E}(\varepsilon_{t-1}^2) + \mathbb{E}(v_t^2) \\
Var(\varepsilon_t) = \rho^2Var(\varepsilon_{t-1}) + Var(v_t)
\end{align*}
since $\varepsilon$s and $v$s are uncorrelated.

Because of homoskedasticity, notice that $Var(u_t) = Var(u_{t-1}) = \sigma^2$, and since $v_t \sim i.i.d. N(0, \sigma^2)$ its variance is $Var(v_t)= \sigma_v^2$. Plugging these back into the expression above we get
\begin{align*}
Var(\varepsilon_t) 
  &= \rho^2Var(\varepsilon_{t-1}) + Var(v_t) \\
  &= \rho^2Var(\varepsilon_{t}) + \sigma_v^2 \\
Var(\varepsilon_t) - \rho^2Var(\varepsilon_{t})  &= \sigma_v^2 \\
Var(\varepsilon_t)(1-\rho^2) &= \sigma_v^2 \\
Var(\varepsilon_t) &= \frac{\sigma_v^2}{1 - \rho^2}.
\end{align*}
as desired.

\bigskip

\item[Answer (ii):] 
To obtain the covariance, first we need to multiply the AR(1) scheme by $\varepsilon_{t-1}$ and then we can take the expectations of the resulting expression, since that is the covariance. Accordingly,
\begin{align*}
\varepsilon_t &= \rho\varepsilon_{t-1} + v_t \\
\varepsilon_t\varepsilon_{t-1} &= \rho\varepsilon_{t-1}^2 + v_t\varepsilon_{t-1} \\[6pt]
\mathbb{E}(\varepsilon_t\varepsilon_{t-1}) &= \mathbb{E}(\rho\varepsilon_{t-1}^2 + v_t\varepsilon_{t-1}) \\[6pt]
Cov(\varepsilon_t,\varepsilon_{t-1}) = \mathbb{E}(\varepsilon_t\varepsilon_{t-1}) &= \rho\mathbb{E}(\varepsilon_{t-1}^2) + \mathbb{E}(v_t\varepsilon_{t-1}) \\[6pt]
Cov(\varepsilon_t,\varepsilon_{t-1}) &= \rho\mathbb{E}(\varepsilon_{t-1}^2) \ \ \ \ \ \ \text{since } Cov(v_t,\varepsilon_{t-1})=0 \\[6pt]
Cov(\varepsilon_t,\varepsilon_{t-1}) &= \rho\frac{\sigma_v^2}{1 - \rho^2} \ \ \ \ \ \ \text{since } Var(\varepsilon_t) = \frac{\sigma_v^2}{1 - \rho^2}.
\end{align*}
We can then continue in this fashion for the further past periods:
\begin{align*}
Cov(\varepsilon_t,\varepsilon_{t-2}) &= \rho^2 \frac{\sigma_v^2}{1 - \rho^2} \\[6pt]
Cov(\varepsilon_t,\varepsilon_{t-3}) &= \rho^3 \frac{\sigma_v^2}{1 - \rho^2} \\
\vdots
\end{align*}

\bigskip

\item[Answer (iii):] 
Since the correlation coefficient is the ratio of covariance to variance, we get
\[
Corr(\varepsilon_t, \varepsilon_{t-1}) = \frac{Cov(\varepsilon_t,\varepsilon_{t-1})}{Var(\varepsilon_t)} = \frac{\rho\displaystyle\frac{\sigma_v^2}{1 - \rho^2}}{\displaystyle\frac{\sigma_v^2}{1 - \rho^2}} = \rho
\]
and in future periods a similar approach would yield $Corr(\varepsilon_t, \varepsilon_{t-2}) = \rho^2, \ Corr(\varepsilon_t, \varepsilon_{t-3}) = \rho^3, \dots$
\end{description}




\bigskip\bigskip
***
\bigskip\bigskip

### (c) Using the data from the worksheet 'Autocorrelation' estimate the following
\[
gprice = \beta_0 + \beta_1 \ gwage_t + u_t
\]
