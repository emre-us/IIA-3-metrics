---
title: "IIA-3 Econometrics: Supervision 4"
author: "Emre Usenmez"
date: "Christmas Break 2024"
output: pdf_document
header-includes: 
  - \usepackage{amsmath, tcolorbox, dashrule, booktabs, fancyhdr}
  - \tcbuselibrary{listings,most}
  - \allowdisplaybreaks
---

<!-- This comment will not be displayed in the output. Below change to CSS style is to ensure the blocktexts are in the same form size as the rest of the text.-->

```{css style settings, echo = FALSE} 
blockquote {
    padding: 10px 20px;
    margin: 0 0 20px;
    font-size: 14px;
    border-left: 5px solid #eee;
}
```
\pagestyle{fancy}
\fancyhead[L]{2024-25 Part IIA Paper 3}
\fancyhead[R]{Supervision 4 Solutions}
\fancyfoot[L]{University of Cambridge}
\fancyfoot[R]{Emre Usenmez}

\small Very grateful to Dr Oleg Kitov and Dr Clive Lawson for the very informative stylized answers to previous iterations of the supervision questions.
\normalsize


# FACULTY QUESTIONS

\bigskip\bigskip
 

## QUESTION 1

\textbf{Consider the following bivariate linear regression}
\[
y = \alpha + T\beta + u
\]
\textbf{where $T$ is a binary treatment regressor, $\alpha$ and $\beta$ are unknown parameters, and $u$ is an error term.}


\bigskip

### (a) Describe in two sentences an empirical, real-life example where such an equation might arise.

\begin{description}
\item[Answer:] 

We can think of $T$ as "graduated from university" and $y$ as "annual earning after 10 years of graduation."

\end{description}

\bigskip\bigskip
***
\bigskip\bigskip

### (b) Why might $u$ be heteroskedastic in your example.

\begin{description}
\item[Answer:] 

The variance of earnings will likely to be smaller across people who did not graduate from a university compared to those who did it. This may be because those who did not go to university are less likely to be in the professions such as lawyers or doctors, and more likely to be in lower-paying jobs, or unemployed, or out of labor force.

\end{description}


\bigskip\bigskip
***
\bigskip\bigskip

### (c) Why might T be endogenous in your example?

\begin{description}
\item[Answer:] 

Broadly, variables that are correlated with the error term are called \textit{endogeneous variables}, and those that are uncorrelated with the error term are called \textit{exogeneous variables}.\footnote{See Chapter 12: Instrumental Variables Regression p.428 in Stock J H, and Watson M W (2020) Introduction to Econometrics, $4^{th}$ Global Ed, Pearson; and Section 8.5: Instrumental Variables in Dougherty C (2016) Introduction to Econometrics $5^{th}$ ed, OUP in addition to Chapter 9: More on Specification and Data Issues in Wooldridge J M (2021) Introductory Econometrics: A Modern Approach, $7^{th}$ ed, Cengage}

Thus the question is asking us to consider some of the reasons as to why $T$ might be correlated with the error term. There are certainly nonnegligible number of high earners who either never went to a university or dropped out. There may be omitted variable or even simultaneity is possible.

Let's consider what the implications of of endogeneity are for the OLS estimator of $\beta$.

Variable $T$ would be endogenous if $\mathbb{E}(u|T) \neq 0$. Endogeneity would imply that $Cov(T,u)\neq 0$. 

We can first look at whether it is biased. For that, we need to use the law of iterated expectations whereby
\[
\mathbb{E}(\hat{\beta}^{OLS}) = \mathbb{E}\Big[\mathbb{E}(\hat{\beta}^{OLS}\ |\ T_1,\dots,T_n)\Big]
\]
The OLS estimator of $\beta$ would be:
\begin{align*}
\mathbb{E}(\hat{\beta}^{OLS} | T_1,\dots,T_n) 
  &= \mathbb{E}\Bigg( \frac{\widehat{Cov}(T_i,Y_i)}{\widehat{Var}(T_i)} \ \Bigg{|}\ T_1,\dots,T_n \Bigg) = \mathbb{E} \Big( \frac{\hat{\sigma}_{TY}}{\hat{\sigma}_{TT}} \ \bigg{|}\ T_1,\dots,T_n \Big) \\[4pt]
  &= \mathbb{E}\left( \frac{\displaystyle\sum_{i=1}^n(T_i-\bar{T})(Y_i-\bar{Y})}{\displaystyle\sum_{i=1}^n(T-\bar{T})^2}\ \Bigg{|}\  T_1,\dots,T_n \right) \\[4pt]
  &= \mathbb{E}\left( \frac{\displaystyle\sum_{i=1}^n(T_i-\bar{T})\big((\alpha + \beta T_i + u_i) - (\alpha + \beta \bar{T} + \bar{u})\big)}{\displaystyle\sum_{i=1}^n(T-\bar{T})^2} \ \Bigg{|}\ T_1,\dots,T_n \right)\\[4pt]
  &= \mathbb{E}\left( \frac{\displaystyle\sum_{i=1}^n(T_i-\bar{T})\big(\beta(T_i-\bar{T}) + u_i - \bar{u}\big)}{\displaystyle\sum_{i=1}^n(T-\bar{T})^2}  \ \Bigg{|}\ T_1,\dots,T_n \right) \\[4pt]
  &= \mathbb{E}\left( \frac{\displaystyle\sum_{i=1}^n \beta(T_i-\bar{T})^2 + \displaystyle\sum_{i=1}^n(T_i-\bar{T})(u_i - \bar{u})}{\displaystyle\sum_{i=1}^n(T-\bar{T})^2} \ \Bigg{|}\ T_1,\dots,T_n \right) \\[4pt]
  &= \mathbb{E}\left( \beta + \frac{\displaystyle\sum_{i=1}^n(T_i-\bar{T})(u_i-\bar{u})}{\displaystyle\sum_{i=1}^n(T_i-\bar{T})^2} \ \Bigg{|}\ T_1,\dots,T_n \right) \\[4pt]
  &= \mathbb{E}\left( \beta + \frac{\displaystyle\sum_{i=1}^n(T_i-\bar{T})u_i - \bar{u}\displaystyle\sum_{i=1}^n(T_i-\bar{T})}{\displaystyle\sum_{i=1}^n(T_i-\bar{T})^2} \ \Bigg{|}\ T_1,\dots,T_n \right) \\[4pt]
  &= \mathbb{E}\left(\beta + \frac{\displaystyle\sum_{i=1}^n(T_i-\bar{T})u_i - \bar{u}\left(\sum_{i=1}^nT_i-n\bar{T}\right)}{\displaystyle\sum_{i=1}^n(T_i-\bar{T})^2} \ \Bigg{|}\ T_1,\dots,T_n \right) \\[4pt]
  &= \mathbb{E}\left( \beta + \frac{\displaystyle\sum_{i=1}^n(T_i-\bar{T})u_i - \bar{u}\left(n\bar{T}-n\bar{T})\right)}{\displaystyle\sum_{i=1}^n(T_i-\bar{T})^2} \ \Bigg{|}\ T_1,\dots,T_n \right) \\[4pt]
  &= \mathbb{E}\left( \beta + \frac{\displaystyle\sum_{i=1}^n(T_i-\bar{T})u_i}{\displaystyle\sum_{i=1}^n(T_i-\bar{T})^2} \ \Bigg{|}\ T_1,\dots,T_n \right) \\[4pt]
  &= \beta + \frac{\displaystyle\sum_{i=1}^n(T_i-\bar{T})\ \mathbb{E}(u_i \ |\ T_1,\dots,T_n)}{\mathbb{E}\left( \displaystyle\sum_{i=1}^n(T_i-\bar{T})^2 \ \Big{|}\ T_1,\dots,T_n \right)} \\[4pt]
\end{align*}
Notice that since $\mathbb{E}(u|T) \neq 0$, the numerator of this last expression is also nonzero. That is, $\sum_{i=1}^n(T_i-\bar{T})\ \mathbb{E}(u_i \ |\ T_1,\dots,T_n) \neq 0$. Therefore the expectation of this expectation is also not equal to $\beta$:
\[
\mathbb{E}(\hat{\beta}^{OLS}) = \mathbb{E}\Big[\mathbb{E}(\hat{\beta}^{OLS}\ |\ T_1,\dots,T_n)\Big] = \mathbb{E}\left[ \mathbb{E}\left( \beta + \frac{\displaystyle\sum_{i=1}^n(T_i-\bar{T})u_i}{\displaystyle\sum_{i=1}^n(T_i-\bar{T})^2} \ \Bigg{|}\ T_1,\dots,T_n \right] \right) \neq \beta
\]
which means the OLS estimator is \textit{not} unbiased.

We can also check for consistency by examining the probability limit of this expression as $n$ tends towards infinity. For that, we can rewrite the OLS estimator as:
\[
\hat{\beta}^{OLS} = \beta + \frac{\displaystyle\frac{1}{n}\displaystyle\sum_{i=1}^n(T_i-\bar{T})u_i}{\displaystyle\frac{1}{n}\displaystyle\sum_{i=1}^n(T_i-\bar{T})^2} 
\]
Using the law of large numbers, we can see that as $n \to \infty$
\begin{align*}
\frac{1}{n}\sum_{i=1}^n(T_i-\bar{T})u_i &\overset{p}{\to} \mathbb{E}\big[(T_i - \bar{T})u_i\big] = Cov(T_i,u_i)\neq 0 \\
\text{and}\\
\frac{1}{n}\sum_{i=1}^n(T_i-\bar{T})^2 &\overset{p}{\to} \mathbb{E}\Big[(T_i-\bar{T})^2\Big] = Var(T_i) = \sigma^2_T < \infty
\end{align*}
Note that $Var(T_i) = \sigma^2_T < \infty$ is an additional assumption.

Since $Cov(T_i,u_i)\neq 0$, the OLS estimator as $n \to \infty$ (using Slutsky's theorem):
\[
\hat{\beta}^{OLS} \overset{p}{\to}\beta+\frac{Cov(T_i,u_i)}{Var(T_i)}\neq \beta
\]
which means the OLS estimator is not only biased but also inconsistent for $\beta$.
\end{description}

\bigskip\bigskip
***
\bigskip\bigskip

### (d) Suppose a single instrument $z$ is available. Show that the population coefficient $\beta$ satisfies
\[
\beta = \frac{Cov(z,y)}{Cov(z,T)}
\]
\textbf{where $Cov(z,y)$ and $Cov(z,T)$ denote, respectively, the population covariance between $z$ and $y$, and $z$ and $T$. How can you use this information to obtain a consistent estimate of $\beta$?}
\begin{description}
\item[Answer:]
Instrument $z$ needs to satisfy the following conditions:
\begin{itemize}
\item \textit{Instrument relevance}: $z$ must have non-trivial explanatory power for $T$, namely $Cov(z,T)\neq 0$.
\item \textit{Instrument exogeneity}: $z$ must affect $Y$ only through its influence on $T$ and not in any other way. That is, $z$ must be exogenous with respect to $u$ in regression $y = \alpha + \beta T + u$. Formally, $\mathbb{E}(u|z)=0$. This is why it is said "z is exogenous in $y = \alpha + \beta T + u$. Exogeneity of instrument $z$ implies that $Cov(z, u)=0$.
\end{itemize}

In the context of omitted variables, instrument exogeneity means that $z$ should be uncorrelated with the omitted variables, i.e. $Cov(z,u)=0$, and $z$ should be related, positively or negatively, to the endogeneous explanatory variable $T$, i.e. $Cov(z,T)\neq 0$.\footnote{see Section 15-1: Omitted Variables in a Simple Regression Model in Wooldridge J M (2021) Introductory Econometrics: A Modern Approach, $7^{th}$ ed, Cengage}

The underlying reasoning is that if an instrument is relevant, then variation in that instrument $z$ is related to variation in $T$, and if it is also exogeneous, then that part of the variation of $T$ captured by $z$ is exogeneous. Therefore, an instrument that is relevant and exogeneous can capture movements in $T$ that are exogeneous. This exogeneous variation can in turn be used to estimate the population coefficient $\beta$.\footnote{see Section 12.1: The IV Estimator with a Single Regressor and a Single Instrument in Stock and Watson (2020, $4^{th}$ ed.).}  

These conditions serve to \textit{identify} the parameter $\beta$. In this context, \textit{identification of a parameter} means that we can write $\beta$ in terms of population moments that can be estimated using a sample of data. 

To write $\beta$ in terms of population covariances we use $y = \alpha + \beta T + u$:
\[
Cov(z,y) = Cov(z, \ \alpha+\beta T + u) = \beta Cov(z,T) + Cov(z,u)
\]

Since instrument exogeneity condition assumes that $Cov(z,u)=0$ then $Cov(z,y)=\beta Cov(z,T)$. Rearranging this gives:
\[
\beta=\frac{Cov(z,y)}{Cov(z,T)}
\]
as desired. Notice that this only holds if instrument relevance also holds, since this expression would fail if $Cov(z,T)=0$. What this expression is telling us is that $\beta$ is identified by the ratio of population covariance between $z$ and $y$ to population covariance between $z$ and $T$. 

Given a random sample, we estimate the population quantities by the sample analogs:
\[
\hat{\beta}^{IV} = \frac{\displaystyle\frac{1}{n}\displaystyle\sum_{i=1}^n(z_i-\bar{z})(y_i-\bar{y})}{\displaystyle\frac{1}{n}\displaystyle\sum_{i=1}^n(z_i-\bar{z})(T_i-\bar{T})} = \frac{\displaystyle\sum_{i=1}^n(z_i-\bar{z})(y_i-\bar{y})}{\displaystyle\sum_{i=1}^n(z_i-\bar{z})(T_i-\bar{T})}.
\]
With a sample data on $T, y,$ and $z$ we can obtain the IV estimator above. The IV estimator for the intercept $\alpha$ is $\alpha = \bar{y} - \hat{\beta}^{IV}\bar{T}$. Also notice that when $z=T$, we get the OLS estimator of $\beta$. That is, when $T$ is exogeneous, it can used as its own IV, and the IV estimator is then identical to the OLS estimator.

A similar set of steps we used in part (c) will show that IV estimator is consistent for $\beta$. That is, $\underset{n\to \infty}{plim}(\hat{\beta}) = \beta$.

Note that, an important feature of IV estimator is that when $T$ and $u$ are in fact correlated, and thus instrumental variables estimation is actually needed, it is essentially \underline{never unbiased}. This means, in small samples, the IV estimator can have a substantial bias, which is one reason why large samples are preferred. 
\end{description}

\bigskip\bigskip
***
\bigskip\bigskip

### (e) Can you give an example of an instrument in your example? Argue why it might be a sensible IV.
\begin{description}
\item[Answer:]
Distance from nearest college can be an example of an instrument, where $z=1$ if individual lived near college and $0$ otherwise. This may be violated for a number of reasons, though; for e.g. if wealthy parents choose to live near college. This would mean that $z$ is correlated with unobserved factors that also affect wage, our $y$. For any example, exogeneity and relevance conditions need to be checked.
\end{description}




\bigskip\bigskip\bigskip
\hdashrule[0.5ex]{\textwidth}{1pt}{3mm}
\bigskip\bigskip\bigskip


## QUESTION 2

\textbf{Consider the following wage equation that explicitly recognizes that ability affects $log(wage)$}
\[
log(wage) = \alpha + \beta_1 educ + \beta_2 ability + u
\]
\textbf{The above model shows explicitly that we would like to hold ability fixed when measuring the returns on education. Assuming that the primary interest is in obtaining a reliable estimate of the slope parameters $\beta_1$, and that there is no direct measurement for ability, explain how you would do this using a method based upon a proxy variable and an IV estimator. In doing so evaluate the following statement:}

"\textbf{\textit{whilst $IQ$ is a good candidate as a proxy for variable for ability, it is not a good instrumental variable for $educ$.}}"

\begin{description}
\item[Answer:] 
This question is essentially aiming to ensure the students understand the difference between proxy variable and instrumental variable.
\begin{itemize}
\item[proxy variable] refers to an \textit{observed} variable that is correlated with but not identical to the \textit{unobserved} variable.
\item[instrumental variable] refers to a variable that does not appear in the regression, uncorrelated with the error in the equation, and partially correlated with the endogenous explanatory variable in an equation where such endogenous explanatory variable exists.
\end{itemize}


\underline{Proxy Variable:}

Notice in this question $educ$ is observed but $ability$ is unobserved, and we would not even know how to interpret it's coefficient $\beta_2$ since 'ability' itself is a vague concept. We can instead use intelligence quotient, or $IQ$, as a proxy for ability as long as $IQ$ is correlated with ability. This is captured by the following simple regression:
\[
ability = \delta_0 + \delta_2IQ + v_2
\]
where $v_2$ is an error due to the fact that $ability$ and $IQ$ are not exactly related. The parameter $\delta_2$ measures the relationship between $ability$ and $IQ$. If $\delta_2=0$ then $IQ$ is not a suitable proxy for $ability$.

Note that the intercept $\delta_0$ allows $ability$ and $IQ$ to be measured on different scales and thus can be positive or negative. That is, the unobserved $ability$ is not required to have the same average value as $IQ$ in the population.

To use $IQ$ to get unbiased, or at least consistent, estimators for $\beta_1$, the coefficient of $educ$, we would regress $log(wage)$ on $educ$ and $IQ$. This is called \textit{the plug-in solution to the omitted variables problem} since we plug-in $IQ$ for $ability$ before running the OLS. However, since $IQ$ and $educ$ are not the same, we need to check if this does give consistent estimator for $\beta_1$. 

For the plug-in solution to provide consistent estimator for $\beta_1$ the following two assumptions need to hold true:
\begin{itemize}
\item The error $u$ is uncorrelated with $educ$ and $ability$ as well as $IQ$. That is, $\mathbb{E}(u|educ,ability,IQ)=0$. What this means is that $IQ$ is irrelevant in the population model which is true by definition since $IQ$ is a proxy for $ability$, it is $ability$ that directly affects $log(wage)$ not $IQ$.
\item The error $v_2$ is uncorrelated with $educ$ and $IQ$. For $v_2$ to be a uncorrelated with $educ$, $IQ$ needs to be a 'good' proxy for $ability$. 
\end{itemize}

What is meant by 'good' proxy in this sense is that
\[
\mathbb{E}(ability\ |\ educ,IQ) = \mathbb{E}(ability\ |\ IQ) = \delta_0 + \delta_2IQ.
\]
Here, the first equality, which is the most important one, says that once $IQ$ is controlled for, the expected value of $ability$ does not depend on $educ$. In other words, $ability$ has zero correlation with $educ$ once $IQ$ is partialled out. Thus the average level of $ability$ only changes with $IQ$ and not with $educ$.

To see why these two assumptions are enough for the plug-in solution to work, we can rewrite the $log(wage)$ equation in the question as:
\begin{align*}
log(wage) 
  &= \alpha + \beta_1 educ + \beta_2 ability + u \\
  &= \alpha + \beta_1 educ + \beta_2 (\delta_0 + \delta_2IQ + v_2) + u \\
  &= (\alpha + \beta_2\delta_0) + \beta_1 educ + \beta_2\delta_2IQ + u + \beta_2v_2 \\
  &= (\alpha + \beta_2\delta_0) + \beta_1 educ + \beta_2\delta_2IQ + \epsilon \\
  &= \gamma_0 + \beta_1 educ + \gamma_2 IQ + \epsilon.
\end{align*}

Notice that the composite error $\epsilon$ depends on both the error in the model of interest in the question, $u$, and on the error in the proxy variable equation, $v_2$. Since both $u$ and $v_2$ have zero mean and each is uncorralated with $educ$ and $IQ$, $\epsilon$ also has zero mean and is uncorralted with $educ$ and $IQ$.

So when we regress $log(wage)$ on $educ$ and $IQ$, we will \underline{not} get unbiased estimators of $\alpha$ and $\beta_2$. Instead, we will get unbiased, or at least consistent, estimators of $\gamma_0, \beta_1,$ and $\gamma_2$. The important thing is that we get good estimators of $\beta_1$. 

In most cases, the estimate of $\gamma_2$ is more interesting than an estimate of $\beta_2$ anyway, since $\gamma_2$ measures the return to $log(wage)$ given one more point on $IQ$ score.


\begin{tcolorbox}[breakable, title=Bias and Multicollinearity when using a proxy, skin=enhancedlast]
\textbf{When using a proxy variable can still lead to bias?}

If the two assumptions above are not satisfied, then using a proxy variable can lead to a bias. To see this, suppose now that ability is not only related to $IQ$ but also to $educ$:
\[
ability = \delta_0 + \delta_1 educ + \delta_2 IQ + v_3
\]
where the error $v_3$ has a zero mean and uncorrelated with $educ$ and $IQ$. In the proxy variable discussion above, it was essentially assumed that $\delta_1=0$. We can re-write $log(wage)$ with this plug-in solution:
\begin{align*}
log(wage) 
  &= \alpha + \beta_1 educ + \beta_2 ability + u \\
  &= \alpha + \beta_1 educ + \beta_2 (\delta_0 + \delta_1 educ + \delta_2 IQ + v_3) + u \\
  &= (\alpha + \beta_2\delta_0) + (\beta_1 + \beta_2\delta_1) educ + \beta_2\delta_2 IQ + u + \beta_2v_3 \\
\end{align*}
Since the error term $u+\beta_2v_3$ has zero mean and is uncorrelated with $educ$ and $IQ$, we have $plim(\hat{\beta}_1)=\beta_1 + \beta_2\delta_1$. If $educ$ is partially and positively correlated with $ability$, i.e. $\delta_1 > 0$, and if the coefficient of $ability$ is positively correlated with $log(wage$, i.e. $\beta_2 > 0$, then $plim(\hat{\beta}_1)=\beta_1 + \beta_2\delta_1 > \beta_1$ giving us an upward bias. That is, in this case where $IQ$ is not a good proxy for $ability$ but we still use it, then we'd still be getting an upward bias for the coefficient of $educ$.

Having said that, the bias is likely to be smaller than if we ignored the problem of omitted ability entirely.

\bigskip

\textbf{What about multicollinearity?}

Even if $IQ$ is a good proxy for $ability$, using it in a regression that includes $educ$ can exacerbate the multicollinearity problem, which, in turn, is likely to lead a less precise estimate of the coefficient for $educ$, i.e. $\beta_1$.

However, notice that
\begin{itemize}
\item inclusion of $IQ$ in the regression means that the part of $ability$ explained by $IQ$ is removed from the error term, reducing the error variance. This is likely to be reflected in a smaller standard error of the regression, though that reduction may not happen because of degrees of freedom adjustment.

\item if we want to have a less bias for $\beta_1$, ie, the estimator of the coefficient for $educ$, then we have to live with increased multicollinearity. This is an important point. Since $educ$ and $ability$ are thought to be correlated, and if we could include $ability$ in the regression, then there would be inevitable multicollinearity caused by the correlation between these two variables. Since $IQ$ is a proxy for $ability$, $educ$ and $IQ$ are also correlated, and a similar argument ensues.
\end{itemize}
\end{tcolorbox}

\bigskip

\underline{Instrumental Variable}

Suppose now that the proxy variable does not have the required properties for a consistent estimator of $\beta_1$. Then we put $ability$ in the error term since it is unobserved and we don't have a proxy for it. This leaves us with:
\[
log(wage) = \beta_0 + \beta_1 educ + \epsilon
\]
where $\epsilon$ contains $ability$. If $ability$ and $educ$ are correlated, then we have a biased and inconsistent estimate of $\beta_1$.

However, we can still use this equation as the basis for estimation as long as we can find an instrumental variable for $educ$. For this we can introduce an \textit{instrumental variable} $z$ which satisfies the "instrument relevance", i.e. $Cov(z, educ)\neq 0$, and "instrument exogeneity", i.e. $Cov(z,\epsilon)=0$ conditions as discussed in Question 1(d).

Note that we cannot really test for "instrument exogeneity" assumption and need to consider economic behavior in order to maintain the $Cov(z, \epsilon)=0$ assumption. At times, there may be an observable proxy for some factor contained in $\epsilon$ and we can check if $z$ and the proxy variable are more or less uncorrelated. And, of course, as discussed above, if we have a good proxy then we would add that variable to the equation and estimate the expanded form by OLS. 

This is exactly where we see a tension between a good proxy vs a good instrumental variable. For $IQ$ to be a good proxy, it needs to be as highly correlated with $ability$ as possible. Yet for $IQ$ to be a good instrumental variable, it needs to be uncorrelated with $ability$ since $ability$ is contained in $\epsilon$ and a good instrumental variable should not covary with the error term. That is, a good instrumental variable should affect $log(wage)$ only through its influence on $educ$ and not in any other way.

Thus, in this question, although $IQ$ is a good candidate as a proxy variable for $ability$, it is not a good instrumental variable for $educ$.

\end{description}








\bigskip\bigskip\bigskip
\hdashrule[0.5ex]{\textwidth}{1pt}{3mm}
\bigskip\bigskip\bigskip


## QUESTION 3
\textbf{The following regression explores the relationship between television watching and childhood obesity, using a cross-section of US children. The variables are:}

\begin{tabular}{llccc}
Name   & Description & Minimum & Maximum & Mean \\ 
\midrule
tvyest & hours of TV watched yesterday & 0  & 6  & 3.14 \\
black  & dummy, 1 if black             & 0  & 1  & 0.31 \\
hisp   & dummy, 1 if hispanic          & 0  & 1  & 0.36 \\
ageyrs & age in years                  & 5  & 16 & 9.4  \\
bmi    & child's Body Mass Index       & 11 & 55 & 19   \\
dadbmi & father's BMI                  & 11 & 58 & 26   \\
mombmi & mother's BMI                  & 14 & 56 & 26   \\
\midrule
\end{tabular}

```{r include=FALSE}
libraries <- c("haven",       # to import/export SPSS, STATA, SAS files
               "readxl",      # to import/export Excel files   
               "tidyverse",   # for tidy data
               "Statamarkdown", # for using STATA commands in R
               "ivreg", #for regressions with instrumental variables
               "mnormt", #for creating bivariate normal distributions
               "kableExtra", # for creating nice tables in R
               "rstatix")     # converts stats functions to a tidyverse-friendly format

invisible(lapply(libraries, library, character.only=TRUE))  # will load the libraries

```

\bigskip

The output from a 2SLS regression appears below:


```{stata echo=FALSE}
* Change the working directory to access the data file:
    quietly cd .. 
* Load the data:
    use Data/obesekids.dta

* Run the instrumental variables regression
    ivregress 2sls tvyest black hisp ageyrs (bmi = dadbmi mombmi)

```

\textbf{Now answer the following questions.}

### (a) Why might an OLS regression of $tvyest$ on the child's BMI give us inconsistent estimates of the causal effect of BMI on TV watching?

\begin{description}
\item[Answer:]
Recall that correlation between the error term and any of the regressors generally causes all of the OLS estimators to be inconsistent. In fact, if the error term is correlated with any of the independent variables, then OLS is both biased and inconsistent. This means any bias persists even as the sample size grows.

Here, if we only regress $tvyest$ on $bmi$ then inevitably all the omitted variables would be contained in the error term and they would be correlated with $bmi$, which would give us inconsistent estimates of the causal effect of $bmi$ on tv watching.
\end{description}


\bigskip\bigskip
***
\bigskip\bigskip

### (b) Interpret the coefficient $0.73$ on $black$.
\begin{description}
\item[Answer:]
The coefficient implies that holding other variables constant, black children watched on average about 0.73 hours more tv than non-black children.
\end{description}


\bigskip\bigskip
***
\bigskip\bigskip

### (c) Can you state a reason why we may doubt the validity of the 2SLS estimates reported above?
\begin{description}
\item[Answer:]
In the least, the 2SLS estimation method have the following assumptions:

\begin{itemize}
\item the error term of the structural equation is uncorrelated with each of the exogenous explanatory variables
\item there exists at least one exogenous variable that is partially correlated with the endogenous variable in the structural equation but itself is not in the structural equation to ensure consistency
\item the structural error term cannot depend on any of the exogeneous variables, i.e. homoskedasticity assumption. This ensures the 2SLS standard errors and $t$-statistics to be asymptotically valid. 
\end{itemize}

\item Violation of any one of these assumptions would make us doubt the validity of the 2SLS estimates reported above.
\end{description}















































\pagebreak

# SUPPLEMENTARY QUESTIONS

\bigskip\bigskip

## QUESTION 1

\textbf{Consider the simple regression model:}
\begin{equation} \label{eq:SQ1}
Y_i = \alpha + \beta X_i + \varepsilon_i, \ \ i = 1,2,\dots,m
\end{equation}
\textbf{where $Y_i$ is the mean expenditure on alcohol in group $i$ and $X_i$ is the mean income of group $i$. Each group $i$ has $N_i$ members and the model satisfies all the classical assumptions except that the variance of $\varepsilon_i$ is equal to $\sigma^2/N_i$.}


### (a) What are the statistical properties of the OLS estimates of $\alpha$ nad $\beta$ in this case?

\begin{description}
\item[Answer:] 
Recall that when demonstrating unbiasedness and consistency of OLS estimators, homoskedasticity assumption did not play any role. That is, if the variance of the unobserved error is not constant, i.e. heteroskedastic, it does not impact whether an estimator is unbiased or consistent. Similarly, the interpretation of the goodness-of-fit measures, $R^2$ and $\bar{R}^2$, are also unaffected by the presence of heteroskedasticity.

The problem with the presence of heteroskedasticity is that the estimators of the variances are biased. Since the OLS standard errors are based on these variances, they are no longer valid for constructing confidence intervals and $t$-statistics. In this situation the OLS $t$-statistics do not have $t$ distributions and the problem is not resolved by increasing the sample size. Similarly, $F$-statistics are not longer $F$-distributed. Finally, the OLS is no longer BLUE as it is no longer asymptotically efficient.

Recall that the OLS estimator is
\[
\hat{\beta} = \beta + \frac{\displaystyle\sum_{i=1}^m (X_i - \bar{X})\varepsilon_i}{\displaystyle\sum_{i=1}^m (X_i - \bar{X})^2} = \beta + \frac{\displaystyle\sum_{i=1}^m (X_i - \bar{X})\varepsilon_i}{{SST}_X^2} 
\]

and its variance when homoskedasticity is present is
\begin{align*}
Var(\hat{\beta})  
  &= Var \left( \beta + \frac{\displaystyle\sum_{i=1}^m (X_i - \bar{X})\varepsilon_i}{\displaystyle\sum_{i=1}^m (X_i - \bar{X})^2} \right) \\[6pt] 
  &= Var \left(\frac{\displaystyle\sum_{i=1}^m (X_i - \bar{X})\varepsilon_i}{\displaystyle\sum_{i=1}^m (X_i - \bar{X})^2} \right) \ \ \ \ \text{since $\beta$ is constant} \\[6pt] 
  &= \left(\frac{1}{\displaystyle\sum_{i=1}^m (X_i - \bar{X})^2} \right)^2 \ Var\Big( \displaystyle\sum_{i=1}^m (X_i - \bar{X})\varepsilon_i \Big)  \ \ \ \ \text{since we are conditioning on $X_i$, $SST_X$ is nonrandom}   \\[6pt]
  &= \Big( \frac{1}{SST_X} \Big)^2 \Big( \displaystyle\sum_{i=1}^m (X_i - \bar{X})^2 \ Var(\varepsilon_i) \Big) \ \ \ \ \text{since we are conditioning on $X_i$, $X_i - \bar{X}$ is nonrandom} \\[6pt]
  &= \Big( \frac{1}{SST_X} \Big)^2 \Big( \displaystyle\sum_{i=1}^m (X_i - \bar{X})^2\sigma_\varepsilon^2 \Big)  \ \ \ \ \text{since $Var(\varepsilon_i) = \sigma^2$ for all $i$ when homoskedastic} \\[6pt]
  &= \sigma_\varepsilon^2 \Big( \frac{1}{SST_X}\Big)^2 SST_X \\[6pt]
  &= \frac{\sigma_\varepsilon^2}{SST_X} 
  = \frac{\sigma_\varepsilon^2}{\displaystyle\sum_{i=1}^m(X_i - \bar{X})^2}
\end{align*}

and its variance when heteroskedasticity is present is
\[
Var(\hat{\beta}) 
  = \Big( \frac{1}{SST_X} \Big)^2 \Big( \displaystyle\sum_{i=1}^m (X_i - \bar{X})^2\sigma_i^2 \Big) 
  = \frac{\displaystyle\sum_{i=1}^m (X_i - \bar{X})^2\sigma_i^2}{\Big( \displaystyle\sum_{i=1}^m(X_i - \bar{X})^2 \Big)^2}.
\]

\underline{Spherical Errors}

We assume homoskedasticity and no autocorrelation in estimating the variance of OLS estimates. That is, we assume that all errors have the same variance $\sigma^2$ and that there is no correlation across errors. If these hold true, then we have \textit{spherical errors}, or that the error term follows a \textit{spherical distribution}. This is represented in matrix form as follows:
\begin{equation*}
\mathbb{E}(\vec{u}\vec{u}^T | \mathbf{X}) = 
\begin{bmatrix}
\sigma^2 & 0 & \dots & 0 \\
0 & \sigma^2 & \dots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \dots & \sigma^2
\end{bmatrix}
= \sigma^2 \mathbf{I}
\end{equation*}

To see why this is called a spherical distribution lets look at a special case of two dimensions, i.e. circular distribution, as opposed to three dimensions for spherical distribution. Consider two random errors, $u_i$ and $u_j$ which are graphed below as density plots and contour plots, the latter of which shows what you'd see when you look straight down from the top of the density plot.

The shapes of these plots depend on the variances and covariances of these two random errors. If $u_i$ and $u_j$ are homoskedastic and they are not autocorrelated, then the contour lines will be circles. If there were three random error variables $u_i$, $u_j$, and $u_k$ then we would have four-dimensional density plot and the contours would form a sphere. If there were more than three random error variables then the contours would form a hyper-sphere. This is why the errors are spherically distributed.

What we are plotting is therefore:
\begin{equation*}
\mathbb{E}
\begin{pmatrix}
u_i \\
u_j
\end{pmatrix}
=
\begin{pmatrix}
0 \\
0
\end{pmatrix}
\ \ \ \ ; \ \ \ \
Var
\begin{pmatrix}
u_i \\
u_j
\end{pmatrix}
=
\begin{pmatrix}
1 & 0 \\
0 & 1
\end{pmatrix}
\end{equation*}

where errors are homoskedastic and there is no autocorrelation.

\end{description}


```{r echo=FALSE}
#build bivariate normal distribution
x <- seq(-3,3,0.25)
y <- seq(-3,3,0.25)
mu <- c(0,0)
sigma <- matrix(c(1,0,0,1), nrow = 2)
f <- function(x,y) dmnorm(cbind(x,y),mean = mu, varcov = sigma)
z <- outer(x,y,f)

#plot side by side
par(mfrow = c(1,2))
#density plot
persp(x,y,z, main="Density",  xlab = expression(error[i]), ylab=expression(error[j]), zlab=expression('density'))
#contour plot
contour(x,y,z, main="Contour", axes=TRUE, xlab=expression(error[i]),ylab=expression(error[j]))


```


If on the other hand, heterskedasticity is present then we loose the symmetry of the joint density plot and get a more elliptic contours. Suppose now the variance-covariance matrix is as follows:
\begin{equation*}
Var
\begin{pmatrix}
u_i \\
u_j
\end{pmatrix}
=
\begin{pmatrix}
0.25 & 0 \\
0 & 2
\end{pmatrix}
\end{equation*}

```{r echo=FALSE}
#build bivariate normal distribution
x <- seq(-3,3,0.25)
y <- seq(-3,3,0.25)
mu <- c(0,0)
sigma <- matrix(c(0.25,0,0,2), nrow = 2)
f <- function(x,y) dmnorm(cbind(x,y),mean = mu, varcov = sigma)
z <- outer(x,y,f)

#plot side by side
par(mfrow = c(1,2))
#density plot
persp(x,y,z, main="Density",  xlab = expression(error[i]), ylab=expression(error[j]), zlab=expression('density'))
#contour plot
contour(x,y,z, main="Contour", axes=TRUE, xlab=expression(error[i]),ylab=expression(error[j]))

?contour
```

Similarly, we would also get an elliptic contours if the errors are homoskedastic but there is autocorrelation. The slope of the main axis of the ellipse would depend on the sign of the correlation between the errors. Suppose now the variance-covariance matrix is as follows:
\begin{equation*}
Var
\begin{pmatrix}
u_i \\
u_j
\end{pmatrix}
=
\begin{pmatrix}
0.1 & -0.5 \\
-0.5 & 1
\end{pmatrix}
\end{equation*}

```{r echo=FALSE}
#build bivariate normal distribution
x <- seq(-3,3,0.25)
y <- seq(-3,3,0.25)
mu <- c(0,0)
sigma <- matrix(c(1,-0.5,-0.5,1), nrow = 2)
f <- function(x,y) dmnorm(cbind(x,y),mean = mu, varcov = sigma)
z <- outer(x,y,f)

#plot side by side
par(mfrow = c(1,2))
#density plot
persp(x,y,z, main="Density",  xlab = expression(error[i]), ylab=expression(error[j]), zlab=expression('density'))
#contour plot
contour(x,y,z, main="Contour", axes=TRUE, xlab=expression(error[i]),ylab=expression(error[j]))


```



\bigskip\bigskip
***
\bigskip\bigskip


### (b) How should equation \ref{eq:SQ1} on page \pageref{eq:SQ1} be transformed so that the OLS estimates of $\alpha$ and $\beta$ are BLUE?
\begin{description}
\item[Answer:] The variances of the error terms are given in the question, thus \textit{known}. We can therefore estimate using the \textit{generalized least squares (GLS)} estimation method where we minimize a \textit{weighted sum of squared residuals.}
$\hookrightarrow$ For remedial measures when $\sigma_i^2$ is unknown, see Question 2 below.

 
\[
Var(\varepsilon_i) = \frac{\sigma^2}{N_i} = \sigma_i^2
\]
So we transform equation \ref{eq:SQ1} on page \pageref{eq:SQ1} by dividing it with theses known standard deviations, $\sigma_i$:
\[
\frac{Y_i}{\sigma_i} = \frac{\alpha}{\sigma_i} + \beta \frac{X_i}{\sigma_i} + \frac{\varepsilon_i}{\sigma_i}
\]
so that
\begin{align*}
Var(\frac{\varepsilon_i}{\sigma_i}) 
  &= \mathbb{E}\Bigg[ \bigg( \frac{\varepsilon_i}{\sigma_i} \bigg)^2 \Bigg] - \Bigg[ \mathbb{E}\bigg( \frac{\varepsilon_i}{\sigma_i} \bigg) \Bigg]^2 \\[6pt]
  &= \mathbb{E}\Bigg[ \bigg( \frac{\varepsilon_i}{\sigma_i} \bigg)^2 \Bigg] \ \ \ \ \text{since } \mathbb{E}\bigg( \frac{\varepsilon_i}{\sigma_i} \bigg) = 0 \\[6pt]
  &= \frac{1}{\sigma_i^2}\mathbb{E}(\varepsilon_i^2) \ \ \ \ \text{since $\sigma_i^2$ is known, thus it is a collection of constants} \\[6pt] 
  &= \frac{1}{\sigma_i^2}\sigma_i^2 = 1
\end{align*}
which is a constant. This means, the variance of the transformed disturbance term $\frac{\varepsilon_i}{\sigma_i}$ is now homoskedastic. Since all the other assumptions of classical model still hold true, this means that if we apply OLS method to the transformed model, we will get estimators that are BLUE.

Thus, GLS is OLS on the transformed variables that satisfy the standard least-squares assumptions. The estimators that are obtained these way are GLS estimators which are BLUE.
\end{description}




\bigskip\bigskip
***
\bigskip\bigskip

### (c) Derive $\hat{\alpha}$ in terms of $\hat{\beta}$ in this case.

\begin{description}
\item[Answer:]
In this case, what we want is a transformation of the equation \ref{eq:SQ1} on page \pageref{eq:SQ1} in such a way that the variance of the transformed error, $Var(\varepsilon_i*)$, is constant $\sigma^2$. 

For this, we can work backwards. We know that $Var(\varepsilon_i) = \mathbb{E}(\varepsilon_i^2) = \frac{\sigma^2}{N_i}$ so if the transformation resulted in $Var(\varepsilon_i^*)=N_i\mathbb{E}(\varepsilon_i^2)$ then it would equal to constant $\sigma^2$. Fro that to happen, we can set $\varepsilon_i^* = \varepsilon_i\sqrt{N_i}$, so that
\[
Var(\varepsilon_i^*) = \mathbb{E}\big((\varepsilon_i^*)^2\big) - \big[\mathbb{E}(\varepsilon_i^*)\big]^2 = \mathbb{E}\big((\varepsilon_i^*)^2\big) = \mathbb{E}\Big((\varepsilon_i\sqrt{N_i})^2\Big) = N_i\mathbb{E}(\varepsilon_i^2) = N_i\frac{\sigma^2}{N_i} = \sigma^2
\]
as desired.

Thus using the weighting of $\sqrt{N_i}$ the sample regression function becomes:
\[
Y_i\sqrt{N_i} = \alpha\sqrt{N_i} + \beta \sqrt{N_i}X_i + \varepsilon_i\sqrt{N_i} \\
Y_i^* = \alpha^* + \beta^* X_i + \varepsilon^*
\]
In general, to obtain the estimators for the coefficients, the weighted least-squares method minimizes the weighted residual sum of squares:
\[
\sum w_i\hat{\varepsilon}_i^2 = \sum w_i(Y_i - \hat{\alpha}^* - \hat{\beta}^* X_i)^2
\]
where $\alpha^*$ and $\beta^*$ are the weighted least squares estimators. Differentiating these with respect to $\hat{\alpha}^*$ and $\hat{\beta}^*$ gives us:
\begin{align*}
\frac{\partial}{\partial\hat{\alpha}^*}\sum w_i\hat{\varepsilon}_i^2 &= 2 \sum w_i(Y_i - \hat{\alpha}^* - \hat{\beta}^* X_i)(-1) \\[6pt]
\frac{\partial}{\partial\hat{\beta}^*}\sum w_i\hat{\varepsilon}_i^2 &= 2 \sum w_i(Y_i - \hat{\alpha}^* - \hat{\beta}^* X_i)(-X_i)
\end{align*}
Setting these equal to $0$ gives us:
\begin{align*}
\sum w_iY_i &= \hat{\alpha}^*\sum w_i + \hat{\beta}^* \sum w_iX_i \\
\sum w_iX_iY_i &= \hat{\alpha}^*\sum w_iX_i + \hat{\beta}^*\sum w_iX_i^2
\end{align*}
Solving these simultaneously, we get:
\begin{align*}
\hat{\alpha}^* 
  &= \frac{\sum w_iY_i}{\sum w_i} - \hat{\beta}^* \frac{\sum w_iX_i}{\sum w_i} \\[6pt]
  &= \bar{Y}^* - \hat{\beta}^*\bar{X}^* \\[18pt]
\hat{\beta}^*
  &= \frac{(\sum w_i)(\sum w_iX_iY_i) - (\sum w_iX_i)(\sum w_iY_i)}{(\sum w_i)(\sum w_iX_i^2) - (\sum w_iX_i)^2}
\end{align*}
Notice that in this question $w_i = N_i$ and not $\sqrt{N_i}$.  
\end{description}










\bigskip\bigskip\bigskip
\hdashrule[0.5ex]{\textwidth}{1pt}{3mm}
\bigskip\bigskip\bigskip

## QUESTION 2
\textbf{Using the Heteroskedasticity worksheet in sup4.xls}

Load the data in R:
```{r, eval=FALSE}
property_df <- read_excel("../Data/sup4.xls")

# You can use any of the following to examine data frame (df): 
# `dim()`: for its dimensions, by row and column
# `str()`: for its structure
# `summary()`: for summary statistics on its columns
# `colnames()`: for the name of each column
# `head()`: for the first 6 rows of the data frame
# `tail()`: for the last 6 rows of the data frame
# `View()`: for a spreadsheet-like display of the entire data frame
```


\bigskip\bigskip


### (a) Estimate the following and comment on your results:
\begin{equation} \label{eq:SQ2a}
PRICE_t = \beta_0 + \beta_1LOTSIZE_t + \beta_2SQRFT_t + \beta_3BDRMS_t + u_t
\end{equation}

In R run the following:
```{r, eval=FALSE}
SQ2a_lm <- lm(PRICE ~ BDRMS + LOTSIZE + SQRFT, data = property_df)
print(summary(SQ2a_lm), digits=7)
```

and in STATA run the following:
```{stata}
/* load the data */
quietly cd ..
import excel using Data/sup4.xls, ///
  sheet("heteroscedasticity") firstrow

/* `firstrow` indicates that the first row contains the variable names */
/* `describe` command would give basic information about the data set */

/* run the regression */
regress PRICE LOTSIZE SQRFT BDRMS
```

We see that the $F$-stat is high at $57.46$ with its $p$ value being $0$. We also see that both $LOTSIZE$ and $SQRFT$ are significant with $t$-values $3.22$ and $9.28$ with near $0$, or $0$, $p$-values, respectively. On the other hand, $BDRMS$ look insignificant with $t$-value at $1.54$, though it may perhaps be due to multicollinearity.

To check for heteroskedasticity, usually the first thing to do is to plot the residuals against the estimated values of the independent variable as an amalgamation of all the dependent variables.

In R we do this with the following:
```{r eval=FALSE}
# the following will provide four important plots that are usually needed
# since there are four graphs, we want to display in 2x2 format first then plot
par(mfrow = c(2,2))
plot(SQ2a_lm)

# if it is only the residuals vs fitted that we are interested, then
plot(SQ2a_lm, which=1)
# or
plot(fitted(SQ2a_lm), resid(SQ2a_lm))
# we can also add a horizontal line at 0
abline(0,0)

# to make this look nicer, we can  also use `autoplot` command from `ggfortify` library
library(ggfortify)
autoplot(SQ2a_lm)
```

In STATA we instead use the following:
```{stata, eval=FALSE}
/* plot residuals against fitted values */
rvfplot, yline(0)
```

In either case we get the following plot:

```{r, echo=FALSE}
property_df <- read_excel("../Data/sup4.xls")
SQ2a_lm <- lm(PRICE ~ BDRMS + LOTSIZE + SQRFT, data = property_df)
plot(SQ2a_lm, which=1)
abline(0,0)
```

There seems to be a downward trend which can suggest heteroskedasticity but it is difficult to tell, as it could be due to outliers.



\bigskip\bigskip
***
\bigskip\bigskip

### (b) Calculate robust standard errors for the equation \ref{eq:SQ2a} specified on page \pageref{eq:SQ2a} and compare your results.

\begin{description}
\item[Answer:]
White (1980)\footnote{White, H (1980) "A Heteroscedasticity Consistent Covariance Matrix Estimator and a Direct Test of Heteroscedasticity", \textit{Econometrica}, 48:817-828. Though the possibility of such heterskedasticity-robust standard errors were previously discussed by Eicker (1967) and Huber (1967) and so sometimes these are also called \textit{White-Huber-Eicker standard errors.} See Eicker, F (1967) "Limit Theorems for Regressions with Unequal and Dependent Errors", \textit{Proceedings of the Fifth Berkeley Symposium on Mathematical Statistics nd Probability} 1:59-82, and Huber, P J (1967) "The Behavior of Maximum Likelihood Estimates under Nonstandard Conditions",\textit{Proceedings of the Fifth Berkeley Symposium on Mathematical Statistics nd Probability} 1:221-233.} has shown that asymptotically consistent estimates of variances and covariances of OLS estimators can be obtained even if there is heteroskedasticity present so that asymptotically valid statistical inferences can be made about the true paramter values. White's heteroskedasticity-corrected standard errors are also known as \textit{robust standard errors}. 

\begin{tcolorbox}[breakable, title=White's robust standard errors, skin=enhancedlast]

\textbf{How do we get heteroskedasticity-consistent variances and standard errors?}\footnote{Gujarati and Porter (2009), Appendix 11A.4; Wooldridge (2021), Section 8.2}

Consider the simple regression model:
\[
Y_i = \beta_0 + \beta_1 X_1 + u_i
\]
where $Var(u_i)=\sigma_i$; that is, it is heteroskedastic. In Question 1 part (a) we have shown that 
\begin{equation}\label{eq:WhiteVar1}
Var(\hat{\beta_1}) = \frac{\displaystyle\sum_{i=1}^n (X_i - \bar{X})^2\sigma_i^2}{\Big( \displaystyle\sum_{i=1}^n(X_i - \bar{X})^2 \Big)^2}
\end{equation}
Since $\sigma_i^2$ are not directly observable, White argues for using the squared residual of each $i$, $\hat{u}_i^2$, instead and estimating the variance of the estimator via:
\begin{equation}\label{eq:WhiteVar2}
Var(\hat{\beta}_1) = \frac{\displaystyle\sum_{i=1}^n (X_i - \bar{X})^2\hat{u}_i^2}{\Big( \displaystyle\sum_{i=1}^n(X_i - \bar{X})^2 \Big)^2}
\end{equation}
White has shown that when this equation \ref{eq:WhiteVar2} is multiplied by the sample size $n$, it converges in probability to $\frac{\mathbb{E}[(X_i - \mu_X)^2u_i^2]}{(\sigma_X^2)^2}$ which is the probability limit of equation \ref{eq:WhiteVar1} multiplied by $n$, and where $\mu_X$ is the expected value of $X$, and $\sigma_X^2$ is the population variance of $X$. Thus, the law of large numbers and the central limit theorem are key in establishing these convergences, which are necessary for justifying the use of standard errors to construct confidence intervals and $t$-statistics.

$\hookrightarrow$ One can first obtain the residuals from the usual OLS regression and then calculate the variance using equation \ref{eq:WhiteVar2}. Statistical software do this automatically.

This can be extended to $k$-variable regression model
\[
Y_i = \beta_0 + \beta_1 X_{1i} + \beta_2 X_{2i} + \dots + \beta_kX_{ki} + u_i
\]
The variance of any partial regression coefficient, say $\hat{\beta}_j$ is then obtained via
\begin{equation}\label{eq:WhiteVar3}
Var(\hat{\beta}_j) 
  = \frac{\displaystyle\sum_{i=1}^n \hat{v}_{ji}^2 \hat{u}_i^2}{\Big( \displaystyle\sum_{i=1}^n \hat{v}_{ji}^2 \Big)^2}
  = \frac{\displaystyle\sum_{i=1}^n \hat{v}_{ji}^2 \hat{u}_i^2}{RSS_j^2}
\end{equation}
where $\hat{v}_{ji}$ denotes the $i^{th}$ residual from regressing $X_j$ on all other independent variables, and $RSS_j$ is the residual sum of squares from this regression. 

The square root of this expression in equation \ref{eq:WhiteVar3} is called \textbf{heteroskedasticity-robust standard error} for $\hat{\beta}_j$.

Also note that, sometimes the equation \ref{eq:WhiteVar3} is adjusted for degrees of freedom by multiplying it with $frac{n}{n-(k+1)}$ before taking the square root. This is because if $\hat{u}_i$ were the same for all $i$ then we would get the usual OLS standard errors. Since all forms of this equation has asymptotic justification, and they are asymptotically equivalent, no one form is unanimously preferred over others. Usually, we use whatever form the software we work with uses. 
\end{tcolorbox}

\end{description}

We can now calculate this in R as follows:

```{r eval=FALSE}
#we need two additional libraries for this:
library(lmtest) #for `coeftest` function
library(sandwich) #for `vcovHC` function

coeftest(SQ2a_lm, vcov = vcovHC(SQ2a_lm, "HC1")) #the default in vcovHC is "HC3" but to get the exact result as STATA we use "HC1"
```

Similarly, we can calculate this in STATA as follows:
```{stata eval=FALSE}
/* run the regression with additional `robust` command */

regress PRICE LOTSIZE SQRFT BDRMS, robust
```

However, to present the "robust" and "nonrobust" results side by side in a table, we can use the following set of commands instead:

```{stata}
/* load the data */
  quietly cd ..
  import excel using Data/sup4.xls, ///
    sheet("heteroscedasticity") firstrow

/* run the regression with `robust` command */
  quietly regress PRICE LOTSIZE SQRFT BDRMS, robust

/* store the estimates under the heading "robust" */
  estimates store robust
  
/* run the regression for nonrobust */
  quietly regress PRICE LOTSIZE SQRFT BDRMS 
  
/* store the estimates under the heading "nonrobust" */
  estimates store nonrobust
  
/* create the table for robust and nonrobust estimates of beta, s.e., and t-values */
  estimates table robust nonrobust, b se t

```

Notice that all the $t$-values are lower for each variable and in the case of $LOTSIZE$ this reduction means it is no longer significant. This is at least suggestive that $LOTSIZE$ may be a major source of the heteroskedasticity.




\bigskip\bigskip
***
\bigskip\bigskip

### (c) Using the specification in equation \ref{eq:SQ2a} on page \pageref{eq:SQ2a}, conduct a Goldfeld-Quandt test for heteroskedasticity in the $LOTSIZE$ dimension (exclude the middle 24 observations).

\begin{description}
\item[Answer:]
The Goldfeld-Quandt\footnote{Goldfeld S, and Quandt R E (1972) \textit{Nonlinear Methods in Econometrics}, North Holland Publishing Company, Amsterdam.} test is applicable when we assume that the heteroskedastic variance, $\sigma_i^2$, is positively related to \textit{one} of the explanatory variables in the regression model. In this question we are assuming that the heteroskedastic variance is related to $LOTSIZE$.

\begin{tcolorbox}[breakable, title=Goldfeld-Quandt Test, skin=enhancedlast]

\textbf{What are the reasoning and mechanics of the test?}\footnote{Gujarati and Porter (2009), Section 11.5}

Consider the simple regression model:
\[
Y_i = \beta_0 + \beta_1 X_1 + u_i
\]
and suppose $\sigma_i^2$ is monotonically related to $X_i$. One plausible assumption of this is 
\begin{equation}\label{eq:GQTest1}
\sigma_i^2 = \sigma^2X_i^2.
\end{equation}
What this assumption says is that $\sigma_i^2$ is proportional to the square of the $X$ variable. If this assumption is appropriate, it would mean the larger $X_i$ values are, the larger $\sigma_i^2$ gets. If that turns out to be the case, heteroskedasticity is most likely to be present in the model. 

To test this, Goldfeld and Quandt provide the following steps:
\begin{itemize}
\item[Step 1:] Order or rank the observations according to the values of $X_i$ beginning with the lowest $X$ value;

\item[Step 2:] Omit $c$ central observations, where $c$ is specified a priori, and divide the remaining onservations into two groups, each of $\frac{n-c}{2}$ observations;

\item[Step 3:] Fit separate OLS regressions to these two groups of observations and obtain the respective residual sum of squares $RSS_1$ and $RSS_2$, where $RSS_1$ represents the $RSS$ from the regression corresponding to the smaller $X_i$ values, i.e. small variance group, and $RSS_2$ to the larger $X_i$ values, i.e. the large variance group.

These $RSS$ each have $\displaystyle\frac{n-c}{2}-(k+1)$ degrees of freedom where $k$ is the number of parameters to be estimated, excluding the intercept - hence $+1$.

\item[Step 4:] Compute the following ratio:
\[
\lambda = \frac{\frac{RSS_2}{df}}{\frac{RSS_1}{df}}
\]
\end{itemize}

The main argument of this test is that if the assumption of homoskedasticity and $u_i$ are normally distributed both hold true, then $\lambda$ of equation \ref{eq:GQTest1} follows the $F$-distribution with $\displaystyle\frac{n-c}{k}-(k+1)$ degrees of freedom in both the numerator and denominator.

As usual, if the computed $\lambda$ which is equal to $F$-statistic, is greater than the critical $F$ value at the chosen level of significance, we can reject the null hypothesis of homoskedasticity.

\textbf{Why we ommit $c$ central observations?}
These observations are omitted to accentuate the difference between the small variance group, $RSS_1$, and the large variance group $RSS_2$. However, the \textit{power} of the test depends on how $c$ is chosen. Recall that \textit{power of a test} is measured by the probability of rejecting the null hypothesis when it is false, and it is calculated by $1-prob(Type II error)$.

Goldfeld and Quandt suggest $c=8$ for models with two-explanatory variables if $n=30$ and double if $n=60$.
\end{tcolorbox}
\end{description}

In this question $c=24$ and we order $LOTSIZE$ from small to large. To run the Goldfeld-Quandt test in R we can use the `gqtest()` function from the `lmtest` library:

```{r include=FALSE}
library(lmtest)
```

```{r}
gqtest(SQ2a_lm, order.by = property_df$LOTSIZE, fraction = 24, alternative="two.sided")
qf(0.975, 28, 28, lower.tail = TRUE) #critical F-value
```

In STATA there are more steps involved. First we need to order the data and removed the middle 24 observations before running regression on each:

```{stata}
/* load the data */
  quietly cd ..
  import excel using Data/sup4.xls, ///
    sheet("heteroscedasticity") firstrow
 
/* Step 1: Order the data according to LOTSIZE values */  
  sort LOTSIZE
  
/* create an index on which we will impose our condition for splitting data */  
  gen index=_n

/* run the regressions on each splitted data */  
  reg PRICE BDRMS LOTSIZE SQRFT if index<33
  
  reg PRICE BDRMS LOTSIZE SQRFT if index>56
  
/* Derive F-stat by dividing RSS of each (since df cancel out) */
  display 1.3970e+11/8.5839e+10
  
/* compute critical F value */
  display invfprob(28,28,0.025)

```

Both R and STATA give the same result that the $F$-statistic of $1.6275654$ is smaller than the critical $F$-value of $2.1299243$ which means we cannot reject the null of homoskedasticity. Therefore, it seems as though there is no heteroskedasticity according to the Goldfeld-Quandt test. However, the form of heteroskedasticity may be more complicated.




\bigskip\bigskip
***
\bigskip\bigskip

### (d) Test for heteroskedasticity by first estimating an equation that regresses the squared residuals from equation \ref{eq:SQ2a} on page \pageref{eq:SQ2a} against all of the independent variables used to estimate equation \ref{eq:SQ2a}. (Calculate both F and LM versions of this test). Verify your results using the 'hettest' command in Stata. Compare these results with the results of the White Test in Stata.

\begin{description}
\item[Answer:]
Goldfeld-Quandt test depends not only on the number of observations we omit but also on identifying the correct $X$ variable that needs to be ordered. These limitations of this test can be avoided with \textit{Breusch-Pagan Test},\footnote{Breusch, T and Pagan A (1979) "A Simple Test for Heteroscedasticity and Random Coefficient Variation", \textit{Econometrica}, 47:1287-1294.} or BP test, which is also called \textit{Breusch-Pagan-Godfrey Test},\footnote{Godfrey L (1978) "Testing for Multiplicative Heteroscedasticity" \textit{Journal of Econometrics}, 8:227-236.} or BPG test.  

\begin{tcolorbox}[breakable, title=Breusch-Pagan / Breusch-Pagan-Godfrey Test, skin=enhancedlast]

\textbf{What are the reasoning and mechanics of the test?}\footnote{Gujarati and Porter (2009), Section 11.5; Wooldridge (2021), Section 8.3}


\end{tcolorbox}

\end{description}

