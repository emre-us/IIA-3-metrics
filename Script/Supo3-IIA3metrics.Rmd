---
title: "IIA-3 Econometrics: Supervision 3"
author: "Emre Usenmez"
date: "`r Sys.Date()`"
output: pdf_document
header-includes: 
  - \usepackage{amsmath, tcolorbox, dashrule}
  - \tcbuselibrary{listings,most}
---

<!-- This comment will not be displayed in the output. Below change to CSS style is to ensure the blocktexts are in the same form size as the rest of the text.-->

```{css style settings, echo = FALSE} 
blockquote {
    padding: 10px 20px;
    margin: 0 0 20px;
    font-size: 14px;
    border-left: 5px solid #eee;
}
```

\tiny{Very grateful to Dr Oleg Kitov and Dr Clive Lawson for the very informative stylized answers to previous iterations of the supervision questions.}
\normalsize

# SUPPLEMENTARY QUESTIONS

\bigskip\bigskip

## QUESTION 1

\textbf{In a study of the Cobb-Douglass production function, a researcher suspects that the parameters are subject to change over time. Data on output, $Y$, labor input, $X_1$, and capital stock, $X_2$, are available for years 1929 to 1967. $T$ represents the time trend. The results obtained are as follows (t-values in parantheses):}

\begin{alignat*}{6}
\text{Full Sample:  } \ \ \widehat{log Y} & {}={} && -3.02 {}+{} && 1.34 \ logX_1 {}+{} && 0.29 \ logX_2 {}+{} && 0.0052 \ T \\
 & && (-6.65) && (14.68) && (4.89) && (2.34) \\
 & && && R^2 = 0.99535 && &&\hat{\sigma} = 0.03274 \\
\text{1929-48:  } \ \ \widehat{log Y} & {}={} && -3.22 {}+{} && 1.36 \ logX_1 {}+{} && 0.32 \ logX_2 {}+{} && 0.0051 \ T \\
 & && (-4.63) && (4.95) && (1.36) && (1.40) \\
 & && && R^2 = 0.97853 && && \hat{\sigma} = 0.04449 \\
\text{1949-67:  } \ \ \widehat{log Y} & {}={} && -1.56 {}+{} && 1.02 \ logX_1 {}+{} && 0.33 \ logX_2 {}+{} && 0.0095 \ T \\
 & && (-2.21) && (7.58) && (2.33) && (1.85) \\
 & && && R^2 = 0.99565 && && \hat{\sigma} = 0.0135 \\
\end{alignat*}

#### a) Conduct a test of the hypothesis that the four regression coefficients are jointly the same in both sub-periods, against the alternative that they differ.
\begin{description}
\item[Answer:] 

This question is effectively testing if there is a structural break from the start of 1949 which may be caused by different intercept, different slope coefficient, or both. Suppose the coefficients of the full sample regression are $\beta$s, and coefficients of the 1929-48 regression are $\psi$s, and coefficients of the 1949-67 are $\gamma$s. Our hypothesis is therefore:
\[
\mathbb{H}_0: (\psi_0=\gamma_0) \cap (\psi_1=\gamma_1) \cap (\psi_2=\gamma_2) \cap (\psi_3=\gamma_3) 
\]
We can test this in two ways. First is to reparameterize the model and then run an $F$-test, and the second is to run a Chow test.

\underline{First Approach:}

We can create a new coefficient $\delta = \psi-\gamma$ whereby our model becomes: $\widehat{logY}=\delta_0 + \delta_1 \ log X_1 + \delta_2 \ log X_2 + \delta_3 T + \varepsilon$ for which the hypothesis becomes:
\[
\mathbb{H}_0: (\delta_0=0) \cap (\delta_1=0) \cap (\delta_2=0) \cap (\delta_3=0)
\]
We would then use the $F$-test for this join hypothesis.

\underline{Second Approach:}

An alternative appraoch is to use \textit{Chow Test}.\footnote{Chow, C Gregory (1960) \textit{Tests of Equality Between Sets of Coefficients in Two Linear Regressions}, Econometrica, 28(3) 591:605} This test assumes that:
\begin{itemize}
  \item The error terms in the subperiod regressions are normally distributed with the same, i.e. homoskedastic, variance $\sigma^2$. That is, $u_{1929-48t} \sim N(0,\sigma^2)$ and $u_{1949-67t} \sim N(0,\sigma^2)$.
  \item The two error terms $u_{1929-48t}$ and $u_{1949-67t}$ are independently distributed.
\end{itemize}

The Chow test is an $F$-ratio which means we will need the $RSS$ of both unrestricted and restricted models. Here, \textit{the full sample model is the restricted model} since that is the model we have by imposing the restrictions that all $\psi_j=\gamma_j$ for $j=0,\dots,3$. The $RSS$ of the unrestricted model, on the other hand - and this is a key insight - is the combination of the two sub-sample $RSS$s.

\begin{tcolorbox}[breakable, title=Why $RSS$ and not $R^2$ form of $F$-Test, skin=enhancedlast]
Note that the Chow test uses $RSS$ and that there is no simple $R^2$ form of the $F$-test if separate regressions have been estimated for each group. This is because the $TSS$s are not the same as $\bar{Y}$ is not the same in both samples.
\end{tcolorbox}

The steps to carry out a Chow test is as follows:
\begin{enumerate}
  \item Obtain the restricted model's residual sum of squares, $RSS_R$, by estimating the regression for the full sample model with $(n-k-1)$ degrees of freedom, where $n=n_1+n_2$ with $n_1$ being the sample size of the first sub-sample, and $n_2$ being he sample size of the second sub-sample, and where $k$ is the number of regressors in that model.
  \item Estimate the first sub-sample model to obtain its residual sum of squares, $RSS_1$ with $n_1-k-1$ degrees of freedom.
  \item Do the same for the second sub-sample model to obtain $RSS_2$ with $n_2-k-1$ degrees of freedom.
  \item Add the two $RSS$s to compute the unrestricted model's residual sum of squares: $RSS_{UR}=RSS_1+RSS_2$.
  \item Compute the $F$-ratio:
  \[
  F = \frac{\dfrac{RSS_R-RSS_{UR}}{k+1}}{\dfrac{RSS_{UR}}{n-2(k+1)}} = \frac{\dfrac{\text{improvement in fit}}{\text{extra degrees of freedom used up}}}{\dfrac{\text{residual sum of squares remaining}}{\text{degrees of freedom remaining}}}
  \]
  \subitem
  $\hookrightarrow$
    Because we are splitting the sample into two, we are estimating k regressors for each sample plus their intercepts, so we are using up $(k+1)$ degrees of freedom twice.
  \item Compare the $F$-ratio to the critical $F$ value with \big($(k+1),n-2(k+1)$\big) degrees of freedom and fail to reject the null hypothesis of \textit{parameter stability}, i.e. no structural change, if $F$-ratio does not exceed the critical value at the chosen significance level.
\end{enumerate}

Accordingly, we first need to calculate the $RSS$s. For that, recall that $RSS = \hat{\sigma}^2(n-k-1)$ where $n=39, n_1=20, n_2=19$ because the dates are inclusive. Therefore:

\begin{align*}
RSS_R 
  &= 0.03274^2\times 35 
  &= 0.03751677 \\
RSS_UR 
  &= RSS_1 + RSS_2 \\ 
  &= 0.04449^2\times 16 + 0.0135^2\times 15 \\
  &= 0.03440351
\end{align*}

With these we can now calculate our $F$-ratio:
\[
F = \frac{\dfrac{0.03751677-0.04304995}{4}}{\dfrac{RSS_{UR}}{n-2(k+1)}} = \frac{\dfrac{0.03751677 - 0.03440351}{4}}{\dfrac{0.03440351}{31}} = 0.7013157
\]

The F-statistic for $\alpha=0.05$ is $2.678667$ and for $\alpha=0.01$ is $3.992811$, and thus we fail to reject the null hypothesis of parameter stability at either of the $\psi$ values.
\end{description}

```{r}
qf(p=c(0.05, 0.01), df1=4, df2=31, lower.tail = FALSE)
```


\begin{tcolorbox}[breakable, title=Why $RSS$ equals $\hat{\sigma}^2(n-k-1)$?, skin=enhancedlast]
Consider how we estimate the error variance, $\sigma^2$. First notice that $\sigma^2=\mathbb{E}(u^2)$, so an unbiased estimator of $\sigma^2$ is $\frac{1}{n}\sum_{i=1}^nu_i^2$. However, since we do not observe the errors $u_i$ this is not a true estimator. What we have, though, is the estimates of the errors $u_i$ which are the OLS residuals $\hat{u}_i$. If we replace the errors with the OLS residuals then we have
\[
\sigma^2 = \frac{\sum_{i=1}^nu_i^2}{n} = \frac{RSS}{n}
\]
which is a true estimator because it gives a computable rule for any sample of data on $X$s and $Y$. However, this is biased because it does not account for the restrictions that must be satisfied by the OLS residuals. These restrictions are given by the two OLS first order conditions:
\[
\sum_{i=1}^n\hat{u}_i = 0, \ \ \ \ \ \ \sum_{i=1}^nX_i\hat{u}_i=0
\]
for a simple regression with one regressor. In a way, if we know $n-k-1$ residuals, we can always get the other remaining residuals by using the restrictions implied by the first order conditions. Therefore there are only $n-k-1$ degrees of freedom in the OLS residuals, as opposed to $n$ degrees of freedom in the errors.

The unbiased estimator of the error variance is therefore:
\[
\hat{\sigma}^2 = \frac{\sum_{i=1}^n\hat{u}_i^2}{n-k-1} = \frac{RSS}{n-k-1}
\]
where in simple regression with one regressor, $k=1$.
\end{tcolorbox}


\bigskip\bigskip
***
\bigskip\bigskip

### (b) It is believed that irrespective of whether the form of the relationship has changed over the two periods (i.e. whether the coefficients of the equation have changed), there has still been a structural break. Test this hypothesis and use this result to comment on the assumptions made in part (a). What limitations are there to these methods for testing for stability over the whole period.

\begin{description}
\item[Answer:] What this question is effectively asking is that whether the error variances of the two subperiod regressions are the same. Recall from part (a) one of the two key assumptions of the Chow test is that the variances of the errors are homoskedastic. Therefore the hypothesis is:
\begin{align*}
\mathbb{H}_0: \sigma_\psi = \sigma_\gamma \ \ \ \ \ \ \ \text{or } \ \ \ \ \mathbb{H}_0: \frac{\sigma_\psi}{\sigma_\gamma}=1 \\
\mathbb{H}_1: \sigma_\psi \neq \sigma_\gamma \ \ \ \ \ \ \ \text{or } \ \ \ \ \mathbb{H}_0: \frac{\sigma_\psi}{\sigma_\gamma}\neq 1
\end{align*}
If the variances of the two subpopulations are the same, i.e. $\sigma_\psi=\sigma_\gamma$, as assumed by the Chow test, then the ratios of the ratios of estimated error variance to population error variance has an $F$ distribution with $(n_1-k-1)$ and $(n_2-k-1)$ degrees of freedom in the numerator and denominator, respectively. That is,
\[
\frac{\dfrac{\hat{\sigma}_\psi^2}{\sigma_\psi^2}}{\dfrac{\hat{\sigma}_\gamma^2}{\sigma_\gamma^2}} \sim F_{(n_1-k-1),(n_2-k-1)}.
\]

Notice that if $\sigma_\psi=\sigma_\gamma$ then this ratio and thus the $F$-test becomes:
\[
F = \frac{\sigma_\psi^2}{\sigma_\gamma^2}
\]
where by convention the larger estimated variance is in the numerator.

Accordingly, our $F$-statistic is:
\[
F = \frac{0.04449^2}{0.0135^2} = 10.86069.
\]

Since this is a two-tailed test, but we are putting the higher variance in the numerator, then we can treat it as one-sided test with alternative hypothesis is greater than $1$, The critical values for $\alpha=0.05$ with $(16,15)$ degrees of freedom is $2.384875$, and for $\alpha=0.01$ it is $3.485246$.

Thus at both $\alpha$ levels we can reject the null hypothesis and conclude that the subperiod variances are not the same at $\alpha=0.01$. 
This means, the assumption of Chow test does not hold and we shouldn't use the Chow test, at least not in this form. There are modifications to Chow test that can be utilized but that is beyond this class.

Another point regarding the Chow test to bear in mind is that it is sensitive to the choice of the time at which we divide the subperiods. The $F$ values would be different if the cut-off point was 1947 or 1949.

Finally, the Chow test will tell us only if the two regressions are different but not whether the difference is due to the intercepts, the slopes, or both. We can use dummy variables for that, though.

\end{description}

```{r}
qf(c(0.05, 0.01), df1 = 16, df2 = 15, lower.tail = FALSE)
```


\bigskip\bigskip\bigskip
\hdashrule[0.5ex]{\textwidth}{1pt}{3mm}
\bigskip\bigskip\bigskip

## QUESTION 2

\textbf{The following demand for money function was estimated from 60 observations, for which $\sum(M-\bar{M})^2=45600$:}
\[
\hat{M}_t = 284+0.56Y_t - 0.43M_{t-1} \ \ \ \ \ \ \ R^2=0.841
\]
\textbf{When a further 8 observations became available, the equation was re-estimated. The pooled data had $\sum(M-\bar{M})^2 = 50100$, and the re-estimated equation $R^2=0.818$. Carry out a Chow test for predictive failure. What do you conclude from your results? Explain carefully the role of the dummy variable in this test.}

\begin{description}
\item[Answer:]
The approach to the Chow test is similar to Question 1 whereby the $RSS$ for restricted model is the pooled data. Hwoever, notice we are not running a two separate regressions on the subsamples. So we follow the first approach from Question 1(a) by taking the differences of the model:
\[
\mathbb{H}_0: (\delta_0=0) \cap (\delta_1=0) \cap (\delta_2=0) 
\]
where $\delta$s are the coefficients of model $D_t$ which is the difference between the initial model and the model after the new observations.

Accordingly, we first need to derive the respective $RSS$s using the identity $R^2=1-\frac{RSS}{TSS}$ or $RSS = (1-R^2)TSS$ where $TSS=\sum(M-\bar{M})^2$:
\begin{align*}
RSS_R &= (1-0.841)\times 45,600 = 7,250.4 \\
RSS_{UR} &= (1-0.818)\times 50,100 = 9,118.2
\end{align*}

We can then calculate the Chow test whereby
\[
F = \\frac{\dfrac{9118.2-7250.4}{8}}{\dfrac{7250.4}{57}} = 1.835495
\]
The $F$ critical values for $\alpha=0.05$ and for $\alpha=0.01$ with $(8,57)$ degrees of freedom are $2.105599$ and $2.840694$ respectively. Accordingly we cannot reject the null hypothesis and conclude that there is no predictive failure.  

\end{description}

```{r}
qf(c(0.05, 0.01), df1 = 8, df2 = 57, lower.tail = FALSE)
```




\bigskip\bigskip\bigskip
\hdashrule[0.5ex]{\textwidth}{1pt}{3mm}
\bigskip\bigskip\bigskip

## QUESTION 3

### (a) Use the dataset sup3.xls to import the following variables for the period 1955-1990:
\begin{description}
\item[W =] \textbf{Wages and Salaries (Â£ million), CSO code: CFAJ\_AU}
\item[P =] \textbf{Implied Deflator for Consumers Expenditure, CSO code: GIEF\_AU} 
\item[E =] \textbf{Employees in Employment, CSO code: BCAD\_AU}
\item[WF =] \textbf{Workforce, CSO code: DYDB\_AU}
\item[U =] \textbf{Unemployed, CSO code: BCAB\_AU}
\end{description}

Load the libraries:
```{r}
libraries <- c("haven",       # to import/export SPSS, STATA, SAS files
               "readxl",      # to import/export Excel files   
               "tidyverse",   # for tidy data
               "Statamarkdown", # for using STATA commands in R
               "kableExtra", # for creating nice tables in R
               "rstatix")     # converts stats functions to a tidyverse-friendly format

# lapply(libraries, library, character.only=TRUE) will load the libraries
```

```{r include = FALSE}
invisible(lapply(libraries, library, character.only=TRUE))
```


Load the data:
```{r}
salaries_df <- read_excel("../Data/sup3.xls", sheet = 1)
```
Briefly examine the data frame
```{r}
# Yuo can use any of the following to examine data frame (df): 
# `dim()`: for its dimensions, by row and column
# `str()`: for its structure
# `summary()`: for summary statistics on its columns
# `colnames()`: for the name of each column
# `head()`: for the first 6 rows of the data frame
# `tail()`: for the last 6 rows of the data frame
# `View()`: for a spreadsheet-like display of the entire data frame

summary(salaries_df)
```
We can change the column names to something more memorable:
```{r} 
colnames(salaries_df) <- c("Year", "W", "P", "E", "WF", "U")
```


\bigskip\bigskip
***
\bigskip\bigskip

### (b) Use the data set to estimate the following wage equation by the OLS method:
\[
\Delta ln W_t = \beta_0 +\beta_1 ln P + \beta_2 ln P_{t-1} + \beta_3 ln E_t + \beta_4 ln E_{t-1} + \beta_5 ln UR_t + \beta_6 ln UR_{t-1} +\varepsilon_t 
\]
\textbf{where $\beta_i$ are constants, $\Delta$ denotes the first difference operator and $UR_t=\frac{U_i}{WF_i}$}.

\textbf{Verify that your estimate of $\beta_1$ is $.91170$. Also show that the above is a generalized version of the following hypothesis:}
\[
\Delta ln\Big(\frac{W}{EP}\Big)_t = \beta_0 + \varepsilon.
\]
\textbf{How would you explain the inclusion of the $UR$ terms?}

\begin{description}
\item[Answer:]
\end{description}

```{r}
#Transform the data to get it ready for the regression
salaries_df<- salaries_df %>% 
  mutate(lnW = log(W),
         DlnW = lnW - lag(lnW,1),
         lnP = log(P),
         lag_lnP = lag(lnP,1),
         lnE = log(E),
         lag_lnE = lag(lnE,1),
         lnUR = log(U/WF),
         lag_lnUR = lag(lnUR,1))

#Let's put these in a new dataframe to keep them neat:
salaries_new_df <- data.frame(Year = salaries_df$Year,
         lnW = salaries_df$lnW,
         DlnW = salaries_df$DlnW,
         lnP = salaries_df$lnP,
         lag_lnP = salaries_df$lag_lnP,
         lnE = salaries_df$lnE,
         lag_lnE= salaries_df$lag_lnE,
         lnUR = salaries_df$lnUR,
         lag_lnUR = salaries_df$lag_lnUR,
         stringsAsFactors = FALSE)
```

Since we are taking the differences those columns should have any value for 1955:
```{r}
#Look at total number of `NA` values per column:
colSums(is.na(salaries_new_df))

```

Thus we can adjust our sample size to take account of this:
```{r}
n_unr <- nrow(salaries_new_df) - sum(apply(is.na(salaries_df), 1, any)) 
# Note, if we replace 1 with 2 within apply function it will give us 4, the total number of NAs. 
n_unr
```

Thus our sample size is $35$.

```{r}
# Run the regression
lm_Q3b <- lm(DlnW ~ lnP + lag_lnP + lnE + lag_lnE + lnUR + lag_lnUR, data = salaries_new_df)
summary(lm_Q3b)
```

We can therefore see that only lnP and lag_lnP coefficients are significant. However, $F$-stat is pretty high which suggests presence of multicollinearity. We can look at the variance inflation factor, VIF:

```{r}
car::vif(lm_Q3b)
```

where we see each of the $VIF$ value is greater than $5$ suggesting serious collinearity.


\bigskip\bigskip
***
\bigskip\bigskip

### (c) Append the variable $IP$ (strength of Income Policy Index) in Table 1 to your data set (use the `generate` command in Stata to generate a new variable, or copy and paste the data from the question sheet) and run the above regression including $IP$ as an additional regressor and give both a statistical and an economic interpretation of your results.

\begin{description}
\item[Answer:]
Once we append $IP$, the model we will be testing is:
\[
\Delta ln W_t = \beta_0 +\beta_1 ln P + \beta_2 ln P_{t-1} + \beta_3 ln E_t + \beta_4 ln E_{t-1} + \beta_5 ln UR_t + \beta_6 ln UR_{t-1} + \beta_7 IP_t + \varepsilon_t
\]
\end{description}

```{r}
#Append IP with data from the question

salaries_new_df <- salaries_new_df %>% 
  mutate(IP = 0)
salaries_new_df$IP[salaries_new_df$Year == 1962] <- 1.0
salaries_new_df$IP[salaries_new_df$Year == 1965] <- 1.0
salaries_new_df$IP[salaries_new_df$Year == 1966] <- 1.5
salaries_new_df$IP[salaries_new_df$Year == 1967] <- 1.0
salaries_new_df$IP[salaries_new_df$Year == 1968] <- 1.5
salaries_new_df$IP[salaries_new_df$Year == 1969] <- 1.75
salaries_new_df$IP[salaries_new_df$Year == 1975] <- 1.0
salaries_new_df$IP[salaries_new_df$Year == 1976] <- 3.0
salaries_new_df$IP[salaries_new_df$Year == 1977] <- 4.5
salaries_new_df$IP[salaries_new_df$Year == 1978] <- 1.0
salaries_new_df$IP[salaries_new_df$Year == 1979] <- 1.0

# Now the number of regressors increased to 7:
k_3c <- 7

#Run the regression
lm_Q3c <- lm(DlnW ~ lnP + lag_lnP + lnE + lag_lnE + lnUR + lag_lnUR + IP, data = salaries_new_df)
summary(lm_Q3c)
```

With the introduction of IP, we now see that all are statistically significant except for $UR$ and its lag, as well as the intercept. Thus we can see that by adding a required variable, $IP$, we reduce the standard error for the whole equation, which in turn, increases the $t$-values of the other coefficients to rise; except for $UR$ which, as we will see, is because $UR$ has no effect.

$R^2$ is not helpful here since it will always increase when a new variable is added. Adjusted $R^2$ also increases from $0.7965$ to $0.8728$. However, a more useful indicated here is the $F$-statistic which tests that all slope coefficients are zero. If this is higher, then the equation is an improvement. With the introduction of IP, we see an increase from $23.18$ to $34.31$.

We can also check the significance of $IP$ by looking at its $t$-value which is $-4.216$. Since the $p$-value is $0.00025$ we know it is statistically significant and we don't need to calculate the $t$-statistic. As long as $p$-value is less than $0.05$ it is significant at the $5%$ level.


\bigskip\bigskip
***
\bigskip\bigskip

### (d) Test the following set of restrictions jointly by re-specifying your preferred equation:
\[
\beta_1+\beta_2 = 0; \ \ \beta_3+\beta_4=0; \ \ \beta_5+\beta_6 = 0
\]
\textbf{Verify your results using the Wald Test (the} `test` \textbf{command in Stata). Interpret your results.}

\begin{description}
\item[Answer:]
\end{description}
We can do this in two different ways. First is to use the `test` command in Stata, or `linearHypothesis` command from the `car` package in R for the Wald Test. The second way is to impose the constraint on the equation and then compare the constrained and unconstrained equations, testing the restriction on the constrained equation.

```{r}
# Approach 1:

car::linearHypothesis(lm_Q3c, c("lnP=-lag_lnP", "lnE=-lag_lnE", "lnUR=-lag_lnUR"))
```
which gives us the $F$-statistic as $0.7598$.

In the second approach, the restricted model we will test is:
\[
\Delta ln W_t = \beta_0 +\beta_1 \Delta ln P_t + \beta_3 \Delta ln E_t + \beta_5 \Delta ln UR_t + \beta_7 IP_t + \Delta \varepsilon_t
\]


\begin{tcolorbox}[breakable, title=Why restricted model is a \textit{dynamic} regression model?, skin=enhancedlast]
Suppose we have the following model:
\[
Y_t = \beta_0 + \beta_1X_t + u_t
\]
This model is known as \textit{level form}. Since this level form holds true for every time period, this can also be written as their lagged values:
\[
Y_{t-1} = \beta_0 + \beta_1X_{t-1} + u_{t-1}
\]
If we subtract the latter from the former, we will get what is called \textit{first difference form}:
\[
\Delta Y_t = \beta_1\Delta X_t + \Delta u_t
\]
where the \textit{first difference operator}, $\Delta$, tells us to take successive differences of the variables in the equation. 

So, for example, if $X$ and $Y$ in the level form represent the logarithms of income and consumption expenditure, then $\Delta X$ and $\Delta Y$ represent changes in the logs of income and consumption expenditure, respectively. So, if we are interested in the relationships between the variables in their growth form, then the difference form may be more appropriate. This is because a change in the log of a variable is a relative change, or a percentage change if multiplied by 100.

If the error term in the level form satisfies the OLS assumptions, especially the assumption of no autocorrelation, then the error term $v_t = \Delta u_t = u_t - u_{t-1}$ is autocorrelated. To see this we need to take the expectation and variance of $v_t$:
\[
\mathbb{E}(v_t) = \mathbb{E}(u_t - u_{t-1}) = \mathbb{E}(u) - \mathbb{E}(u_{t-1}) = 0
\]
since $\mathbb{E}(u)=0$ for each t.
\begin{align*}
Var(v_t) 
  &= Var(u_t - u_{t-1}) \\
  &= Var(u_t) + Var (-u_{t-1}) \\
  &= \sigma^2 + \sigma^2 \\
  &= 2\sigma^2 
\end{align*}
thus $v_t$ is homoskedastic. However we also need to look at the covariance of $v_t$ and $v_{t-1}$:
\begin{align*}
Cov(v_t, v_{t-1})
  &= \mathbb{E}(v_t v_{t-1}) - mathbb{E}(v_t)mathbb{E}(v_{t-1}) \\
  &= \mathbb{E}\big( (u_t-u_{t-1})(u_{t-1}-u_{t-2}) \big) - 0 \\
  &= \mathbb{E}( u_t u_{t-1} - u_t u_{t-2} - u_{t-1}^2 + u_{t-1}u_{t-2}) \\
  &= \mathbb{E}(-u_{t-1}^2) \\
  &= -\sigma^2
\end{align*}
which is nonzero. Therefore, although the $u$'s are not autocorrelated the $v$'s are.

Now to see why the restrictions in the question are expressed in the first difference form, consider a general distributed-lag model with a finite lag of $k$ time periods:
\[
Y_t = \beta_0 + \beta_1 X_1 + \beta_2 X_{t-1} + \dots + \beta_{k+1} X_{t-k} + u_t
\]
The coefficient $\beta_1$ is known as the \textit{short-run multiplier}, or \textit{impact multiplier}, because it gives the change in the mean value of $Y$ following a unit change in $X$ in the same time period. If the change in $X$ is maintained at the same level thereafter, then $(\beta_1+\beta_2)$ gives the change in the mean value of $Y$ in the next period, $(\beta_1+\beta_2 + \beta_3)$ in the following period, so on. These partial sums are called \textit{interim multipliers} or \textit{intermediate multipliers}. After $k$ time periods we obtain:
\[
\sum_{i=1}^k+1 \beta_i = \beta_1 + \beta_1 + \dots + \beta_k+1 = \beta
\]
which is known as the \textit{long-run distributed-lag multiplier} or \textit{total distributed-lag multiplier}.

In this question, what we are testing is if that total distributed-lag multiplier is different from $0$ for first differences form.
\end{tcolorbox}

```{r}
#Approach 2

#Modify the data frame to add the constrained versions:
salaries_new_df <- salaries_new_df %>% 
  mutate(DlnP = lnP - lag_lnP,
         DlnE = lnE - lag_lnE,
         DlnUR = lnUR - lag_lnUR)

#Estimate the constrained model:
lm_Q3d <- lm(DlnW ~ DlnP + DlnE + DlnUR + IP, data = salaries_new_df)
summary(lm_Q3d)
```

We will then need to calculate the $F$-statistic:

\[
F = \frac{\dfrac{RSS_R - RSS_{UR}}{df_1}}{\dfrac{RSS_{UR}}{df_2}}
\]

which means we need to obtain the $RSS$s from both regressions which can be done either by `deviance` function in R or by calculating the $RSS$ manually:

```{r}
#RSS unrestricted
# Manual calculation: sum(resid(lm_Q3c)^2)
RSS_unr_Q3d <- deviance(lm_Q3c)

#RSS restricted
#Manual calculation: sum(resid(lm_Q3d)^2)
RSS_res_Q3d <- deviance(lm_Q3d)

# Difference between the two models (ie number of restrictions):
q_3d <- 3

#F-statistic with (3,27) df:
Fstat_Q3d <- ((RSS_res_Q3d - RSS_unr_Q3d)/q_3d)/(RSS_unr_Q3d/(n_unr-k_3c-1))

#F Critical value:
Crit_Value_Q3d <- qf(0.05, df1=q_3d, df2=n_unr-k_3c-1,lower.tail = FALSE)

tbl_Q3d <- as.table(c("RSS_unr"=RSS_unr_Q3d, 
           "RSS_res"=RSS_res_Q3d, 
           "F-Stat"=Fstat_Q3d, 
           "Critical_Value"=Crit_Value_Q3d, 
           "df_1"=q_3d, 
           "df_2"=n_unr-k_3c-1))
tbl_Q3d %>%
   kbl(caption = "F-Test for Supplementary Question 3(d)") %>%
  kable_classic(full_width=FALSE)
```

We again obtain the same $F$-statistic of $0.7598$ as in Approach 1, which is much lower than the critical value of $2.96$, thus we cannot reject the null hypothesis. That is, the constraints seem to be correct.


\bigskip\bigskip
***
\bigskip\bigskip

### (e) Test the further restrictions that $\beta_5 = \beta_6 = 0$. (N.B. your unrestricted equation is always your currently preferred equation, so in this case you only need to test $\beta_5=0$ explicitly.)

\textbf{Answer:}
We can respecify the constraint and run the regression on this model. Our constrained is $\beta_5=0$. This is because in part (d) we have $\beta_5+\beta_6 = 0$ thus if $\beta_5=0$ then $\beta_6=0$. a t-test or check the t-statistic of the regression summary:

```{r}
summary(lm_Q3d)$coefficients["DlnUR", "t value"]
```

We see that the $t$-stat for $DlnUR$ is $-0.4770329$. 

But the question asks this to be done by respecifying the model. Also note that this time our unrestricted model is the restricted model in (d) since we are imposing further restrictions. Accordingly the model we test is:

\[
\Delta ln W_t = \beta_0 +\beta_1 \Delta ln P_t + \beta_3 \Delta ln E_t + \beta_7 IP_t + \Delta \varepsilon_t
\]
```{r}
#Estimate the constrained model:
lm_Q3e <- lm(DlnW ~ DlnP + DlnE + IP, data = salaries_new_df)
summary(lm_Q3e)
```

As we did in part (d) we can obtain the $F$-statistic:
```{r}
# New RSS unrestricted:
RSS_unr_Q3e <- RSS_res_Q3d

# Number of regressors in the new unrestricted model:
k_3e <- 4

#RSS restricted
#Manual calculation: sum(resid(lm_Q3e)^2)
RSS_res_Q3e <- deviance(lm_Q3e)

# Difference between the two models (ie number of restrictions):
q_3e <- 1

#F-statistic with (1,30) df:
Fstat_Q3e <- ((RSS_res_Q3e - RSS_unr_Q3e)/q_3e)/(RSS_unr_Q3e/(n_unr-k_3e-1))

#F Critical value:
Crit_Value_Q3e <- qf(0.05, df1=q_3e, df2=n_unr-k_3e-1,lower.tail = FALSE)

tbl_Q3e <- as.table(c("RSS_unr"=RSS_unr_Q3e, 
           "RSS_res"=RSS_res_Q3e, 
           "F-Stat"=Fstat_Q3e, 
           "Critical_Value"=Crit_Value_Q3e, 
           "df_1"=q_3e, 
           "df_2"=n_unr-k_3e-1))
tbl_Q3e %>%  
  kbl(caption = "F-Test for Supplementary Question 3(e)") %>%
  kable_classic(full_width=FALSE)
```

If we take the square of the t-statistic from the summary table, we should also get this F-statistic:
```{r}
summary(lm_Q3d)$coefficients["DlnUR", "t value"]^2
```

which gives us the F-Stat as expected.



\bigskip\bigskip
***
\bigskip\bigskip

### (f) Interpret your prefereed equation in light of your results to (c), (d), and (e).

\begin{description}
\item[Answer:]
The equation in (e), $\Delta ln W_t = \beta_0 +\beta_1 \Delta ln P_t + \beta_3 \Delta ln E_t + \beta_7 IP_t + \varepsilon_t$, should be our new preferred equation since on all measures it is the best equation so far. This is good for the first hypothesis, i.e. the real wage resistance hypothesis, as this is exactly what we would expect if the hypothesis is correct.
\end{description}


\bigskip\bigskip
***
\bigskip\bigskip

### (g) Test the hypothesis that $\beta_3=1$ by respecifying your preferred equation in terms of average wages and salaries. (Your F-statistic for this test should be about 0.086)

\begin{description}
\item[Answer:]
One way to answer this question is to run a $t$-test on the coefficient of $\Delta ln E$:
\end{description}

```{r}
(summary(lm_Q3e)$coefficients["DlnE","Estimate"]-1)/summary(lm_Q3e)$coefficients["DlnE","Std. Error"]
```

We check against the critical $t$ value
```{r}
qt(p=0.025, df=34, lower.tail = FALSE)
```

Thus we cannot reject the null hypothesis.

However, the question actually ask us to respecify the equation. For this, we set the coefficient on $\Delta ln E$ to 1, i.e. $\beta_3=1$ and then take it over to the other side of the equation so that the model becomes:
\[
\Delta ln W_t - \Delta ln E_t = \beta_0 +\beta_1 \Delta ln P_t + \beta_7 IP_t + \Delta \varepsilon_t
\]
or
\[
\Delta ln\Big(\frac{W_t}{E_t}\Big) = \beta_0 +\beta_1 \Delta ln P_t + \beta_7 IP_t + \Delta \varepsilon_t
\]
We can run the regression on this:

```{r}
# First modify the data frame to reflect the new dependent variable:
salaries_new_df <- salaries_new_df %>% 
  mutate("DlnWE" = DlnW-DlnE)

# Then run the regression:
lm_Q3g <- lm(DlnWE ~ DlnP + IP, salaries_new_df)
summary(lm_Q3g)
```

We can now derive the $F$-statistic as before:
```{r}
# New RSS unrestricted:
RSS_unr_Q3g <- RSS_res_Q3e

# Number of regressors in the new unrestricted model:
k_3g <- 3

#RSS restricted
#Manual calculation: sum(resid(lm_Q3g)^2)
RSS_res_Q3g <- deviance(lm_Q3g)

# Difference between the two models (ie number of restrictions):
q_3g <- 1

#F-statistic with (1,30) df:
Fstat_Q3g <- (RSS_res_Q3g - RSS_unr_Q3g/q_3g)/(RSS_unr_Q3g/(n_unr-k_3g-1))

#F Critical value:
Crit_Value_Q3g <- qf(0.05, df1=q_3g, df2=n_unr-k_3g-1,lower.tail = FALSE)

tbl_Q3g <- as.table(c("RSS_unr"=RSS_unr_Q3g, 
           "RSS_res"=RSS_res_Q3g, 
           "F-Stat"=Fstat_Q3g, 
           "Critical_Value"=Crit_Value_Q3g, 
           "df_1"=q_3g, 
           "df_2"=n_unr-k_3g-1))
tbl_Q3g %>%
   kbl(caption = "F-Test for Supplementary Question 3(g)") %>%
  kable_classic(full_width=FALSE)
```

we can check that this is the square of the $t$-stat:

```{r}
sqrt(Fstat_Q3g)
```



\bigskip\bigskip
***
\bigskip\bigskip

### (h) In the light of your preferred specification for the total period does it appear that price inflation is fully passed on in wage claims?

\begin{description}
\item[Answer:]
This question is basically asking if the coefficient of $\Delta ln P$ is equal to $1$ or not. We can use the $t$-test for this:
\end{description}

```{r}
(summary(lm_Q3g)$coefficients["DlnP","Estimate"]-1)/summary(lm_Q3g)$coefficients["DlnP","Std. Error"]
```

We check against the critical $t$ value
```{r}
qt(p=0.025, df=34, lower.tail = FALSE)
```

Thus we cannot reject the null hypothesis.


\bigskip\bigskip
***
\bigskip\bigskip

### (i) Interpret and comment upon your findings (especially in relation to the hypothesis suggested in (b)) and explain why your preferred equation is now given by:
\[
\Delta ln\Big(\frac{W_t}{E_t}\Big) = \beta_0 + \beta_1 \Delta ln P_t + \beta_7 IP_t + \Delta \varepsilon
\]

\begin{description}
\item[Answer:]
We have tested back as far as we can go and at each stage the real wage resistance hypothesis gives us the best equation. It is not clear what an hypothesis would have to do in order to be better.
\end{description}




\bigskip\bigskip
***
\bigskip\bigskip

### (j) Why might it be expected that a structural break took place around 1979/1980? Test the hypothesis that the structure of wage determination changed significantly from 1980 onwards using the method of dummy variables (NB this is a different test to those you used in Q1 and Q2). Discuss your results.

\begin{description}
\item[Answer:] The short answer is Thatcher. End to corporatism and wage bargaining etc.

To test this, rather than using the Chow Test, use the dummy variables method. For this, we define a variable, $M$ which is $0$ for the period 1956-1979, and $1$ for 1980 onwards. We then estimate the following:
\[
\Delta ln W_t = \beta_0 + \beta_1 \Delta ln P_t + \beta_8 M_t + \beta_9 M\Delta ln P_t + \Delta \varepsilon
\]

\end{description}

```{r}
# First, add M column to the data frame:

salaries_new_df <- salaries_new_df %>%
  mutate(
    M = case_when(
      Year < 1980 ~ 0,
      Year >= 1980 ~ 1
    ),
    MDlnP = M * DlnP
  )

# Then run the regression:

lm_Q3j <- lm(DlnW ~ DlnP + M + MDlnP, salaries_new_df)
summary(lm_Q3j)
```

Notice that there is not a $MlnP$ term because if we include it we would have perfect multicollinearity. From the regression output we can see that neither $M$ nor $MDlnP$ are significant - see their t-stats, so we can conclude that there is not any evidence of structural break for these years. 






\bigskip\bigskip
***
\bigskip\bigskip

### (k) Discuss the advantages of using the method of dummy variables over the Chow Test you used in part 1(a).

\begin{description}
\item[Answer:]
With the dummy variable we find out which variable or variables have changed. Whereas Chow test tells us only that something has changed. That is, it will tell us if the two sub-regressions are different but not whether the difference is due to the intercepts, the slope, or both.
\end{description}








































\pagebreak

# FACULTY QUESTIONS

\bigskip\bigskip

## QUESTION 1

\textbf{An investigator analysing the relationship between food expenditure, disposable income and prices across a random sample of 25 counties in the UK estimates the relationship}
\begin{alignat*}{4}
log(FOOD) = & 4.7377 {}+{} && 0.3506 \ log(PDI) {}-{} && 0.5086 \ log(PRICE) \\
& (0.6805) && (0.0899) && (0.1010)
\end{alignat*}

\textbf{where figures in paranthesis are standard errors and where:}

\textbf{FOOD = Average household expenditure on food}

\textbf{PDI = Average personal dosposable income}

\textbf{PRICE = Average price of food deflated by a general price index}

#### (i) Give an economic interpretation of the coefficients on log(PDI) and log(PRICE).

\begin{description}
\item[Answer:] The coefficient of $log(PDI)$ is income elasticity, and the coefficient of $log(PRICE)$ is the price elasticity.
\end{description}


\bigskip\bigskip
***
\bigskip\bigskip

### (ii) Test the hypothesis, using 5% significance level, that the coefficient of log(PRICE) is equal to zero against the alternative that it is nonzero.
\begin{description}
\item[Answer:] For this we would use a $t$-test for the hypothesis that
\[
\mathbb{H}_0: \beta_2=0 \text{vs } \mathbb{H}_{A}: \beta_2 \neq 0
\]
The $t$-statistic is:
\[
\frac{-0.5086 - 0}{0.1010} = -5.035644
\]
which is significantly different than $t_{0.025,24}=-2.064$, and so we would reject the null hypothesis. 
\end{description}



\bigskip\bigskip
***
\bigskip\bigskip

### (iii) Test the hypothesis using 5% significance level that the coefficient of log(INCOME) is equal to 1 against the alternative that it is significantly different from 1.

\begin{description}
\item[Answer:] For this we would use a $t$-test for the hypothesis that
\[
\mathbb{H}_0: \beta_1=1 \text{vs } \mathbb{H}_{A}: \beta_1 \neq 1
\]
The $t$-statistic is:
\[
\frac{0.3506 - 1}{0.0899} = -7.223582
\]
which is significantly different than $t_{0.025,24}=-2.064$, and so we would reject the null hypothesis. 
\end{description}






\bigskip\bigskip
***
\bigskip\bigskip

### (iv) You are now given the following extra information:
\begin{align*}
TSS &= \sum_{i=1}^n (y_i-\bar{y})^2 = 0.52876 \\
RSS &= \sum_{i=1}^n e_i^2 = 0.46276
\end{align*}
\textbf{Compute ESS and $R^2$ for the above regression}.

\begin{description}
\item[Answer:]
\begin{align*}
ESS &= TSS - RSS = 0.52876 - 0.46276 = 0.066 \\
R^2 &= \frac{ESS}{TSS} = \frac{0.066}{0.52876} = 0.1248203.
\end{align*}
\end{description}




\bigskip\bigskip
***
\bigskip\bigskip

### (v) Test the joint hypothesis at the 5% level that the two slope coefficients are all equal to zero against the alternative that at least one slope coefficient is nonzero.

\begin{description}
\item[Answer:] For this we would use an $F$-test for the hypothesis that
\[
\mathbb{H}_0: (\beta_1=0) \cap (\beta_2=0)
\]
The $F$-statistic is:
\[
F = \frac{\dfrac{ESS}{df_1}}{\dfrac{RSS}{df_2}} = \frac{\dfrac{0.066}{2}}{\dfrac{0.46279}{22}} = 1.568746
\]
or
\[
F = \frac{\dfrac{R^2_unr - R^2_res}{df_1}}{\dfrac{1-R^2_unr}{df_2}} = frac{\dfrac{0.1248203}{2}}{\dfrac{1-0.1248203}{22}} = 1.568847
\]
giving a marginal difference due to rounding errors. In either case the F-statistic is significantly different than $F_{0.05,2, 22}=0.0514$, and so we would reject the null hypothesis. 
\end{description}







\bigskip\bigskip\bigskip
\hdashrule[0.5ex]{\textwidth}{1pt}{3mm}
\bigskip\bigskip\bigskip

## QUESTION 2

\textbf{Download the dataset} `wage2.dta` \textbf{Use the STATA commands} `des` \textbf{and} `sum` \textbf{to understand the structure of the data and the meaning of the variable labels. Now answer the following questions relating to performance on the IQ test for this sample of working age women. The IQ test was taken as an adult after the woman had completed her formal education.}


Load the data in R, though the equivalent commands produce output that is not as neat as Stata here are the commands for obtaining summary and structure of the data frame. I keep them in the "comment" format to save paper when printing:
```{r}
wage2_df <- read_dta("../Data/wage2.dta")
# summary(wage2_df)
# str(wage2_df)

# We should also check for any NA data entries per column
colSums(is.na(wage2_df))
```





### (a) Run a regression of IQ test score on parents' education. What do you conclude?

\begin{description}
\item[Answer:]
We need to estimate the following regression:
\[
IQ = \beta_0 + \beta_1 feduc_i + \beta_2 meduc_i + u_i, \ \ \ i = 1,\dots,n.
\]
\end{description}
The regression would give us:
```{r}
lm_FQ2a <- lm(IQ ~ feduc + meduc, data = wage2_df)
summary(lm_FQ1a)
```

We can see that the coefficients for both parents' education are positive, individually close to unity, and statistically significant given the high $t$-values and very low $p$-values. We can also see that the $F$-statistic of $61.72$ is quite high which means the regression coefficients are jointly significant. On the other hand, we can see that the goodness-of-fit is relatively low at $R^2=0.147$.   



\bigskip\bigskip
***
\bigskip\bigskip

### (b) Suppose I want to know whether the only way parents' education increases their daughter's test score is through the daughter's own education. How would you test that hypothesis?

\begin{description}
\item[Answer:]
This question is effectively asking us to test whether parents' education level has no effect on daughter's IQ beyond its influence on the child's education level. Therefore, we need to estimate a regression on $IQ$ on the daughter's own education level, in addition to father's and mother's education levels, and subsequently test the joint significance of the parents' education. Thus we have a restricted and unrestricted model:
\begin{align*}
\text{Unestricted Model:  }  IQ &= \beta_0 + \beta_1 feduc_i + \beta_2 meduc_i + \beta_3educ_i + u_i, \\
\text{Restricted Model:  } IQ &= \gamma_0 + \gamma_1educ_i + v_i.
\end{align*}

The joint hypothesis for the $F$-test is:
\begin{equation*}
\mathbb{H}_0: (\beta_1 = 0) \cap (\beta_2= 0) \\
\mathbb{H}_A: (\beta_1 \neq 0) \cup (\beta_2 \neq 0)
\end{equation*}

The $F$-test statistic has $F_{q,(n-k-1)}$ distribution and is obtained either via $RSS$ or $R^2$:
\[
F = \frac{\frac{RSS_{res} - RSS_{unr}}{q}}{\frac{RSS_{unr}}{(n-k-1)}}
  = \frac{\frac{(R_{unr}^2 - R_{res}^2)}{q}}{\frac{1-R_{unr}^2}{(n-k-1)}}
  \sim F_{q, (n-k-1)}
\]

We can calculate this in R as follows:
\end{description}

```{r}
# Before obtaining estimates we need to remove the rows with NAs:
 wage2_nona_df <- wage2_df %>%
  filter(!is.na(feduc), !is.na(meduc))

# Obtain the estimates
lm_FQ2b_unr <- lm(IQ ~ feduc + meduc + educ, data = wage2_df)
lm_FQ2b_res <- lm(IQ ~ educ, data = wage2_nona_df)

#RSS unrestricted
# Manual calculation: sum(resid(lm_FQ2b_unr)^2)
RSS_unr_FQ2b <- deviance(lm_FQ2b_unr)

#RSS restricted
#Manual calculation: sum(resid(lm_FQ2b_res)^2)
RSS_res_FQ2b <- deviance(lm_FQ2b_res)

# or to use R-squareds:
Rsq_unr <- summary(lm_FQ2b_unr)$r.squared
Rsq_res <- summary(lm_FQ2b_res)$r.squared

# Difference between the two models (ie number of restrictions):
q_FQ2b <- 2

# Sample size - since we have NAs in both columns but in different rows we need to remove them:
n_FQ2b <- nrow(wage2_nona_df)

# number of regressors in unrestricted model:
k_FQ2b <- 3

#F-statistic:
Fstat_FQ2b_withRSS <- ((RSS_res_FQ2b - RSS_unr_FQ2b)/q_FQ2b)/(RSS_unr_FQ2b/(n_FQ2b-k_FQ2b-1))
Fstat_FQ2b_withRsq <- ((Rsq_unr-Rsq_res)/q_FQ2b)/((1-Rsq_unr)/(n_FQ2b-k_FQ2b-1))

#F Critical value:
Crit_Value_FQ2b <- qf(0.95, df1=q_FQ2b, df2=n_FQ2b-k_FQ2b-1,lower.tail = TRUE)

tbl_FQ2b <- as.table(c("RSS_unr"=round(RSS_unr_FQ2b,4), 
           "RSS_res"=round(RSS_res_FQ2b,4), 
           "R-squared_unr" = round(Rsq_unr,4),
           "R-squared_res" = round(Rsq_res,4),
           "F-Stat (using RSS)"=round(Fstat_FQ2b_withRSS, 4),
           "F-Stat (using R-sq)"=round(Fstat_FQ2b_withRsq, 4),
           "Critical_Value"=round(Crit_Value_FQ2b, 4),
           "df_1"=round(q_FQ2b, 4),
           "df_2"=round(n_FQ2b-k_FQ2b-1,4)))

tbl_FQ2b %>%
  kbl(caption = "F-Test for Faculty Question 2(b)") %>%
  kable_classic(full_width=FALSE)
```

Which gives us an F-statistic of $13.18272$. Comparing this to the critical values at $95% = 3.008$ we can reject the null.


The same test can be done using `linearHypothesis()` function from the `car` package:
```{r}
# A quick way to do the F-test:
car::linearHypothesis(lm_FQ1b_unr, c("feduc=0","meduc=0"))
```

Which gives us an F-statistic of $13.183$. Comparing this to the critical values at $95% = 3.008$ and $99% = 4.635$ we can reject the null.
```{r}
qf(0.99,2,718, lower.tail = TRUE)
qf(0.95,2,718, lower.tail = TRUE)
```







\bigskip\bigskip
***
\bigskip\bigskip

### (c) What do you conclude from your test of part (b)?

\begin{description}
\item[Answer:]
In either approaches, we can conclude that the parents' education affects daughter's IQ beyond their influence on the daughter's own education. Perhaps there is a direct genetic effect on IQ.
\end{description}




\bigskip\bigskip
***
\bigskip\bigskip

### (d) What do you conclude from your test of part (b)?

\begin{description}
\item[Answer:]
Notice from the coefficients of the unrestricted model that mother's education has twice the partial effect of the father's education and its statistical significance is higher. One could suggest that the emphasis of the policy should be on raising educational level on women.
\end{description}




\bigskip\bigskip
***
\bigskip\bigskip

### (e) Suppose I wish to know whether the impact of own and parents' education on IQ test score varies by race. Conduct such a test of hypothesis and report your finding.

\begin{description}
\item[Answer (though not necessarily a robust way to answer the question)]
One approach is to regress $IQ$ on father's education, mother's education, and own education for two subsets separately, using a dummy variable for $black$, and then test the hypothesis that the coefficents in the two regressions are the same.

To conduct this test we need three regressions: a full sample regression which we already obtained in part (b), a regression for the sub-sample of non-black individuals, and a regression for the sub-sample of black individuals:
\begin{align*}
black = 0: IQ &= \beta_0^{b0} + \beta_1^{b0} educ_i + \beta_2^{b0} meduc_i + \beta_3^{b0} educ_i +u_i \\
black = 1: IQ &= \beta_0^{b1} + \beta_1^{b1} educ_i + \beta_2^{b1} meduc_i + \beta_3^{b1} educ_i +u_i
\end{align*}

The hypothesis we are testing is whether the coefficients are equal or not:
\begin{align*}
\mathbb{H}_0: &(\beta_0^{b0} = \beta_0^{b1}) \cap (\beta_1^{b0} = \beta_1^{b1}) \cap (\beta_2^{b0} = \beta_2^[b1]) \cap (\beta_3^{b0} = \beta_3^{b1}) \\
\mathbb{H}_A: &(\beta_0^{b0} \neq \beta_0^{b1}) \cup (\beta_1^{b0} \neq \beta_1^{b1}) \cup (\beta_2^{b0} \neq \beta_2^{b1}) \cup (\beta_3^{b0} \neq \beta_3^{b1})
\end{align*}

To test this we use the Chow test with the following $F$-test statistic:
\[
F = \frac{\frac{\big(RSS - (RSS^{b0} + RSS^{b1})\big)}{k+1}}{\frac{(RSS^{b0}+RSS^{b1})}{n-2(k+1)}}
\sim F_{(k+1,(n-2(k+1)))}
\]

The estimation are as follows:
\end{description}

```{r}
#First divide data into two based on the dummy variable `black`:
wage2_nona_b1_df <- wage2_nona_df %>%
  filter(black==1)
wage2_nona_b0_df <- wage2_nona_df %>%
  filter(black==0)

# Estimate the regressions:
lm_FQ2e_b0 <- lm(IQ ~ feduc + meduc + educ, data = wage2_nona_b0_df)
lm_FQ2e_b1 <- lm(IQ ~ feduc + meduc + educ, data = wage2_nona_b1_df)

#Conduct the Chow test:
RSS_FQ2e_b0 <- deviance(lm_FQ2e_b0)
RSS_FQ2e_b1 <- deviance(lm_FQ2e_b1)

# For degrees of freedom:
k_FQ2e <- 3

# Chow Test F-statistic:
F_Chow_FQ2e <- ((RSS_unr_FQ2b-(RSS_FQ2e_b0 + RSS_FQ2e_b1))/(k_FQ2e+1))/((RSS_FQ2e_b0 + RSS_FQ2e_b1)/(n_FQ2b-2*(k_FQ2e+1)))

# F Critical value:
Crit_Value_FQ2e <- qf(0.95, df1=k_FQ2e+1, df2=n_FQ2b-2*(k_FQ2e+1),lower.tail = TRUE)

tbl_FQ2e <- as.table(c(
  "RSS" = round(RSS_unr_FQ2b,4),
  "RSS_b0" = round(RSS_FQ2e_b0, 4),
  "RSS_b1" = round(RSS_FQ2e_b1, 4),
  "Chow F-Stat" = round(F_Chow_FQ2e,4),
  "Crit. Value" = round(Crit_Value_FQ2e, 4),
  "df1" = k_FQ2e+1,
  "df2" = n_FQ2b-2*(k_FQ2e+1)
))

tbl_FQ2e %>%
   kbl(caption = "Chow Test for Faculty Question 2(e)") %>%
  kable_classic(full_width=FALSE)
```


The $F$-stat of $17.1274$ is much higher than the critical value of $2.3844$ at $95%$ with $(4,714)$ degrees of freedom. We can therefore reject the null hypothesis and conclude that the coefficients for the black and non-black individuals are different. Accordingly, from the data we can ascertain that race may have an effect on how one's own and parental education influences IQ.

However, notice that the Chow test here is testing the equality of all coefficients including the intercept. But in this question we are interested only in the three slope coefficients.

\bigskip
\textbf{Answer: A more robust approach}

A more appropriate approach would be to test the hypothesis of race difference with interacting variables as in the following model:
\[
IQ = \beta_0 + \beta_1 feduc_i + \beta_2 meduc_i + \beta_3 educ_i + \beta_4 (feduc_i \times black_i) + \beta_5 (meduc_i \times black_i) + \beta_6 (educ_i \times black_i) + u_i
\]
Notice that the additional variables are the interactions of the educational variables with the dummy variable. The coefficients $\beta_4, \beta_5$ and $\beta_6$ show the additional partial effect of father's education, mother's education, and own education for black individuals as compared to non-black individuals. Therefore, in order to test whether the impact of own and parents' education on IQ test score varies by race, we have the following hypothesis:
\begin{align*}
\mathbb{H}_0: & (\beta_4 = 0) \cap (\beta_5 = 0) \cap (\beta_6 = 0) \\[4pt]
\mathbb{H}_A: & (\beta_4 \neq 0) \cup (\beta_5 \neq 0) \cup (\beta_6 \neq 0)
\end{align*}

We therefore create a new set of variables with these interacting terms in the dataframe and then run the regression:

```{r}
# create the new variables
wage2_nona_df <- wage2_nona_df %>%
  mutate(
    educb = educ * black,
    feducb = feduc * black,
    meducb = meduc * black
  )

# run the regression

lm_FQ2e <- lm(IQ ~ feduc + meduc + educ + feducb + meducb + educb, data = wage2_nona_df)
```

We would now conduct an $F$-test where this model is the unrestricted regression, and the unrestricted model in 2(b) as the restricted regression here:

```{r}
#RSS unrestricted
# Manual calculation: sum(resid(lm_FQ2e_unr)^2)
RSS_unr_FQ2e <- deviance(lm_FQ2e)

#RSS restricted
#Manual calculation: sum(resid(lm_FQ2b_unr)^2)
RSS_res_FQ2e <- deviance(lm_FQ2b_unr)

# or to use R-squareds:
Rsq_unr_FQ2e <- summary(lm_FQ2e)$r.squared
Rsq_res_FQ2e <- summary(lm_FQ2b_unr)$r.squared

# Difference between the two models (ie number of restrictions):
q_FQ2e <- 3

# Sample size - since we have NAs in both columns but in different rows we need to remove them:
n_FQ2e <- nrow(wage2_nona_df)

# number of regressors in unrestricted model:
k_FQ2e <- 6

#F-statistic:
Fstat_FQ2e_withRSS <- ((RSS_res_FQ2e - RSS_unr_FQ2e)/q_FQ2e)/(RSS_unr_FQ2e/(n_FQ2e-k_FQ2e-1))
Fstat_FQ2e_withRsq <- ((Rsq_unr_FQ2e-Rsq_res_FQ2e)/q_FQ2e)/((1-Rsq_unr_FQ2e)/(n_FQ2e-k_FQ2e-1))

#F Critical value:
Crit_Value_FQ2e <- qf(0.95, df1=q_FQ2e, df2=n_FQ2e-k_FQ2e-1,lower.tail = TRUE)

tbl_FQ2b <- as.table(c("n" = n_FQ2e,
          "RSS_unr"=round(RSS_unr_FQ2e,4), 
          "RSS_res"=round(RSS_res_FQ2e,4), 
          "R-squared_unr" = round(Rsq_unr_FQ2e,4),
          "R-squared_res" = round(Rsq_res_FQ2e,4),
          "F-Stat (using RSS)"=round(Fstat_FQ2e_withRSS, 4),
          "F-Stat (using R-sq)"=round(Fstat_FQ2e_withRsq, 4),
          "Critical_Value @ 95%"=round(Crit_Value_FQ2e, 4),
          "df_1"=round(q_FQ2e, 4),
          "df_2"=round(n_FQ2e-k_FQ2e-1,4)))

tbl_FQ2b %>%
  kbl(caption = "F-Test for Faculty Question 2(e)") %>%
  kable_classic(full_width=FALSE)
```

The test statistic of $22.2773$ is greater than the critical value of $2.6174$ so we reject the null hypothesis and conclude that race has an effect on the way education affects IQ test score. 

As before, we could run an F-test in R instead of calculating it manually:
```{r}
car::linearHypothesis(lm_FQ2e, c("feducb=0", "meducb=0", "educb=0"))
```

which yields the same result.




\bigskip\bigskip\bigskip
\hdashrule[0.5ex]{\textwidth}{1pt}{3mm}
\bigskip\bigskip\bigskip

## QUESTION 3

\textbf{Let us revisit the following problem you did in the previous supervision sheet. Consider the regression model} $Y_i = \beta_0+\beta_1X_i + U_i$ \textbf{for an i.i.d. sample with} $N=1000$ \textbf{observations. Suppose} $U_i \sim i.i.d. (0,\sigma^2)$ \textbf{and the} $X_i$ \textbf{are i.i.d. for} $i=1,2,\dots,1000$ \textbf{and that} $X_i$ \textbf{is independent of} $U_i$. \textbf{Let} $\hat{\beta}_i$ \textbf{denote the OLS estimator of} $\beta_1$ \textbf{and consider another estimator} $\tilde{\beta_1}$ \textbf{of} $\beta_1$ \textbf{constructed in the following way:}
\[
\tilde{\beta}_1 = \frac{Y_3-Y_2}{X_3-X_2}
\]
\textbf{You can assume that} $X_i$ \textbf{are continuously distributed and that} $X_3-X_2$ \textbf{never takes the value of} $0$.

\textbf{Is} $\tilde{\beta}_1$ \textbf{a consistent estimator of} $\beta_1$\textbf{? Why?}

\begin{description}
\item[Answer:]
First observe that $\tilde{\beta}_1$ does not change as more data are added, and so it remains a random variable as $n \to \infty$. That is, it does not converge to $\beta_1$.

For consistency, the variance of this estimator also needs to converge to $0$ as $n \to \infty$. So, let's calculate its variance. For this, recall that the regressors $X_i$ are not fixed but are assumed to random and drawn from some distribution. This means, \underline{$X_i$ have an expectation and a variance}. Importantly, this also means $\mathbb{E}(X_i)\neq X_i$ but $E(X_i | X_i = x) = x$, or, alternatively, $\mathbb{E}(X_i|X_i)=X_i$. That is, conditional on $X_i$ taking a realization $x_i$, the expection of $X_i$ is $x$. Therefore, we will need to rely on \textit{the law of iterated expectations} to obtain the variance of this estimator.

For this, we need to follow these steps:
\begin{enumerate}
\item Express the estimator in terms of $X_1, X_2$ and $U_1, U_2$,
\item Obtain the variance of this estimator
\item Take the conditional expectation of this variance
\item Utilize the law of iterated expectations to derive the unconditional variance
\end{enumerate}

\item[Step 1: Express $\tilde{\beta}_1$ without $Y_2, Y_3$] 
\begin{align*}
\tilde{\beta}_1 
  &= \frac{Y_3-Y_2}{X_3-X_2} \\[4pt]
  &= \frac{\beta_0+\beta_1X_3 + U_3 - \beta_0 - \beta_1X_2 - U_2}{X_3-X_2} \\[4pt]
  &= \frac{\beta_1(X_3-X_2) + (U_3 - U_2)}{X_3-X_2} \\[4pt]
  &= \beta_1 + \frac{U_3 - U_2}{X_3-X_2}.
\end{align*}

\item[Step 2: Take variance of this expression]
\begin{align*}
Var(\tilde{\beta}_1)
  &= Var(\beta_1 + \frac{U_3 - U_2}{X_3-X_2}) \\[4pt]
  &= Var(\frac{U_3 - U_2}{X_3-X_2}) \ \ \ \text{because $\beta_1$ is a constant parameter} \\[4pt]
  &= \mathbb{E}\Bigg[\Big(\frac{U_3 - U_2}{X_3-X_2}\Big)^2\Bigg] - \Bigg[\mathbb{E}\Big(\frac{U_3 - U_2}{X_3-X_2}\Big)\Bigg]^2 \\[4pt]
  &= \mathbb{E}\Bigg[\Big(\frac{U_3 - U_2}{X_3-X_2}\Big)^2\Bigg] - \Bigg[\Big(\frac{1}{X_3-X_2}\Big)\big(\mathbb{E}(U_3) - \mathbb{E}(U_2)\big)\Bigg]^2 \\[4pt]
  &= \mathbb{E}\Bigg[\Big(\frac{U_3 - U_2}{X_3-X_2}\Big)^2\Bigg] - 0 \ \ \ \text{since } \mathbb{E}(U_i) = 0, \ i=2,3 \\[4pt]
\end{align*}

\item[Step3: Take the conditional expectation of this expression]
\begin{align*}
\mathbb{E}[Var(\tilde{\beta}_1|X_2,X_3)] 
  &= \mathbb{E}\Bigg[\Big(\frac{U_3 - U_2}{X_3-X_2}\Big)^2 \bigg{|} X_2, X_3 \Bigg] \\[6pt]
  &= \mathbb{E}\Bigg[\Big(\frac{U_3^2 + U_2^2 - 2U_3U_2}{(X_3-X_2)^2}\Big) \bigg{|} X_2, X_3 \Bigg] \\[6pt]
  &= \mathbb{E}\Big(\frac{1}{(X_3-X_2)^2} \bigg{|} X_2, X_3 X_2, X_3 \Big) \\
    &\qquad \times \Bigg[\mathbb{E}\big(U_3^2 | X_2, X_3\big) + \mathbb{E}\big(U_2^2 | X_2, X_3\big) - \mathbb{E}\big(2U_3U_2 | X_2, X_3\big)\Bigg] \\[6pt]
  &= \mathbb{E}\Big(\frac{1}{(X_3-X_2)^2} \bigg{|} X_2, X_3 X_2, X_3 \Big) (\sigma^2 + \sigma^2 - 0) \\
    &\qquad \text{since } Var(u_i|\vec{X})=\sigma^2, \ \ i = 1,\dots,n \\
    &\qquad \text{and } Cov(u_i,u_j)=\mathbb{E}(u_iu_j)-\mathbb{E}(u_i)\mathbb{E}(u_j) = \mathbb{E}(u_iu_j)=0 \\[6pt]
  &= \frac{2\sigma^2}{(X_3-X_2)^2} \ \ \ \text{since } \mathbb{E}\Big(\frac{1}{(X_3-X_2)^2} \bigg{|} X_2, X_3 X_2, X_3 \Big) = \frac{1}{(X_3-X_2)^2}.
\end{align*}

\item[Step 4: Utilize law of iterated expectations to derive unconditional variance]
\begin{align*}
Var(\tilde{\beta}_1) 
  &= \mathbb{E}\big[\mathbb{E}(\tilde{\beta}_1)|X_2,X_3\big] \\[6pt]
  &= \mathbb{E}\Bigg(\mathbb{E}\Bigg[\Big(\frac{U_3 - U_2}{X_3-X_2}\Big)^2 \bigg{|} X_2, X_3 \Bigg]\Bigg) \\[6pt]
  &= \mathbb{E}\Big(\frac{2\sigma^2}{(X_3-X_2)^2}\Big) \\[6pt]
  &= 2\sigma^2\mathbb{E}\Big(\frac{2\sigma^2}{(X_3-X_2)^2}\Big). 
\end{align*}

$\hookrightarrow$
Note that we cannot simplify this any further since $\mathbb{E}\left( \frac{1}{(X_3-X_2)^2} \right) \neq \frac{1}{(X_3-X_2)^2}$ unlike $\mathbb{E}\left(\frac{1}{(X_3-X_2)^2} \bigg{|} X_2, X_3\right) = \frac{1}{(X_3-X_2)^2}$. This is because $X_i$ are random variables which means $\mathbb{E}(X_i)\neq X_i$ but $\mathbb{E}(X_i|X_i=x)=x$, or alternatively $\mathbb{E}(X_i|X_i)=X_i$, as discussed at the top of this answer.

Now we can check how this variance behaves when $n$ tends to infinity. We can again see that this variance does not change as more data are added, and consequently does not converge to $0$ as $n \to \infty$. Accordingly, this estimator is not consistent.

\end{description}










\pagebreak

## STATA - FACULTY QUESTIONS

Load the data in STATA:
```{stata, collectcode=TRUE}
* Change the working directory to access the data file:
    quietly cd .. 
* Load the data:
    use Data/wage2
* Run the command for describe and summarize:
  * des
  * summ

* part (a) regression:
  * regress IQ feduc meduc

* part (b) regressions:
  * regress IQ feduc meduc educ
  * regress IQ educ if ((feduc!=.) & (meduc!=.))

* part (b) alternative way:
  * regress IQ feduc meduc educ
  * test feduc meduc

* part (e) regressions for not so robust answer:
  * reg IQ feduc meduc educ if black == 0
  * reg IQ feduc meduc educ if black == 1

* part (e) regressions for more robust answer:
  * generate the new variables:
      gen educb = educ * black
      gen feducb = feduc * black
      gen meducb = meduc * black
  * reg IQ feduc meduc educ feducb meducb educb
  * test feducb meducb educb

* put all of these into a neat table using the `STATA` command `esttab` from the `estout` package:
  * first install the package:
      ssc install estout, replace
  * then run the following regressions together to build the table:
      eststo: quietly reg IQ feduc meduc
      eststo: quietly reg IQ feduc meduc educ
      eststo: quietly reg IQ educ if ((feduc!=.) & (meduc!=.))
      eststo: quietly reg IQ feduc meduc educ if black == 0
      eststo: quietly reg IQ feduc meduc educ if black == 1
      eststo: quietly reg IQ feduc meduc educ feducb meducb educb
      esttab, r2 ar2 scalars(F rss rmse)
```

