---
title: "IIA-3 Econometrics: Supervision 3"
author: "Emre Usenmez"
date: "`r Sys.Date()`"
output: pdf_document
header-includes: 
  - \usepackage{amsmath, tcolorbox, dashrule}
  - \tcbuselibrary{listings,most}
---

<!-- This comment will not be displayed in the output. Below change to CSS style is to ensure the blocktexts are in the same form size as the rest of the text.-->

```{css style settings, echo = FALSE} 
blockquote {
    padding: 10px 20px;
    margin: 0 0 20px;
    font-size: 14px;
    border-left: 5px solid #eee;
}
```

# FACULTY QUESTIONS

\bigskip\bigskip

## QUESTION 1

\textbf{An investigator analysing the relationship between food expenditure, disposable income and prices across a random sample of 25 counties in the UK estimates the relationship}
\begin{alignat*}{4}
log(FOOD) = & 4.7377 {}+{} && 0.3506 \ log(PDI) {}-{} && 0.5086 \ log(PRICE) \\
& (0.6805) && (0.0899) && (0.1010)
\end{alignat*}

\textbf{where figures in paranthesis are standard errors and where:}

\textbf{FOOD = Average household expenditure on food}

\textbf{PDI = Average personal dosposable income}

\textbf{PRICE = Average price of food deflated by a general price index}

#### (i) Give an economic interpretation of the coefficients on log(PDI) and log(PRICE).

\begin{description}
\item[Answer:] The coefficient of $log(PDI)$ is income elasticity, and the coefficient of $log(PRICE)$ is the price elasticity.
\end{description}


\bigskip\bigskip
***
\bigskip\bigskip

### (ii) Test the hypothesis, using 5% significance level, that the coefficient of log(PRICE) is equal to zero against the alternative that it is nonzero.
\begin{description}
\item[Answer:] For this we would use a $t$-test for the hypothesis that
\[
\mathbb{H}_0: \beta_2=0 \text{vs } \mathbb{H}_{A}: \beta_2 \neq 0
\]
The $t$-statistic is:
\[
\frac{-0.5086 - 0}{0.1010} = -5.035644
\]
which is significantly different than $t_{0.025,24}=-2.064$, and so we would reject the null hypothesis. 
\end{description}



\bigskip\bigskip
***
\bigskip\bigskip

### (iii) Test the hypothesis using 5% significance level that the coefficient of log(INCOME) is equal to 1 against the alternative that it is significantly different from 1.

\begin{description}
\item[Answer:] For this we would use a $t$-test for the hypothesis that
\[
\mathbb{H}_0: \beta_1=1 \text{vs } \mathbb{H}_{A}: \beta_1 \neq 1
\]
The $t$-statistic is:
\[
\frac{0.3506 - 1}{0.0899} = -7.223582
\]
which is significantly different than $t_{0.025,24}=-2.064$, and so we would reject the null hypothesis. 
\end{description}






\bigskip\bigskip
***
\bigskip\bigskip

### (iv) You are now given the following extra information:
\begin{align*}
TSS &= \sum_{i=1}^n (y_i-\bar{y})^2 = 0.52876 \\
RSS &= \sum_{i=1}^n e_i^2 = 0.46276
\end{align*}
\textbf{Compute ESS and $R^2$ for the above regression}.

\begin{description}
\item[Answer:]
\begin{align*}
ESS &= TSS - RSS = 0.52876 - 0.46276 = 0.066 \\
R^2 &= \frac{ESS}{TSS} = \frac{0.066}{0.52876} = 0.1248203.
\end{align*}
\end{description}




\bigskip\bigskip
***
\bigskip\bigskip

### (v) Test the joint hypothesis at the 5% level that the two slope coefficients are all equal to zero against the alternative that at least one slope coefficient is nonzero.

\begin{description}
\item[Answer:] For this we would use an $F$-test for the hypothesis that
\[
\mathbb{H}_0: (\beta_1=0) \cap (\beta_2=0)
\]
The $F$-statistic is:
\[
F = \frac{\dfrac{ESS}{df_1}}{\dfrac{RSS}{df_2}} = \frac{\dfrac{0.066}{2}}{\dfrac{0.46279}{22}} = 1.568746
\]
or
\[
F = \frac{\dfrac{R^2_unr - R^2_res}{df_1}}{\dfrac{1-R^2_unr}{df_2}} = frac{\dfrac{0.1248203}{2}}{\dfrac{1-0.1248203}{22}} = 1.568847
\]
giving a marginal difference due to rounding errors. In either case the F-statistic is significantly different than $F_{0.05,2, 22}=0.0514$, and so we would reject the null hypothesis. 
\end{description}



\bigskip\bigskip\bigskip
\hdashrule[0.5ex]{\textwidth}{1pt}{3mm}
\bigskip\bigskip\bigskip

## QUESTION 2









\pagebreak

# SUPPLEMENTARY QUESTIONS

\bigskip\bigskip

## QUESTION 1

\textbf{In a study of the Cobb-Douglass production function, a researcher suspects that the parameters are subject to change over time. Data on output, $Y$, labor input, $X_1$, and capital stock, $X_2$, are available for years 1929 to 1967. $T$ represents the time trend. The results obtained are as follows (t-values in parantheses):}

\begin{alignat*}{6}
\text{Full Sample:  } \ \ \widehat{log Y} & {}={} && -3.02 {}+{} && 1.34 \ logX_1 {}+{} && 0.29 \ logX_2 {}+{} && 0.0052 \ T \\
 & && (-6.65) && (14.68) && (4.89) && (2.34) \\
 & && && R^2 = 0.99535 && &&\hat{\sigma} = 0.03274 \\
\text{1929-48:  } \ \ \widehat{log Y} & {}={} && -3.22 {}+{} && 1.36 \ logX_1 {}+{} && 0.32 \ logX_2 {}+{} && 0.0051 \ T \\
 & && (-4.63) && (4.95) && (1.36) && (1.40) \\
 & && && R^2 = 0.97853 && && \hat{\sigma} = 0.04449 \\
\text{1949-67:  } \ \ \widehat{log Y} & {}={} && -1.56 {}+{} && 1.02 \ logX_1 {}+{} && 0.33 \ logX_2 {}+{} && 0.0095 \ T \\
 & && (-2.21) && (7.58) && (2.33) && (1.85) \\
 & && && R^2 = 0.99565 && && \hat{\sigma} = 0.0135 \\
\end{alignat*}

#### a) Conduct a test of the hypothesis that the four regression coefficients are jointly the same in both sub-periods, against the alternative that they differ.
\begin{description}
\item[Answer:] 

This question is effectively testing if there is a structural break from the start of 1949 which may be caused by different intercept, different slope coefficient, or both. Suppose the coefficients of the full sample regression are $\beta$s, and coefficients of the 1929-48 regression are $\psi$s, and coefficients of the 1949-67 are $\gamma$s. Our hypothesis is therefore:
\[
\mathbb{H}_0: (\psi_0=\gamma_0) \cap (\psi_1=\gamma_1) \cap (\psi_2=\gamma_2) \cap (\psi_3=\gamma_3) 
\]
We can test this in two ways. First is to reparameterize the model and then run an $F$-test, and the second is to run a Chow test.

\underline{First Approach:}

We can create a new coefficient $\delta = \psi-\gamma$ whereby our model becomes: $\widehat{logY}=\delta_0 + \delta_1 \ log X_1 + \delta_2 \ log X_2 + \delta_3 T + \varepsilon$ for which the hypothesis becomes:
\[
\mathbb{H}_0: (\delta_0=0) \cap (\delta_1=0) \cap (\delta_2=0) \cap (\delta_3=0)
\]
We would then use the $F$-test for this join hypothesis.

\underline{Second Approach:}

An alternative appraoch is to use \textit{Chow Test}.\footnote{Chow, C Gregory (1960) \textit{Tests of Equality Between Sets of Coefficients in Two Linear Regressions}, Econometrica, 28(3) 591:605} This test assumes that:
\begin{itemize}
  \item The error terms in the subperiod regressions are normally distributed with the same, i.e. homoskedastic, variance $\sigma^2$. That is, $u_{1929-48t} \sim N(0,\sigma^2)$ and $u_{1949-67t} \sim N(0,\sigma^2)$.
  \item The two error terms $u_{1929-48t}$ and $u_{1949-67t}$ are independently distributed.
\end{itemize}

The Chow test is an $F$-ratio which means we will need the $RSS$ of both unrestricted and restricted models. Here, \textit{the full sample model is the restricted model} since that is the model we have by imposing the restrictions that all $\psi_j=\gamma_j$ for $j=0,\dots,3$. The $RSS$ of the unrestricted model, on the other hand - and this is a key insight - is the combination of the two sub-sample $RSS$s.

\begin{tcolorbox}[breakable, title=Why $RSS$ and not $R^2$ form of $F$-Test, skin=enhancedlast]
Note that the Chow test uses $RSS$ and that there is no simple $R^2$ form of the $F$-test if separate regressions have been estimated for each group. This is because the $TSS$s are not the same as $\bar{Y}$ is not the same in both samples.
\end{tcolorbox}

The steps to carry out a Chow test is as follows:
\begin{enumerate}
  \item Obtain the restricted model's residual sum of squares, $RSS_R$, by estimating the regression for the full sample model with $(n-k-1)$ degrees of freedom, where $n=n_1+n_2$ with $n_1$ being the sample size of the first sub-sample, and $n_2$ being he sample size of the second sub-sample, and where $k$ is the number of regressors in that model.
  \item Estimate the first sub-sample model to obtain its residual sum of squares, $RSS_1$ with $n_1-k-1$ degrees of freedom.
  \item Do the same for the second sub-sample model to obtain $RSS_2$ with $n_2-k-1$ degrees of freedom.
  \item Add the two $RSS$s to compute the unrestricted model's residual sum of squares: $RSS_{UR}=RSS_1+RSS_2$.
  \item Compute the $F$-ratio:
  \[
  F = \frac{\dfrac{RSS_R-RSS_{UR}}{k+1}}{\dfrac{RSS_{UR}}{n-2(k+1)}} = \frac{\dfrac{\text{improvement in fit}}{\text{extra degrees of freedom used up}}}{\dfrac{\text{residual sum of squares remaining}}{\text{degrees of freedom remaining}}}
  \]
  \subitem
  $\hookrightarrow$
    Because we are splitting the sample into two, we are estimating k regressors for each sample plus their intercepts, so we are using up $(k+1)$ degrees of freedom twice.
  \item Compare the $F$-ratio to the critical $F$ value with \big($(k+1),n-2(k+1)$\big) degrees of freedom and fail to reject the null hypothesis of \textit{parameter stability}, i.e. no structural change, if $F$-ratio does not exceed the critical value at the chosen significance level.
\end{enumerate}

Accordingly, we first need to calculate the $RSS$s. For that, recall that $RSS = \hat{\sigma}^2(n-k-1)$ where $n=39, n_1=20, n_2=19$ because the dates are inclusive. Therefore:

\begin{align*}
RSS_R 
  &= 0.03274^2\times 35 
  &= 0.03751677 \\
RSS_UR 
  &= RSS_1 + RSS_2 \\ 
  &= 0.04449^2\times 16 + 0.0135^2\times 15 \\
  &= 0.03440351
\end{align*}

With these we can now calculate our $F$-ratio:
\[
F = \frac{\dfrac{0.03751677-0.04304995}{4}}{\dfrac{RSS_{UR}}{n-2(k+1)}} = \frac{\dfrac{0.03751677 - 0.03440351}{4}}{\dfrac{0.03440351}{31}} = 0.7013157
\]

The F-statistic for $\alpha=0.05$ is $2.678667$ and for $\alpha=0.01$ is $3.992811$, and thus we fail to reject the null hypothesis of parameter stability at either of the $\psi$ values.
\end{description}

```{r}
qf(p=c(0.05, 0.01), df1=4, df2=31, lower.tail = FALSE)
```


\begin{tcolorbox}[breakable, title=Why $RSS$ equals $\hat{\sigma}^2(n-k-1)$?, skin=enhancedlast]
Consider how we estimate the error variance, $\sigma^2$. First notice that $\sigma^2=\mathbb{E}(u^2)$, so an unbiased estimator of $\sigma^2$ is $\frac{1}{n}\sum_{i=1}^nu_i^2$. However, since we do not observe the errors $u_i$ this is not a true estimator. What we have, though, is the estimates of the errors $u_i$ which are the OLS residuals $\hat{u}_i$. If we replace the errors with the OLS residuals then we have
\[
\sigma^2 = \frac{\sum_{i=1}^nu_i^2}{n} = \frac{RSS}{n}
\]
which is a true estimator because it gives a computable rule for any sample of data on $X$s and $Y$. However, this is biased because it does not account for the restrictions that must be satisfied by the OLS residuals. These restrictions are given by the two OLS first order conditions:
\[
\sum_{i=1}^n\hat{u}_i = 0, \ \ \ \ \ \ \sum_{i=1}^nX_i\hat{u}_i=0
\]
for a simple regression with one regressor. In a way, if we know $n-k-1$ residuals, we can always get the other remaining residuals by using the restrictions implied by the first order conditions. Therefore there are only $n-k-1$ degrees of freedom in the OLS residuals, as opposed to $n$ degrees of freedom in the errors.

The unbiased estimator of the error variance is therefore:
\[
\hat{\sigma}^2 = \frac{\sum_{i=1}^n\hat{u}_i^2}{n-k-1} = \frac{RSS}{n-k-1}
\]
where in simple regression with one regressor, $k=1$.
\end{tcolorbox}


\bigskip\bigskip
***
\bigskip\bigskip

### (b) It is believed that irrespective of whether the form of the relationship has changed over the two periods (i.e. whether the coefficients of the equation have changed), there has still been a structural break. Test this hypothesis and use this result to comment on the assumptions made in part (a). What limitations are there to these methods for testing for stability over the whole period.

\begin{description}
\item[Answer:] What this question is effectively asking is that whether the error variances of the two subperiod regressions are the same. Recall from part (a) one of the two key assumptions of the Chow test is that the variances of the errors are homoskedastic. Therefore the hypothesis is:
\begin{align*}
\mathbb{H}_0: \sigma_\psi = \sigma_\gamma \ \ \ \ \ \ \ \text{or } \ \ \ \ \mathbb{H}_0: \frac{\sigma_\psi}{\sigma_\gamma}=1 \\
\mathbb{H}_1: \sigma_\psi \neq \sigma_\gamma \ \ \ \ \ \ \ \text{or } \ \ \ \ \mathbb{H}_0: \frac{\sigma_\psi}{\sigma_\gamma}\neq 1
\end{align*}
If the variances of the two subpopulations are the same, i.e. $\sigma_\psi=\sigma_\gamma$, as assumed by the Chow test, then the ratios of the ratios of estimated error variance to population error variance has an $F$ distribution with $(n_1-k-1)$ and $(n_2-k-1)$ degrees of freedom in the numerator and denominator, respectively. That is,
\[
\frac{\dfrac{\hat{\sigma}_\psi^2}{\sigma_\psi^2}}{\dfrac{\hat{\sigma}_\gamma^2}{\sigma_\gamma^2}} \sim F_{(n_1-k-1),(n_2-k-1)}.
\]

Notice that if $\sigma_\psi=\sigma_\gamma$ then this ratio and thus the $F$-test becomes:
\[
F = \frac{\sigma_\psi^2}{\sigma_\gamma^2}
\]
where by convention the larger estimated variance is in the numerator.

Accordingly, our $F$-statistic is:
\[
F = \frac{0.04449^2}{0.0135^2} = 10.86069.
\]

Since this is a two-tailed test, but we are putting the higher variance in the numerator, then we can treat it as one-sided test with alternative hypothesis is greater than $1$, The critical values for $\alpha=0.05$ with $(16,15)$ degrees of freedom is $2.384875$, and for $\alpha=0.01$ it is $3.485246$.

Thus at both $\alpha$ levels we can reject the null hypothesis and conclude that the subperiod variances are not the same at $\alpha=0.01$. 
This means, the assumption of Chow test does not hold and we shouldn't use the Chow test, at least not in this form. There are modifications to Chow test that can be utilized but that is beyond this class.

Another point regarding the Chow test to bear in mind is that it is sensitive to the choice of the time at which we divide the subperiods. The $F$ values would be different if the cut-off point was 1947 or 1949.

Finally, the Chow test will tell us only if the two regressions are different but not whether the difference is due to the intercepts, the slopes, or both. We can use dummy variables for that, though.

\end{description}

```{r}
qf(c(0.05, 0.01), df1 = 16, df2 = 15, lower.tail = FALSE)
```


\bigskip\bigskip\bigskip
\hdashrule[0.5ex]{\textwidth}{1pt}{3mm}
\bigskip\bigskip\bigskip

## QUESTION 2

\textbf{The following demand for money function was estimated from 60 observations, for which $\sum(M-\bar{M})^2=45600$:}
\[
\hat{M}_t = 284+0.56Y_t - 0.43M_{t-1} \ \ \ \ \ \ \ R^2=0.841
\]
\textbf{When a further 8 observations became available, the equation was re-estimated. The pooled data had $\sum(M-\bar{M})^2 = 50100$, and the re-estimated equation $R^2=0.818$. Carry out a Chow test for predictive failure. What do you conclude from your results? Explain carefully the role of the dummy variable in this test.}

\begin{description}
\item[Answer:]
The approach to the Chow test is similar to Question 1 whereby the $RSS$ for restricted model is the pooled data. Hwoever, notice we are not running a two separate regressions on the subsamples. So we follow the first approach from Question 1(a) by taking the differences of the model:
\[
\mathbb{H}_0: (\delta_0=0) \cap (\delta_1=0) \cap (\delta_2=0) 
\]
where $\delta$s are the coefficients of model $D_t$ which is the difference between the initial model and the model after the new observations.

Accordingly, we first need to derive the respective $RSS$s using the identity $R^2=1-\frac{RSS}{TSS}$ or $RSS = (1-R^2)TSS$ where $TSS=\sum(M-\bar{M})^2$:
\begin{align*}
RSS_R &= (1-0.841)\times 45,600 = 7,250.4 \\
RSS_{UR} &= (1-0.818)\times 50,100 = 9,118.2
\end{align*}

We can then calculate the Chow test whereby
\[
F = \\frac{\dfrac{9118.2-7250.4}{8}}{\dfrac{7250.4}{57}} = 1.835495
\]
The $F$ critical values for $\alpha=0.05$ and for $\alpha=0.01$ with $(8,57)$ degrees of freedom are $2.105599$ and $2.840694$ respectively. Accordingly we cannot reject the null hypothesis and conclude that there is no predictive failure.  

\end{description}

```{r}
qf(c(0.05, 0.01), df1 = 8, df2 = 57, lower.tail = FALSE)
```




\bigskip\bigskip\bigskip
\hdashrule[0.5ex]{\textwidth}{1pt}{3mm}
\bigskip\bigskip\bigskip

## QUESTION 3

### (a) Use the dataset sup3.xls to import the following variables for the period 1955-1990:
\begin{description}
\item[W =] \textbf{Wages and Salaries (£ million), CSO code: CFAJ\_AU}
\item[P =] \textbf{Implied Deflator for Consumers Expenditure, CSO code: GIEF\_AU} 
\item[E =] \textbf{Employees in Employment, CSO code: BCAD\_AU}
\item[WF =] \textbf{Workforce, CSO code: DYDB\_AU}
\item[U =] \textbf{Unemployed, CSO code: BCAB\_AU}
\end{description}

Load the libraries:
```{r}
libraries <- c("haven",       # to import/export SPSS, STATA, SAS files
               "readxl",      # to import/export Excel files   
               "tidyverse",   # for tidy data
               "rstatix")     # converts stats functions to a tidyverse-friendly format

# lapply(libraries, library, character.only=TRUE) will load the libraries
```

```{r include = FALSE}
invisible(lapply(libraries, library, character.only=TRUE))
```

Load the data:
```{r}
salaries_df <- read_excel("../Data/sup3.xls", sheet = 1)
```
Briefly examine the data frame
```{r}
# Yuo can use any of the following to examine data frame (df): 
# `dim()`: for its dimensions, by row and column
# `str()`: for its structure
# `summary()`: for summary statistics on its columns
# `colnames()`: for the name of each column
# `head()`: for the first 6 rows of the data frame
# `tail()`: for the last 6 rows of the data frame
# `View()`: for a spreadsheet-like display of the entire data frame

summary(salaries_df)
```
We can change the column names to something more memorable:
```{r} 
colnames(salaries_df) <- c("Year", "W", "P", "E", "WF", "U")
```


\bigskip\bigskip
***
\bigskip\bigskip

### (b) Use the data set to estimate the following wage equation by the OLS method:
\[
\Delta ln W_t = \beta_0 +\beta_1 ln P + \beta_2 ln P_{t-1} + \beta_3 ln E_t + \beta_4 ln E_{t-1} + \beta_5 ln UR_t + \beta_6 ln UR_{t-1} +\varepsilon_t 
\]
\textbf{where $\beta_i$ are constants, $\Delta$ denotes the first difference operator and $UR_t=\frac{U_i}{WF_i}$}.

\textbf{Verify that your estimate of $\beta_1$ is $.91170$. Also show that the above is a generalized version of the following hypothesis:}
\[
\Delta ln\Big(\frac{W}{EP}\Big)_t = \beta_0 + \varepsilon.
\]
\textbf{How would you explain the inclusion of the $UR$ terms?}

\begin{description}
\item[Answer:]
\end{description}

```{r}
#Transform the data to get it ready for the regression
salaries_df<- salaries_df %>% 
  mutate(lnW = log(W),
         DlnW = lnW - lag(lnW,1),
         lnP = log(P),
         lag_lnP = lag(lnP,1),
         lnE = log(E),
         lag_lnE = lag(lnE,1),
         lnUR = log(U/WF),
         lag_lnUR = lag(lnUR,1))

#Let's put these in a new dataframe to keep them neat:
salaries_new_df <- data.frame(Year = salaries_df$Year,
         lnW = salaries_df$lnW,
         DlnW = salaries_df$DlnW,
         lnP = salaries_df$lnP,
         lag_lnP = salaries_df$lag_lnP,
         lnE = salaries_df$lnE,
         lag_lnE= salaries_df$lag_lnE,
         lnUR = salaries_df$lnUR,
         lag_lnUR = salaries_df$lag_lnUR,
         stringsAsFactors = FALSE)
```

Since we are taking the differences those columns should have any value for 1955:
```{r}
#Look at total number of `NA` values per column:
colSums(is.na(salaries_new_df))

```

Thus we can adjust our sample size to take account of this:
```{r}
n_unr <- nrow(salaries_new_df) - sum(apply(is.na(salaries_df), 1, any)) 
# Note, if we replace 1 with 2 within apply function it will give us 4, the total number of NAs. 
n_unr
```

Thus our sample size is $35$.

```{r}
# Run the regression
lm_Q3b <- lm(DlnW ~ lnP + lag_lnP + lnE + lag_lnE + lnUR + lag_lnUR, data = salaries_new_df)
summary(lm_Q3b)
```

We can therefore see that only lnP and lag_lnP coefficients are significant. However, $F$-stat is pretty high which suggests presence of multicollinearity. We can look at the variance inflation factor, VIF:

```{r}
car::vif(lm_Q3b)
```

where we see each of the $VIF$ value is greater than $5$ suggesting serious collinearity.


\bigskip\bigskip
***
\bigskip\bigskip

### (c) Append the variable $IP$ (strength of Income Policy Index) in Table 1 to your data set (use the `generate` command in Stata to generate a new variable, or copy and paste the data from the question sheet) and run the above regression including $IP$ as an additional regressor and give both a statistical and an economic interpretation of your results.

\begin{description}
\item[Answer:]
Once we append $IP$, the model we will be testing is:
\[
\Delta ln W_t = \beta_0 +\beta_1 ln P + \beta_2 ln P_{t-1} + \beta_3 ln E_t + \beta_4 ln E_{t-1} + \beta_5 ln UR_t + \beta_6 ln UR_{t-1} + \beta_7 IP_t + \varepsilon_t
\]
\end{description}

```{r}
#Append IP with data from the question

salaries_new_df <- salaries_new_df %>% 
  mutate(IP = 0)
salaries_new_df$IP[salaries_new_df$Year == 1962] <- 1.0
salaries_new_df$IP[salaries_new_df$Year == 1965] <- 1.0
salaries_new_df$IP[salaries_new_df$Year == 1966] <- 1.5
salaries_new_df$IP[salaries_new_df$Year == 1967] <- 1.0
salaries_new_df$IP[salaries_new_df$Year == 1968] <- 1.5
salaries_new_df$IP[salaries_new_df$Year == 1969] <- 1.75
salaries_new_df$IP[salaries_new_df$Year == 1975] <- 1.0
salaries_new_df$IP[salaries_new_df$Year == 1976] <- 3.0
salaries_new_df$IP[salaries_new_df$Year == 1977] <- 4.5
salaries_new_df$IP[salaries_new_df$Year == 1978] <- 1.0
salaries_new_df$IP[salaries_new_df$Year == 1979] <- 1.0

# Now the number of regressors increased to 7:
k_3c <- 7

#Run the regression
lm_Q3c <- lm(DlnW ~ lnP + lag_lnP + lnE + lag_lnE + lnUR + lag_lnUR + IP, data = salaries_new_df)
summary(lm_Q3c)
```

With the introduction of IP, we now see that all are statistically significant except for $UR$ and its lag, as well as the intercept. Thus we can see that by adding a required variable, $IP$, we reduce the standard error for the whole equation, which in turn, increases the $t$-values of the other coefficients to rise; except for $UR$ which, as we will see, is because $UR$ has no effect.

$R^2$ is not helpful here since it will always increase when a new variable is added. Adjusted $R^2$ also increases from $0.7965$ to $0.8728$. However, a more useful indicated here is the $F$-statistic which tests that all slope coefficients are zero. If this is higher, then the equation is an improvement. With the introduction of IP, we see an increase from $23.18$ to $34.31$.

We can also check the significance of $IP$ by looking at its $t$-value which is $-4.216$. Since the $p$-value is $0.00025$ we know it is statistically significant and we don't need to calculate the $t$-statistic. As long as $p$-value is less than $0.05$ it is significant at the $5%$ level.


\bigskip\bigskip
***
\bigskip\bigskip

### (d) Test the following set of restrictions jointly by re-specifying your preferred equation:
\[
\beta_1+\beta_2 = 0; \ \ \beta_3+\beta_4=0; \ \ \beta_5+\beta_6 = 0
\]
\textbf{Verify your results using the Wald Test (the} `test` \textbf{command in Stata). Interpret your results.}

\begin{description}
\item[Answer:]
\end{description}
We can do this in two different ways. First is to use the `test` command in Stata, or `linearHypothesis` command from the `car` package in R for the Wald Test. The second way is to impose the constraint on the equation and then compare the constrained and unconstrained equations, testing the restriction on the constrained equation.

```{r}
# Approach 1:

car::linearHypothesis(lm_Q3c, c("lnP=-lag_lnP", "lnE=-lag_lnE", "lnUR=-lag_lnUR"))
```
which gives us the $F$-statistic as $0.7598$.

In the second approach, the restricted model we will test is:
\[
\Delta ln W_t = \beta_0 +\beta_1 \Delta ln P_t + \beta_3 \Delta ln E_t + \beta_5 \Delta ln UR_t + \beta_7 IP_t + \Delta \varepsilon_t
\]


\begin{tcolorbox}[breakable, title=Why restricted model is a \textit{dynamic} regression model?, skin=enhancedlast]
Suppose we have the following model:
\[
Y_t = \beta_0 + \beta_1X_t + u_t
\]
This model is known as \textit{level form}. Since this level form holds true for every time period, this can also be written as their lagged values:
\[
Y_{t-1} = \beta_0 + \beta_1X_{t-1} + u_{t-1}
\]
If we subtract the latter from the former, we will get what is called \textit{first difference form}:
\[
\Delta Y_t = \beta_1\Delta X_t + \Delta u_t
\]
where the \textit{first difference operator}, $\Delta$, tells us to take successive differences of the variables in the equation. 

So, for example, if $X$ and $Y$ in the level form represent the logarithms of income and consumption expenditure, then $\Delta X$ and $\Delta Y$ represent changes in the logs of income and consumption expenditure, respectively. So, if we are interested in the relationships between the variables in their growth form, then the difference form may be more appropriate. This is because a change in the log of a variable is a relative change, or a percentage change if multiplied by 100.

If the error term in the level form satisfies the OLS assumptions, especially the assumption of no autocorrelation, then the error term $v_t = \Delta u_t = u_t - u_{t-1}$ is autocorrelated. To see this we need to take the expectation and variance of $v_t$:
\[
\mathbb{E}(v_t) = \mathbb{E}(u_t - u_{t-1}) = \mathbb{E}(u) - \mathbb{E}(u_{t-1}) = 0
\]
since $\mathbb{E}(u)=0$ for each t.
\begin{align*}
Var(v_t) 
  &= Var(u_t - u_{t-1}) \\
  &= Var(u_t) + Var (-u_{t-1}) \\
  &= \sigma^2 + \sigma^2 \\
  &= 2\sigma^2 
\end{align*}
thus $v_t$ is homoskedastic. However we also need to look at the covariance of $v_t$ and $v_{t-1}$:
\begin{align*}
Cov(v_t, v_{t-1})
  &= \mathbb{E}(v_t v_{t-1}) - mathbb{E}(v_t)mathbb{E}(v_{t-1}) \\
  &= \mathbb{E}\big( (u_t-u_{t-1})(u_{t-1}-u_{t-2}) \big) - 0 \\
  &= \mathbb{E}( u_t u_{t-1} - u_t u_{t-2} - u_{t-1}^2 + u_{t-1}u_{t-2}) \\
  &= \mathbb{E}(-u_{t-1}^2) \\
  &= -\sigma^2
\end{align*}
which is nonzero. Therefore, although the $u$'s are not autocorrelated the $v$'s are.

Now to see why the restrictions in the question are expressed in the first difference form, consider a general distributed-lag model with a finite lag of $k$ time periods:
\[
Y_t = \beta_0 + \beta_1 X_1 + \beta_2 X_{t-1} + \dots + \beta_{k+1} X_{t-k} + u_t
\]
The coefficient $\beta_1$ is known as the \textit{short-run multiplier}, or \textit{impact multiplier}, because it gives the change in the mean value of $Y$ following a unit change in $X$ in the same time period. If the change in $X$ is maintained at the same level thereafter, then $(\beta_1+\beta_2)$ gives the change in the mean value of $Y$ in the next period, $(\beta_1+\beta_2 + \beta_3)$ in the following period, so on. These partial sums are called \textit{interim multipliers} or \textit{intermediate multipliers}. After $k$ time periods we obtain:
\[
\sum_{i=1}^k+1 \beta_i = \beta_1 + \beta_1 + \dots + \beta_k+1 = \beta
\]
which is known as the \textit{long-run distributed-lag multiplier} or \textit{total distributed-lag multiplier}.

In this question, what we are testing is if that total distributed-lag multiplier is different from $0$ for first differences form.
\end{tcolorbox}

```{r}
#Approach 2

#Modify the data frame to add the constrained versions:
salaries_new_df <- salaries_new_df %>% 
  mutate(DlnP = lnP - lag_lnP,
         DlnE = lnE - lag_lnE,
         DlnUR = lnUR - lag_lnUR)

#Estimate the constrained model:
lm_Q3d <- lm(DlnW ~ DlnP + DlnE + DlnUR + IP, data = salaries_new_df)
summary(lm_Q3d)
```

We will then need to calculate the $F$-statistic:

\[
F = \frac{\dfrac{RSS_R - RSS_{UR}}{df_1}}{\dfrac{RSS_{UR}}{df_2}}
\]

which means we need to obtain the $RSS$s from both regressions which can be done either by `deviance` function in R or by calculating the $RSS$ manually:

```{r}
#RSS unrestricted
# Manual calculation: sum(resid(lm_Q3c)^2)
RSS_unr_Q3d <- deviance(lm_Q3c)

#RSS restricted
#Manual calculation: sum(resid(lm_Q3d)^2)
RSS_res_Q3d <- deviance(lm_Q3d)

# Difference between the two models (ie number of restrictions):
q_3d <- 3

#F-statistic with (3,27) df:
Fstat_Q3d <- ((RSS_res_Q3d - RSS_unr_Q3d)/q_3d)/(RSS_unr_Q3d/(n_unr-k_3c-1))

#F Critical value:
Crit_Value_Q3d <- qf(0.05, df1=q_3d, df2=n_unr-k_3c-1,lower.tail = FALSE)

as.table(c("RSS_unr"=RSS_unr_Q3d, 
           "RSS_res"=RSS_res_Q3d, 
           "F-Stat"=Fstat_Q3d, 
           "Critical_Value"=Crit_Value_Q3d, 
           "df_1"=q_3d, 
           "df_2"=n_unr-k_3c-1))
```

We again obtain the same $F$-statistic of $0.7598$ as in Approach 1, which is much lower than the critical value of $2.96$, thus we cannot reject the null hypothesis. That is, the constraints seem to be correct.


\bigskip\bigskip
***
\bigskip\bigskip

### (e) Test the further restrictions that $\beta_5 = \beta_6 = 0$. (N.B. your unrestricted equation is always your currently preferred equation, so in this case you only need to test $\beta_5=0$ explicitly.)

\textbf{Answer:}
We can respecify the constraint and run the regression on this model. Our constrained is $\beta_5=0$. This is because in part (d) we have $\beta_5+\beta_6 = 0$ thus if $\beta_5=0$ then $\beta_6=0$. a t-test or check the t-statistic of the regression summary:

```{r}
summary(lm_Q3d)$coefficients["DlnUR", "t value"]
```

We see that the $t$-stat for $DlnUR$ is $-0.4770329$. 

But the question asks this to be done by respecifying the model. Also note that this time our unrestricted model is the restricted model in (d) since we are imposing further restrictions. Accordingly the model we test is:

\[
\Delta ln W_t = \beta_0 +\beta_1 \Delta ln P_t + \beta_3 \Delta ln E_t + \beta_7 IP_t + \Delta \varepsilon_t
\]
```{r}
#Estimate the constrained model:
lm_Q3e <- lm(DlnW ~ DlnP + DlnE + IP, data = salaries_new_df)
summary(lm_Q3e)
```

As we did in part (d) we can obtain the $F$-statistic:
```{r}
# New RSS unrestricted:
RSS_unr_Q3e <- RSS_res_Q3d

# Number of regressors in the new unrestricted model:
k_3e <- 4

#RSS restricted
#Manual calculation: sum(resid(lm_Q3e)^2)
RSS_res_Q3e <- deviance(lm_Q3e)

# Difference between the two models (ie number of restrictions):
q_3e <- 1

#F-statistic with (1,30) df:
Fstat_Q3e <- ((RSS_res_Q3e - RSS_unr_Q3e)/q_3e)/(RSS_unr_Q3e/(n_unr-k_3e-1))

#F Critical value:
Crit_Value_Q3e <- qf(0.05, df1=q_3e, df2=n_unr-k_3e-1,lower.tail = FALSE)

as.table(c("RSS_unr"=RSS_unr_Q3e, 
           "RSS_res"=RSS_res_Q3e, 
           "F-Stat"=Fstat_Q3e, 
           "Critical_Value"=Crit_Value_Q3e, 
           "df_1"=q_3e, 
           "df_2"=n_unr-k_3e-1))
```

If we take the square of the t-statistic from the summary table, we should also get this F-statistic:
```{r}
summary(lm_Q3d)$coefficients["DlnUR", "t value"]^2
```

which gives us the F-Stat as expected.



\bigskip\bigskip
***
\bigskip\bigskip

### (f) Interpret your prefereed equation in light of your results to (c), (d), and (e).

\begin{description}
\item[Answer:]
The equation in (e), $\Delta ln W_t = \beta_0 +\beta_1 \Delta ln P_t + \beta_3 \Delta ln E_t + \beta_7 IP_t + \varepsilon_t$, should be our new preferred equation since on all measures it is the best equation so far. This is good for the first hypothesis, i.e. the real wage resistance hypothesis, as this is exactly what we would expect if the hypothesis is correct.
\end{description}


\bigskip\bigskip
***
\bigskip\bigskip

### (g) Test the hypothesis that $\beta_3=1$ by respecifying your preferred equation in terms of average wages and salaries. (Your F-statistic for this test should be about 0.086)

\begin{description}
\item[Answer:]
One way to answer this question is to run a $t$-test on the coefficient of $\Delta ln E$:
\end{description}

```{r}
(summary(lm_Q3e)$coefficients["DlnE","Estimate"]-1)/summary(lm_Q3e)$coefficients["DlnE","Std. Error"]
```

We check against the critical $t$ value
```{r}
qt(p=0.025, df=34, lower.tail = FALSE)
```

Thus we cannot reject the null hypothesis.

However, the question actually ask us to respecify the equation. For this, we set the coefficient on $\Delta ln E$ to 1, i.e. $\beta_3=1$ and then take it over to the other side of the equation so that the model becomes:
\[
\Delta ln W_t - \Delta ln E_t = \beta_0 +\beta_1 \Delta ln P_t + \beta_7 IP_t + \Delta \varepsilon_t
\]
or
\[
\Delta ln\Big(\frac{W_t}{E_t}\Big) = \beta_0 +\beta_1 \Delta ln P_t + \beta_7 IP_t + \Delta \varepsilon_t
\]
We can run the regression on this:

```{r}
# First modify the data frame to reflect the new dependent variable:
salaries_new_df <- salaries_new_df %>% 
  mutate("DlnWE" = DlnW-DlnE)

# Then run the regression:
lm_Q3g <- lm(DlnWE ~ DlnP + IP, salaries_new_df)
summary(lm_Q3g)
```

We can now derive the $F$-statistic as before:
```{r}
# New RSS unrestricted:
RSS_unr_Q3g <- RSS_res_Q3e

# Number of regressors in the new unrestricted model:
k_3g <- 3

#RSS restricted
#Manual calculation: sum(resid(lm_Q3g)^2)
RSS_res_Q3g <- deviance(lm_Q3g)

# Difference between the two models (ie number of restrictions):
q_3g <- 1

#F-statistic with (1,30) df:
Fstat_Q3g <- (RSS_res_Q3g - RSS_unr_Q3g/q_3g)/(RSS_unr_Q3g/(n_unr-k_3g-1))

#F Critical value:
Crit_Value_Q3g <- qf(0.05, df1=q_3g, df2=n_unr-k_3g-1,lower.tail = FALSE)

as.table(c("RSS_unr"=RSS_unr_Q3g, 
           "RSS_res"=RSS_res_Q3g, 
           "F-Stat"=Fstat_Q3g, 
           "Critical_Value"=Crit_Value_Q3g, 
           "df_1"=q_3g, 
           "df_2"=n_unr-k_3g-1))
```

we can check that this is the square of the $t$-stat:

```{r}
sqrt(Fstat_Q3g)
```



\bigskip\bigskip
***
\bigskip\bigskip

### (h) In the light of your preferred specification for the total period does it appear that price inflation is fully passed on in wage claims?

\begin{description}
\item[Answer:]
This question is basically asking if the coefficient of $\Delta ln P$ is equal to $1$ or not. We can use the $t$-test for this:
\end{description}

```{r}
(summary(lm_Q3g)$coefficients["DlnP","Estimate"]-1)/summary(lm_Q3g)$coefficients["DlnP","Std. Error"]
```

We check against the critical $t$ value
```{r}
qt(p=0.025, df=34, lower.tail = FALSE)
```

Thus we cannot reject the null hypothesis.


\bigskip\bigskip
***
\bigskip\bigskip

### (i) Interpret and comment upon your findings (especially in relation to the hypothesis suggested in (b)) and explain why your preferred equation is now given by:
\[
\Delta ln\Big(\frac{W_t}{E_t}\Big) = \beta_0 + \beta_1 \Delta ln P_t + \beta_7 IP_t + \Delta \varepsilon
\]

\begin{description}
\item[Answer:]
We have tested back as far as we can go and at each stage the real wage resistance hypothesis gives us the best equation. It is not clear what an hypothesis would have to do in order to be better.
\end{description}




\bigskip\bigskip
***
\bigskip\bigskip

### (j) Why might it be expected that a structural break took place around 1979/1980? Test the hypothesis that the structure of wage determination changed significantly from 1980 onwards using the method of dummy variables (NB this is a different test to those you used in Q1 and Q2). Discuss your results.

\begin{description}
\item[Answer:] The short answer is Thatcher. End to corporatism and wage bargaining etc.

To test this, rather than using the Chow Test, use the dummy variables method. For this, we define a variable, $M$ which is $0$ for the period 1956-1979, and $1$ for 1980 onwards. We then estimate the following:
\[
\Delta ln W_t = \beta_0 + \beta_1 \Delta ln P_t + \beta_8 M_t + \beta_9 M\Delta ln P_t + \Delta \varepsilon
\]

\end{description}

```{r}
# First, add Z column to the data frame:

salaries_new_df <- salaries_new_df %>%
  mutate(
    M = case_when(
      Year < 1980 ~ 0,
      Year >= 1980 ~ 1
    ),
    MDlnP = M * DlnP
  )

# Then run the regression:

lm_Q3j <- lm(DlnW ~ DlnP + M + MDlnP, salaries_new_df)
summary(lm_Q3j)
```

Notice that there is not a $MlnP$ term because if we include it we would have perfect multicollinearity. From the regression output we can see that neither $M$ nor $MDlnP$ are significant - see their t-stats, so we can conclude that there is not any evidence of structural break for these years. 






\bigskip\bigskip
***
\bigskip\bigskip

### (k) Discuss the advantages of using the method of dummy variables over the Chow Test you used in part 1(a).

\begin{description}
\item[Answer:]
With the dummy variable we find out which variable or variables have changed. Whereas Chow test tells us only that something has changed. That is, it will tell us if the two sub-regressions are different but not whether the difference is due to the intercepts, the slope, or both.
\end{description}
