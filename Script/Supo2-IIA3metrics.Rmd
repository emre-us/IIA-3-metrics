---
title: "2024-25 Tripos IIA Paper 3"
subtitle: "Supervision 2"
author: "Emre Usenmez"
date: "`r Sys.Date()`"
output: pdf_document
header-includes: 
  - \usepackage{tcolorbox, xcolor, changepage, float}
  - \tcbuselibrary{listings,most}
---

# FACULTY QUESTIONS

\tiny{Very grateful to Dr Oleg Kitov for the very informative stylized answers to previous iterations of the supervision questions.}
\normalsize

<!-- This comment will not be displayed in the output. Below change to CSS style is to ensure the blocktexts are in the same form size as the rest of the text.-->

```{css style settings, echo = FALSE} 
blockquote {
    padding: 10px 20px;
    margin: 0 0 20px;
    font-size: 14px;
    border-left: 5px solid #eee;
}
```



\bigskip\bigskip
 

### **QUESTION 1**

In order to understand the determinants of years of schooling among women in the UK, we collected a random sample of 857 individual females aged 28-38 from across the UK and gathered information on a number of background characteristics including their score on an IQ test. The summary statistics are as follows.


```{r echo = FALSE}
tbl1 <- matrix(c(13.58, 2.20, 9, 18, 32.98, 3.09, 28,38, 0.12,0.32,0,1,10.68,2.85,0,18,101.80,15.01,50,145), ncol=4, byrow=TRUE)
colnames(tbl1) <- c('Mean', 'Std Dev', 'Min', 'Max')
rownames(tbl1) <- c('educ', 'age', 'black', 'meduc', 'IQ')
tbl1 <- as.table(tbl1)

library(kableExtra)
tbl1 %>%
  kbl(caption = "Summary statistics for explanatory variables") %>%
  kable_classic(full_width=FALSE)
```

Here, $\mathit{educ}$ represents years of schooling, $\mathit{age}$ is recorded in years, $\mathit{meduc}$ is number of years of schooling for the individual's mother, and $\mathit{IQ}$ is the score on an IQ test. $\mathit{black}$ is a dummy variable identifying whether the woman has African origin. Define $\mathit{leduc}$ to be the natural log of years of education.


The OLS regression output with $\mathit{leduc}$ as the dependent variable is reported below. For the regression we have $N=857, R^2=0.33.$


```{r echo = FALSE}
tbl2 <- matrix(c(0.0050,0.0003,0.0000,0.1202,0.0361,0.0010,-0.0018,0.0005,0.0010,-0.0314,0.0152,0.039,0.0126,0.0017,0.0000,-0.0569,0.5981,0.924),ncol=3,byrow=TRUE)
colnames(tbl2) <- c("Coeff","Std err", "p-value")
rownames(tbl2) <- c("IQ","age", "agesq", "black", "meduc", "constant")

tbl2 %>%
  kbl(caption = "OLS regression for 'leduc' as the dependent variable") %>%
  kable_classic(full_width=FALSE)
```


\bigskip

#### (a) Interpret the coefficient 0.005 on IQ.

##### Answer: 
Lets first write down the model specification:
\[
\mathit{leduc}_i = \beta_0 + \beta_1IQ_i + \beta_2\mathit{age}_i + \beta_3\mathit{age}_i^2 + \beta_4\mathit{black}_i + \beta_5\mathit{meduc}_i + u_i
\]
The OLS estimator of $\beta_1$ is the partial effect of IQ holding other factors fixed:
\[
\frac{\partial{\mathit{leduc}_i}}{\partial{\mathit{IQ}}} \Bigg\rvert_{\mathit{age,black,meduc}} = \beta_1
\]
So, holding age, race, and mother's education constant, a one point increase in IQ increases education by 0.5%.






\bigskip\bigskip\bigskip\bigskip

***
\bigskip\bigskip

#### (b) How would you test if educational attainment is affected by age in this model?

##### Answer:
First, notice that we have two explanatory variables that depend on age. So we want to test $\mathit{age}$ and $\mathit{agesq}$ having no effect on educational attainment against either $\mathit{age}$ or $\mathit{age}^2$, or both, having effect on educational attainment. That is,

\begin{align*}
&\mathbb{H}_0: (\beta_2=0)\cap(\beta_3=0) \\[4pt]
&\mathbb{H}_1: (\beta_2\neq 0)\cup(\beta_3\neq 0)
\end{align*}

To conduct the test, we would first estimate the restricted model
\[
\theta_0 + \theta_1{IQ}_i +\theta_4{black}_i+\theta_5{meduc}_i + v_i
\]
and use the $F-test$ statistic that follows a $F_{q,n-k-1} = F_{2,857-5-1} = F_{2, 851}$ distribution:

\[
F = \frac{\dfrac{(RSS_{res} - RSS_{unr})}{q}}{\dfrac{RSS_{unr}}{n-k-1}} = \frac{\dfrac{(RSS_{res} - RSS_{unr})}{2}}{\dfrac{RSS_{unr}}{851}} \ \sim \ F_{q, n-k-1} \\[4pt]
\]

or,

\[
F = \frac{\dfrac{(R^2_{unr} - R^2_{res})}{q}}{\dfrac{1-R^2_{unr}}{n-k-1}} = \frac{\dfrac{(0.33 - R^2_{res})}{2}}{\dfrac{1-0.33}{851}} \ \sim \ F_{q, n-k-1}
\]

where, $RSS_{unr}$ and $RSS_{res}$ are the sums of squared residuals from the unrestricted and restricted models, respectively; and $R^2_{unr}$ and $R^2_{res}$ are the coefficients of determination of the unrestricted and restricted models, respectively.


\bigskip\bigskip

\begin{tcolorbox}[breakable, title=Why F-test?, skin=enhancedlast]


A joint hypothesis is where two or more restrictions are imposed on the regression coefficients. To test a joint hypothesis, $F$-statistic is used. It may seem possible to use a $t$-statistic to test the restrictions one at a time, but this approach would be unreliable. 

To see this, suppose that $t_1$ is the $t$-statistic for testing the null hypothesis that $\beta_2$ = 0 in this question and $t_2$ is the $t$-statistic for $\beta_3=0$. Do we then reject the joint null hypothesis $\mathbb{H}_0: (\beta_2=0)\cap(\beta_3=0)$ if either $t_1$ or $t_2$ exceeds 1.96 in absolute value?

Answering this requires characterizing the joint sampling distribution of $t_1$ and $t_2$. $\hat{\beta}_2$ and $\hat{\beta}_3$ have a joint normal distribution due to large sample, so under the joint null hypothesis $t_1$ and $t_2$ have a bivariate normal distribution where each $t$-statistic has a mean of 0 and a variance of 1.

Now consider the special case where $t_1$ and $t_2$ are uncorrelated and thus independent in large samples. $\mathbb{H}_0$ is not rejected only if both $|t_1| \leq 1.96$ and $|t_2| \leq 1.96$.

Because the $t_1$ and $t_2$ are independent, 

\begin{align*}
\mathbb{P}(|t_1| \leq 1.96 \text{ and } |t_2|\leq 1.96) 
&= \mathbb{P}(|t_1|\leq 1.96)\times \mathbb{P}(|t_2|\leq 1.96) \\
&= 0,95^2 = 0.9025 = 90.25%.
\end{align*}

So the probability of rejecting $\mathbb{H}_0$ when it is true is $1-0.95^2$=9.75%.$ which is much higher than $5%$ probability we would expect for rejecting the null when it is true. This means, using $t$-statistics one at a time results in rejecting the null too often. This is because it gives us too many chances; i.e. if we fail to reject using the first $t$-statistic, then we get to try again using the second.


\begin{adjustwidth}{1.5em}{1.5em}
$\hookrightarrow$
If the regressors are correlated, then the size of this one at a time appraoch, i.e. the probability of rejecting the null when it is true, depends on the magnitude of that correlation.
\end{adjustwidth}

Since using $t$-statistics one at a time results in null rejection rates that don't match the desired significance level, another approach is needed.

One approach is called the \textbf{Bonferroni method} modifies this $t$-statistics approach so that it uses critical values that ensure that its size equals its significance level. However, this method can have low power. That is, it can frequently fail to reject the null hypothesis when the alternative is true.

This is why F-test is preferred in testing joint hypothesis. It is more powerful, especially when the regressors are highly correlated. When the joint null hypothesis has two restrictions $(q=2)$ the $F$-statistic combines the two $t$-statistics using the formula
\[
F=\frac{1}{2}\left(\frac{t_1^2+t_2^2-2\hat{\rho}_{t_1,t_2}t_1t_2}{1-\hat{\rho}_{t_1,t_2}}\right)
\]
where $\hat{\rho}_{t_1,t_2}$ is an estimator of the correlation between the two $t$-statistics.

In the general case of $q$ restrictions, and if the error $u_i$ is homoskedastic, the $F$-statistic is expressed as
\[
F = \frac{\dfrac{(RSS_{res} - RSS_{unr})}{q}}{\dfrac{RSS_{unr}}{n-k-1}}.
\]
This can be thought of as $F$ measuring the relative increase in $RSS$ when moving from unrestricted to restricted model. Also notice that the denominator of $F$, $\dfrac{RSS_{unr}}{n-k-1}$ is the unbiased estimator of $Var(u)=\sigma_u^2$.

Another way to think of the question $F$-statistic addresses is to ask whether relaxing the $q$ restrictions improves the fit of the regression sufficiently that this improvement is unlikely to be the result of random sampling variation if the null is true. This means there is a relationship between $F$-statistic and $R^2$. A large $F$-statistic should be associated with a substantial increase in the regression $R^2$.

\begin{adjustwidth}{1.5em}{1.5em}
$\hookrightarrow$
If the error $u_i$ is homoskedastic, the $F$-statistic can be written in terms of the improvement in the fit of the regression measured either by increase in $R^2$ or decrease in the residual sum of squares.
\end{adjustwidth}

This $F$-statistic and the one that uses $RSS$s are sometimes referred to as \textbf{homoskedasticity-only $F$-statistic} because it is valid only if the error term is homoskedastic.

This relationship between $F$-statistic and $R^2$ is then expressed as
\[
F = \frac{\dfrac{(R^2_{unr} - R^2_{res})}{q}}{\dfrac{1-R^2_{unr}}{n-k-1}}.
\]

\end{tcolorbox}

\bigskip\bigskip

Also notice that if we differentiate $\mathit{leduc}_i$ with respect to $\mathit{age}_i$
\[
\frac{\partial{\mathit{leduc}_i}}{\partial{\mathit{age}_i}} = \beta_2 + 2\beta_3 \mathit{age}_i
\]
then we see that partial effect of age depends on age.

In addition, since $\hat{\beta}_2 = 0.1202 > 0$ and $\hat{\beta}_3 = -0.0018 < 0$, the relationship between natural log of education and age is *concave*.

We can also calculate the age for which the partial effect of age on log education becomes negative:
\[
age_i=\frac{-\hat{\beta}_2}{2\hat{\beta}_3} = \frac{-0.1202}{2\times -0.0018}
\]
```{r}
0.1202/(2*0.0018)
```

Thus from the age of 33.39, partial effect of age on education becomes negative.





\bigskip\bigskip
***
\bigskip\bigskip

#### (c) How would the coefficients and the intercept change if age was instead recorded in months? What would happen to the corresponding $t$-statistics? Why?

##### Answer: 
Lets introduce a new variable $\mathit{agemth}$ where $\mathit{agemth}_i = 12\mathit{age}_i.$ The regression then becomes

\begin{align*}
\mathit{leduc}_i 
&= \gamma_0 + \gamma_1\mathit{IQ}_i + \gamma_2\mathit{agemth}_i + \gamma_3\mathit{agemth}_i^2 + \gamma_4\mathit{black}_i + \gamma_5\mathit{meduc}_i + u_i \\
&= \gamma_0 + \gamma_1\mathit{IQ}_i + \gamma_212\mathit{age}_i + \gamma_312^2\mathit{age}_i^2 + \gamma_4\mathit{black}_i + \gamma_5\mathit{meduc}_i + u_i \\
&= \gamma_0 + \gamma_1\mathit{IQ}_i + 12\gamma_2\mathit{age}_i + 144\gamma_3\mathit{age}_i^2 + \gamma_4\mathit{black}_i + \gamma_5\mathit{meduc}_i + u_i \\
&= \beta_0 + \beta_1\mathit{IQ}_i + \beta_2\mathit{age}_i + \beta_3\mathit{age}_i^2 + \beta_4\mathit{black}_i + \beta_5\mathit{meduc}_i + u_i \\
\end{align*}

Thus the intercept and the coefficients of other variables that have not been transformed do not change. The coefficients of $age$ and $age^2$ change proportionally where $\gamma_2=\beta_2/12$ and $\gamma_3=\beta_3/144$, since the transformations of $age$ variable do not affect the partial effect of age on natural log of education. 

The $t$-statistic is obtained by adjusting the distance of the estimator from its hypothesized value by its standard error:
\[
t = \frac{\text{estimator - hypothesized value}}{\text{st. err. of estimator}} = \frac{\hat{\beta}_j-\beta_{j,\mathbb{H}_0}}{se(\hat{\beta}_j)}
\]
where $\mathit{se}(\hat{\beta}_j)$ is the standard error of $\hat{\beta}_j$ and is an estimator of the standard deviation of $\beta_j$, $\sigma_{\beta_j}$. That is, $\mathit{se}(\hat{\beta}_j) = \sqrt{\hat{\sigma}_{\beta_j}^2}$. The estimator of the variance of $\beta_j$, in turn, is
\[
\hat{Var}(\beta_j) = \hat{\sigma}_{\beta_j}^2 
= \frac{\sigma^2}{TSS_j(1-R_j^2)} 
= \frac{\sigma^2}{\left(\displaystyle\sum_{i=1}^n(X_{ij}-\bar{X}_j)^2\right)(1-R_j^2)}
\]
Since $Var(X) = \frac{1}{n}\sum_{i=1}^n(X_{ij}-\bar{X}_j)^2$, this can be rewritten as
\[
Var(\beta_j) = \frac{\sigma^2}{nVar(X_j)(1-R_j^2)}
\]
Since $\sigma$ is unknown, we replace it with its estimator $\hat{\sigma}$, which gives us the standard error of $\hat{\beta}_j.$
\[
se(\hat{\beta}_j) = \sqrt{\frac{\hat{\sigma}^2}{TSS_j(1-R_j^2)}} = \sqrt{\frac{\sigma^2}{nVar(X_j)(1-R_j^2)}}
\]
In this question, the standard error of the estimator $\gamma_2$ is then

\begin{align*}
se(\hat{\gamma}_2) 
&= \sqrt{\frac{\hat{\sigma}^2}{\left(\displaystyle\sum_{i=1}^n(\mathit{agemth}_i-\overline{\mathit{agemth}}_j)^2\right)(1-R_\mathit{agemth}^2)}} \\[4pt]
&= \sqrt{\frac{\hat{\sigma}^2}{\left(12^2\displaystyle\sum_{i=1}^n(\mathit{age}_i-\overline{\mathit{age}}_j)^2\right)(1-R_\mathit{age}^2)}} \\[4pt]
&= \sqrt{\left(\frac{1}{12}\right)^2\hat{Var}(\hat{\beta}_2)}
\end{align*}

where $\hat{\sigma}^2$ is the estimator of error variance, and $R_{\mathit{agemth}}^2$ is the coefficient of determination from regressing $\mathit{agemth}_i$ on all other explanatory variables. Accordingly, $R_{\mathit{agemth}}^2 = R_\mathit{age}^2$.

Plugging this into the $t$-statistic
\[
t_{\hat{\gamma}_2} = \frac{\hat{\gamma}_2}{\mathit{se}(\hat{\gamma}_2)} = \frac{\dfrac{\hat{\beta}_2}{12} }{\frac{1}{12}\mathit{se}(\hat{\beta}_2)} = t+{\hat{\beta}_2}.
\]
Thus, the $t$-statistic for $\hat{\beta}_2$ would not change which would make sense since statistical significance of the explanatory variable should not be impacted by measurement units.

We can apply the same argument to $\gamma_3$ with $\gamma_3 = \beta_3/144$.








\bigskip\bigskip

***
\bigskip\bigskip

#### (d) How would you test that the true coefficient on mother's education is 1/3 the negative of the true coefficient on black?

##### Answer: 
The question is asking us to test the following null and alternative hypothesis:

\begin{align*}
&\mathbb{H}_0: \beta_5 = -\frac{1}{3}\beta_4 \ \ \text{ or } \ \ \mathbb{H}_0: 3\beta_5+\beta_4=0 \\[4pt]
&\mathbb{H}_1: \beta_5 \neq -\frac{1}{3}\beta_4 \ \ \text{ or } \ \ \mathbb{H}_1: 3\beta_5+\beta_4\neq 0
\end{align*}

What we want is to be able to reparameterize the regression so that our null ends up with an estimator equal to 0. For that reparameterization, we can add and subtract $3\beta_5\mathit{black}_i$ since our null is $3\beta_5+\beta_4=0$ and $\beta_5$ is the parameter of $\mathit{meduc}_i$ in the original model specification while $\beta_4$ is of $\mathit{black}_i$:

\begin{align*}
\mathit{leduc}_i 
&= \beta_0 + \beta_1\mathit{IQ}_i + \beta_2\mathit{age}_i + \beta_3\mathit{age}_i^2 + \beta_4\mathit{black}_i + +3\beta_5\mathit{black}_i +\beta_5\mathit{meduc}_i - 3\beta_5\mathit{black}_i + u_i \\
&= \beta_0 + \beta_1\mathit{IQ}_i + \beta_2\mathit{age}_i + \beta_3\mathit{age}_i^2 + (\beta_4+3\beta_5)\mathit{black}_i +\beta_5(\mathit{meduc}_i - 3\mathit{black}_i) + u_i
\end{align*}

We can lump $(\mathit{meduc}_i - 3\mathit{black}_i)$ into a new variable for simplicity, say, $\mathit{meb}_i$. Then we can run regression on the following model:
\[
leduc_i = \phi_0 + \phi_1\mathit{IQ}_i + \phi_2\mathit{age}_i + \phi_3\mathit{age}_i^2 + \phi_4\mathit{black}_i +\phi_5\mathit{meb}_i + u_i
\]
where $\mathit{meb}_i = (\mathit{meduc}_i - 3\mathit{black}_i)$ and $\phi_4=(\beta_4+3\beta_5)$.
In this new specification, we would test

\begin{align*}
&\mathbb{H}_0: \phi_4=0 \ \ \ \text{ which is the same as } \ \ \beta_5 = -\frac{1}{3}\beta_4 \ \ \text{ in the original regression.} \\
&\mathbb{H}_1: \phi_4\neq 0
\end{align*}






\newpage
 

### **QUESTION 2**

Consider the regression model $Y_i = \beta_0 + \beta_1X_i + U_i$ for an i.i.d. sample with $N=1,000$ observations. Suppose $U\sim i.i.d.(0, \sigma^2)$ and the $X_i$ are i.i.d. for $i=1,2,\dots,1000$, and that $X_i$ is independent of $U_i$. Let $\hat{\beta}_1$ denote the OLS estimator of $\beta_1$ and consider another estimator of $\beta_1$, $\tilde{\beta}_1$, constructed in the following way:
\[
\tilde{\beta}_1 = \frac{Y_3+Y_1-2Y_2}{X_3+X_1-2X_2}.
\]
You can assume that $X_i$ are continuously distributed and that $(X_3+X_1-2X_2)^{-1}$ has finite expectation.





\bigskip\bigskip

#### (a) Is $\tilde{\beta}_1$ an unbiased estimator of $\beta_1$? Why? 

\begin{adjustwidth}{1.5em}{1.5em}
(Hint: $\mathbb{E}(\frac{A}{B})=\mathbb{E}(A\times \frac{1}{B})$ and also if $A, B$ are independent, what can we say about the random variables $A$ and $\frac{1}{B}$?)
\end{adjustwidth}

##### Answer: 

This question is about applying the law of iterated expectations to calculate the expectation of coefficient estimator $\tilde{\beta}_1$ when the regressors are not fixed numbers, but random variables instead.

\begin{adjustwidth}{1.5em}{1.5em}
$\hookrightarrow$
\textbf{Important}: Notice that we are assuming that the regressors $X_i$ are \textit{not fixed} but that they are \textit{random variables}. That is, $X_i$ are assumed to be random and are drawn from some distribution. This means, \underline{$X_i$ have an expectation and a variance}. Importantly, this also means $\mathbb{E}(X_i)\neq X_i$ but $\mathbb{E}(X_i|X_i=x)=x$, or alternatively $\mathbb{E}(X_i|X_i)=X_i.$ In other words, conditional on $X_i$ taking a realization $x$, the expectation of $X_i$ is $x$.
\end{adjustwidth}

Unbiasedness requires that the expectation of the estimator is the population parameter. Since $X_1, X_2, X_3$ are not fixed numbers but random variables, we can't take the unconditional expectation of $\tilde{\beta}_1$. Instead, we need to
\begin{itemize}
\item calculate the conditional expectation $\mathbb{E}(\tilde{\beta}_1|X_1,X_2,X_3)$ and
\item then utilize the law of iterated expectations.
\end{itemize}

In order to calculate the first step, first plug in the expression for $Y_i$ in the question into the expression for $\tilde{\beta}_1$ so that we can express it in terms of $X_1,X_2,X_3$ and $U_1,U_2,U_3:$

\begin{align*}
\tilde{\beta}_1 
&= \frac{Y_3+Y_1-2Y_2}{X_3+X_1-2X_2} \\[4pt]
&= \frac{\beta_0 + \beta_1X_3 + U_3 + \beta_0 + \beta_1X_1 + U_1 - 2(\beta_0 + \beta_1X_2 + U_2)}{X_3+X_1-2X_2} \\[4pt]
& = \frac{\beta_1(X_3+X_1-2X_2) + (U_3+U_1-2U_2)}{X_3+X_1-2X_2} \\[4pt]
& = \beta_1 + \frac{U_3+U_1-2U_2}{X_3+X_1-2X_2}.
\end{align*}

Now we can calculate the conditional expectation:

\begin{align*}
\mathbb{E}(\tilde{\beta}_1 | X_1, X_2, X_3)
&= \mathbb{E}\left(\beta_1 + \frac{U_3-U_1-2U_2}{X_3+X_1-2X_2} | X_1, X_2, X_3 \right) \\[6pt]
&= \beta_1 + \left(\frac{1}{X_3+X_1-2X_2} \right)\mathbb{E}(U_3+U_1-2U_2 | X_1, X_2, X_3) \\[6pt]
&= \beta_1 + \left(\frac{1}{X_3+X_1-2X_2} \right)\left(\mathbb{E}(U_3 | X_1, X_2, X_3) + \mathbb{E}(U_1 | X_1, X_2, X_3) - 2\mathbb{E}(U_2 | X_1, X_2, X_3) \right) \\[6pt]
&= \beta_1.
\end{align*}

The last equality holds because if we recall from the conditional mean expectation assumption, $\mathbb{E}(u_i|X_1, X_2, X_3) = 0$ for $i=1,2,3.$

For the second step, we utilize the law of iterated expectations whereby
\[
\mathbb{E}\left(\mathbb{E}(\tilde{\beta}_1 | X_1, X_2, X_3)\right) = \mathbb{E}(\tilde{\beta}_1) = \beta_1.
\]
Since the expectation of the estimator $\tilde{\beta}_1$ is the parameter $\beta_1$, this estimator is unbiased.



\bigskip\bigskip

***
\bigskip\bigskip

#### (b) Can $\tilde{\beta}_1$ be a better estimator? Why?

##### Answer: 

The word \textit{better} means lower variance. To answer this question we will again rely on the law of iterated expectations to first obtain a conditional variance, then unconditional variance. We can then compare that expression of the OLS variance by Gauss-Markov and see if it is better.

Lets start with finding the variance:
\begin{align*}
Var(\tilde{\beta}_1) 
&= Var\left(\beta_1 + \frac{U_3+U_1-2U_2}{X_3+X_1-2X_2} \right) \\[4pt]
&= Var\left(\frac{U_3+U_1-2U_2}{X_3+X_1-2X_2}\right) \ \ \text{   because $\beta_1$ is a constant parameter} \\[4pt]
&= \mathbb{E}\left[ \left(\frac{U_3+U_1-2U_2}{X_3+X_1-2X_2}\right)^2 \right] - \left[ \mathbb{E}\left(\frac{U_3+U_1-2U_2}{X_3+X_1-2X_2}\right)  \right]^2 \ \ \text{   since } Var(A) = \mathbb{E}(A^2)-\left(\mathbb{E}(A)\right)^2 \\[4pt]
&= \mathbb{E}\left[ \left(\frac{U_3+U_1-2U_2}{X_3+X_1-2X_2}\right)^2 \right] - \left[ \left(\frac{1}{X_3+X_1-2X_2}\right)\left(\mathbb{E}(U_3) + \mathbb{E}(U_1) - 2\mathbb{E}(U_2) \right) \right]^2 \\[4pt]
&= \mathbb{E}\left[ \left(\frac{U_3+U_1-2U_2}{X_3+X_1-2X_2}\right)^2 \right] - 0 \ \ \text{ since } \mathbb{E}(u_i)=0, \ i=1,2,3 \\[4pt]
&= \mathbb{E}\left(\frac{U_3^2+U_1^2+4U_2^2+2U_3U_1-4U_3U_2-4U_1U_2}{(X_3+X_1-2X_2)^2}\right).
\end{align*}

We can now take the conditional expectation of this:
\begin{align*}
\mathbb{E}\left[Var(\tilde{\beta}_1 | X_1, X_2, X_3)\right]
&= \mathbb{E}\left[ \left(\frac{U_3+U_1-2U_2}{X_3+X_1-2X_2}\right)^2 \bigg{|} X_1, X_2, X_3 \right] \\[6pt]
&= \mathbb{E}\left(\frac{U_3^2+U_1^2+4U_2^2+2U_3U_1-4U_3U_2-4U_1U_2}{(X_3+X_1-2X_2)^2} \bigg{|} X_1, X_2, X_3\right) \\[6pt]
\begin{split}
&= \mathbb{E}\left(\frac{1}{(X_3+X_1-2X_2)^2} \bigg{|} X_1, X_2, X_3 \right) \\
&\qquad \times \bigg{[} \mathbb{E}(U_3^2 | X_1, X_2, X_3) + \mathbb{E}(U_1^2 | X_1, X_2, X_3) \\
&\qquad + \mathbb{E}(4U_2^2 | X_1, X_2, X_3) + \mathbb{E}(2U_3U_1 | X_1, X_2, X_3) \\
&\qquad - \mathbb{E}(4U_3U_2 | X_1, X_2, X_3) - \mathbb{E}(4U_1U_2 | X_1, X_2, X_3)  \bigg{]}
\end{split}
\\[6pt]
\begin{split}
&= \mathbb{E}\left(\frac{1}{(X_3+X_1-2X_2)^2} \bigg{|} X_1, X_2, X_3 \right)\left(\sigma^2 + \sigma^2 + 4\sigma^2 +0-0-0 \right) \\
&\qquad \text{   since } Var(u_i | \vec{X}) = \sigma^2 \ , \ i=1,\dots,n \\
&\qquad \text{and } Cov(u_i,u_j)=\mathbb{E}(u_iu_j)-\mathbb{E}(u_i)\mathbb{E}(u_j) = \mathbb{E}(u_iu_j) = 0 
\end{split}
\\[6pt]
&= \frac{6\sigma^2}{(X_3+X_1-2X_2)^2} \ \ \ \ \text{ since } \mathbb{E}\left(\frac{1}{(X_3+X_1-2X_2)^2} \bigg{|} X_1, X_2, X_3\right) = \frac{1}{(X_3+X_1-2X_2)^2}.
\end{align*}

Now we can utilize the law of iterated expectations to derive the unconditional variance:
\begin{align*}
Var(\tilde{\beta}_1) 
&= \mathbb{E}[\mathbb{E}(\tilde{\beta}_1 | X_1, X_2, X_3)] \\[4pt]
&= \mathbb{E}\left(  \mathbb{E}\left[ \left(\frac{U_3+U_1-2U_2}{X_3+X_1-2X_2}\right)^2 \bigg{|} X_1, X_2, X_3 \right] \right) \\[4pt]
&= \mathbb{E}\left( \frac{6\sigma^2}{(X_3+X_1-2X_2)^2}  \right) \\[4pt]
&= 6\sigma^2\mathbb{E}\left( \frac{1}{(X_3+X_1-2X_2)^2} \right).
\end{align*}

\begin{adjustwidth}{1.5em}{1.5em}
$\hookrightarrow$
Note that we cannot simplify this any further since $\mathbb{E}\left( \frac{1}{(X_3+X_1-2X_2)^2}  \right) \neq \frac{1}{(X_3+X_1-2X_2)^2}$ unlike $\mathbb{E}\left(\frac{1}{(X_3+X_1-2X_2)^2} \bigg{|} X_1, X_2, X_3\right) = \frac{1}{(X_3+X_1-2X_2)^2}$. This is because $X_i$ are random variables which means $\mathbb{E}(X_i)\neq X_i$ but $\mathbb{E}(X_i|X_i=x)=x$, or alternatively $\mathbb{E}(X_i|X_i)=X_i$, as discussed in part (a).
\end{adjustwidth}

We can compare this to the variance of $\hat{\beta}_1:$
\[
Var(\hat{\beta}_1) = \frac{1}{n}\frac{Var\left((X_i-\mu_x)u_i\right)}{\left(Var(X_i)^2\right)}
\]
which we know has the lowest variance among all unbiased linear estimators due to Gauss-Markov theorem.






\bigskip\bigskip
***
\bigskip\bigskip

#### (c) Can you state the general result for estimators of the form $\sum_{i=1}^na_iY_i/\sum_{i=1}^na_iX_i$?

##### Answer: 
Consider the generalized estimator 
\[
\beta_1^* = \frac{\sum_{i=1}^na_iY_i}{\sum_{i=1}^na_iX_i}
\]
where $a_i$ are weights assigned by the estimator to each observation $(Y_i,X_i)$ for $i=1,\dots,n$. 

Notice that $\tilde{\beta}_1$ is one case of $\beta_1^*$. In part (b) we derived $Var(\tilde{\beta})$ as 
\[
Var(\tilde{\beta}_1) = 6\sigma^2\mathbb{E}\left( \frac{1}{(X_3+X_1-2X_2)^2} \right)
\]
which can also be generalized to:
\[
Var(\beta_1^*) = \left( \sigma^2\sum_{i=1}^n a_i^2 \right)\mathbb{E}\left[ \left( \sum_{i=1}^na_iX_i \right)^2 \right].
\]
Consequently, the conditional variance is:
\[
Var(\beta_1^* | \vec{X}) = \frac{\sigma^2\displaystyle\sum_{i=1}^na_i^2}{\left(\displaystyle\sum_{i=1}^na_iX_i\right)^2}
\]
where the conditional variance of $\hat{\beta}_1$ is
\[
Var(\tilde{\beta}_1 | X_1, X_2, X_3) = \frac{6\sigma^2}{(X_3+X_1-2X_2)^2}.
\]


\begin{tcolorbox}[title= Why OLS weights are the best?]
\textbf{Why OLS weights are the best?}

Recall the OLS estimator can be written as
\[
\hat{\beta}_1 = \frac{\displaystyle\sum_{i=1}^n(X_i-\bar{X})Y_i}{\displaystyle\sum_{i=1}^n(X_i-\bar{X})X_i} = \frac{\displaystyle\sum_{i=1}^na_i^{OLS}Y_i}{\displaystyle\sum_{i=1}^na_i^{OLS}X_i}
\]
where the weights are given by the deviation of the regressor from its mean, $a_i = (X_i - \bar{X})$. The OLS weights are the best in the sense that these are the weights that minimize the variance of the general linear unbiased estimator.


We can derive the OLS estimators by setting up the estimation problem as minimization of the variance of the general linear unbiased estimator with respect to the weights of individual observations:
\[
\underset{a_1,\dots,a_n}{min} Var(\beta_1^*|\vec{X}) = \underset{a_1,\dots,a_n}{min} \left(\frac{\sigma^2\displaystyle\sum_{i=1}^na_i^2}{\left(\displaystyle\sum_{i=1}^na_iX_i\right)^2} \right) \ \ \ \ \text{ s.t. } \sum_{i=1}^na_i=0.
\]

\begin{adjustwidth}{1.5em}{1.5em}
$\hookrightarrow$
*Note:* From part(a) we know that all the weights need to add up to 0 for unbiasedness.
\end{adjustwidth}
\end{tcolorbox}




\newpage

# SUPPLEMENTARY QUESTIONS


## QUESTION 1

Using the information given below:

### (i) Compute an estimated population correlation coefficient $\rho$, and the Ordinary Least Squares (OLS) estimates of $\alpha$ and $\beta$ in the regression of $c_t=$ real consumption expenditure, on $y_t=$ real disposable income, over the period 1948-2003 (dates are inclusive).

\begin{equation}
c_t = \alpha + \beta y_t + \epsilon_t, \ \ \ \ t = 1,2,\dots
\end{equation}

where

\begin{table}[htpb]
  \centering
  \begin{tabular}{l l}
    $\displaystyle\sum_{t=1}^n y_t=20680$ & $\displaystyle\sum_{t=1}^n c_t = 19223$ \\
    $\displaystyle\sum_{t=1}^n (y_t-\bar{y})^2 = 1491806$ & $\displaystyle\sum_{t=1}^n (c_t - \bar{c})^2 = 1154516$ \\
    $\displaystyle\sum_{t=1}^n (c_t - \bar{c})(y_t - \bar{y}) = 1309953$ & $\displaystyle\sum_{t=1}^n y_t^2 = 9128478$ \\
    $\displaystyle\sum_{t=1}^n c_t^2 = 7752851$ &
\end{tabular}
\end{table}

and $\bar{y}$ and $\bar{c}$ are the sample means of $y_t$ and $y_t$, respectively.

\begin{description}
\item[Answer:] First notice that $n=56$ since the dates are inclusive. Now we need to obtain the OLS estimates and the correlation coefficient:

\begin{align*}
\hat{\beta} 
  &= \frac{\displaystyle\sum_{t=1}^n (y_t - \bar{y})(c_t - \bar{c})}{\displaystyle\sum_{t=1}^n(y-\bar{y})^2} = \frac{1309953}{1491806} = 0.8780988. \\[6pt]
\hat{\alpha} 
  &= \bar{c}-\hat{\beta}\bar{y} = \frac{\displaystyle\sum_{t=1}^n c_t}{n} - \hat{\beta}\frac{\displaystyle\sum_{t=1}^n y_t}{n} = \frac{19223}{56}-0.8780988\frac{20680}{56} = 18.99853 \\[6pt]
\hat{\rho} 
  &= \frac{\displaystyle\sum_{t=1}^n (c_t - \bar{c})(y_t - \bar{y})}{\sqrt{\displaystyle\sum_{t=1}^n (y_t-\bar{y})^2}\sqrt{\displaystyle\sum_{t=1}^n (c_t - \bar{c})^2}} = \frac{1309953}{\sqrt{1491806}\sqrt{1154516}} = 0.9981586
\end{align*}

\end{description}





\bigskip\bigskip
***
\bigskip\bigskip

### (ii) Show that $\displaystyle\sum_{t=1}^n e_t^2 = \displaystyle\sum_{t=1}^n (c_t - \bar{c})^2 - \hat{\beta}\left( \displaystyle\sum_{t=1}^n (c_t - \bar{c})(y_t-\bar{y}) \right).$

\begin{description}
\item[Answer:] Recall that

\begin{align*}
RSS &= \sum_{t=1}^n e_t^2 = \sum_{t=1}^n(c_t - \hat{c}_t)^2 \\
  &= TSS - ESS = \sum_{t=1}^n (c_t - \bar{c})^2 - \sum_{t=1}^n (\hat{c}_t - \bar{c})^2 \\
  &= \sum_{t=1}^n (c_t - \bar{c})^2 - \sum_{t=1}^n \left( \hat{\alpha}+\hat{\beta}y_t - (\hat{\alpha}+\hat{\beta}\bar{y}) \right)^2 \\
  &= \sum_{t=1}^n (c_t - \bar{c})^2 - \sum_{t=1}^n \left( \hat{\beta}(y_t - \bar{y}) \right)^2 \\
  &= \sum_{t=1}^n (c_t - \bar{c})^2 - \hat{\beta}^2 \sum_{t=1}^n (y_t - \bar{y})^2 \\
  &= \sum_{t=1}^n (c_t - \bar{c})^2 - \hat{\beta} \frac{\displaystyle\sum_{t=1}^n (y_t - \bar{y})(c_t - \bar{c})}{\displaystyle\sum_{t=1}^n(y-\bar{y})^2} \sum_{t=1}^n (y_t - \bar{y})^2 \ \ \ \text{ plugged-in definiton of } \hat{\beta}  \\
  &= \sum_{t=1}^n (c_t - \bar{c})^2 - \hat{\beta} \left( \sum_{t=1}^n (y_t - \bar{y})(c_t - \bar{c}) \right)
\end{align*}

as desired. We can plug-in the values to obtain:

\begin{align*}
\sum_{t=1}^n e_t^2 &= \sum_{t=1}^n (c_t - \bar{c})^2 - \hat{\beta}\left( \sum_{t=1}^n (c_t - \bar{c})(y_t-\bar{y}) \right) \\
&= 1154516 - 0.8780988 \times 1309953 = 4247.843
\end{align*}

\end{description}





\bigskip\bigskip
***
\bigskip\bigskip

### (iii) Test separately the hypotheses that $\alpha$, $\beta$, and $\rho$ are zero, at the 5 percent significance level. (Note: the t-statistic for your test of $\beta$ and $\rho$ should be exactly the same in this case - check that it is) 

\begin{description}
\item[Answer:] Since the hypothesis is testing if they are individually equal to zero or not, we will use two-tailed $t$-test. Recall that 
\[
\frac{\bar{X}-\mu_X}{\sqrt{\dfrac{s_X^2}{n_X}}} \sim N(0,1).
\]
To apply this to each of $\alpha$, $\beta$, and $\rho$, we need to first find their standard errors. For that, we need to find their variances. We derived these in class notes under the discussion of "Precision of OLS Estimators: Standard Errors".
\[
\mathit{se}(\hat{\beta}) = \sqrt{ \dfrac{ \hat{\sigma}_{\epsilon}^2 }{ \displaystyle\sum_{t=1}^n (y-\bar{y})^2 } } = \sqrt{ \dfrac{ \hat{\sigma}_{\epsilon}^2 }{ 1491806 } }
\ \ \ \ \ \ \ \text{ , } \ \ \ \ \ \ \ 
\mathit{se}(\hat{\alpha}) = \sqrt{ \dfrac{ \hat{\sigma}_{\epsilon}^2 \displaystyle\sum_{t=1}^n y_t^2 }{ n \displaystyle\sum_{t=1}^n(y-\bar{y})^2 } } = \sqrt{ \dfrac{ \hat{\sigma}_{\epsilon}^2 \times 9128478 }{ 56 \times 1491806 } } \\
\]
so we will need to find the variance of the residuals next:
\[
\hat{\sigma}_{\epsilon}^2 = \frac{\displaystyle\sum_{t=1}^n e_t^2}{n-2} = \frac{4247.843}{54} = 78.66376
\]


Plugging this back in, we have
\[
\mathit{se}(\hat{\beta}) = \sqrt{ \dfrac{ 78.66376 }{ 1491806 } } = 0.007261581
\ \ \ \ \ \ \ \text{ , } \ \ \ \ \ \ \ 
\mathit{se}(\hat{\alpha}) =  \sqrt{ \dfrac{ 78.66376 \times 9128478 }{ 56 \times 1491806 } } = 2.931814 \\
\]
Now we can calculate the $t$-statistic with the values we calculated in part (i):
\begin{align*}
t_{\hat{\alpha}} 
  &= \frac{\hat{\alpha} - \mu_\alpha}{\mathit{se}(\hat{\alpha})} = \frac{18.99853 - 0}{2.931814} = 6.480128 \\[4pt]
t_{\hat{\beta}}
  &= \frac{\hat{\beta} - \mu_\beta}{\mathit{se}(\hat{\beta})} = \frac{0.8780988 - 0}{0.007261581} = 120.9249
\end{align*}

for both we can then reject the null hypothesis that they are individually equal to zero.

To understand $\rho$, recall that $\epsilon_t = c_t - \hat{c}_t$ where $\hat{c}_t = \hat{\alpha} + \hat{\beta}y_t.$ That is, the randomness in $c_t$ comes from two sources: $y_t$ and $\epsilon_t$. That is, the variance of the dependent variable is:
\[
Var(c_t) = \beta^2Var(y_t)+Var(\epsilon_t),
\]
This means the total variation in $c_t$ can be divided into two parts. The first part, $\beta^2Var(y_t)$ is the variation due to $y_t$, while the second part, $Var(\epsilon_t)$, is the variance of error, which is the variance remaining after knowing $y_t$. 

What we are then looking for is the proportion of variance of $c_t$ that is explained by variation in $y_t$, which is the definition of covariance between $c_t$ and $y_t$. That is, the closer both covary, the more $c_t$ can be approximated by a linear function of $y_t$: i.e. $c_t \approx \hat{c_t} = \alpha + \beta y_t$:
\[
\frac{\beta^2Var(y_t)}{Var(c_t)} = \frac{\left( \dfrac{Cov(c_t,y_t)}{Var(y_t)} \right)^2Var(y_t)}{Var(c_t)} = \frac{\bigg{(} Cov(c_t,y_t) \bigg{)}^2}{Var(c_t)} = \rho^2. 
\]
Since by definition, $\rho = \frac{Cov(c_t,y_t)}{\sqrt{Var(c_t)Var(y_t)}}.$

Thus, $\rho^2$ indicates the strength of our regression model in estimating $c_t$ from $Y_t$. In practice we do not have $\rho$ but we have the observed pairs $(c_1, y_1), (c_2, y_2), \dots, (c_n, y_n)$ and we estimate $\rho$ from the observed data, which is the definition of R-squared, or coefficient of determination.

This means, $\beta$ can also be expressed as:
\[
\beta = \sqrt{\frac{\rho^2Var(c_t)}{Var(y_t)}} = \rho\frac{\sigma_{c_t}}{\sigma_{y_t}}
\]

Therefore we can rewrite our model as:

\begin{align*}
c_t & = \alpha + \beta y_t + \epsilon_t \\
    &= \left( \bar{c} - \rho\frac{\sigma_{c_t}}{\sigma_{y_t}}\bar{y} \right) + \left( \rho\frac{\sigma_{c_t}}{\sigma_{y_t}} \right)y_t
\end{align*}

The standard error of $\rho$ was first derived by Pearson using maximum likelihood. 
\[
\mathit{se}(\hat{\rho}) = \sqrt{\frac{1-\hat{\rho}^2}{n-2}} = \sqrt{\frac{1-0.9981586^2}{54}} = 0.008254527
\]

The t-statistic is therefore:
\[
t_{\hat{\rho}} = \frac{\hat{\rho} - \rho^0}{\mathit{se}(\hat{\rho})} = \frac{0.9981586 - 0}{0.008254527} = 120.9226
\]
as desired. Since it is way more than 1.96 we can reject the null hypothesis.
\end{description}








\bigskip\bigskip
***
\bigskip\bigskip

### (iv) Test the hypothesis that $\beta$ is zero again, now using the following expression for the F-statistic:
\begin{equation}
F = \frac{\dfrac{(RSS_R - RSS_U)}{df}}{\dfrac{RSS_U}{df}}
\end{equation}

where $RSS_R$ and $RSS_U$ are the residual sum of squares of the restricted and unrestricted equation respectively and $df$ are degrees of freedom.

(\textbf{Note:} when testing the same hypothesis, the F-statistic is equal to the t-statistic squared - use this to check your answers to (iii) and (iv)).


\begin{description}
\item[Answer:] Notice that we are only restricting $\beta$ so the numerator degrees of freedom is $q = df_R - df_U = 1$ and the denominator degrees of freedom is $n-k-1 = 56-1-1 = 54.$

Now we just need to find the $RSS$ for unrestricted and restricted. For unrestricted, this is our usual:
\[
RSS_U = \sum_{t=1}^ne_t^2 = \sum_{t=1}^n(c_t-\bar{c})^2
\]

For the restricted one, the model is $c_t = \alpha + \epsilon_t$ thus it only depends on the error. Accordingly,
\[
RSS_R = \sum_{t=1}^ne_t^2
\]
Therefore, the F-test is:
\[
F = \frac{\dfrac{(RSS_R - RSS_U)}{df}}{\dfrac{RSS_U}{df}} = \frac{\dfrac{\left(\displaystyle\sum_{t=1}^n(c_t-\bar{c})^2 - \displaystyle\sum_{t=1}^ne_t^2\right)}{1}}{\dfrac{\displaystyle\sum_{t=1}^ne_t^2}{54}} = \frac{\dfrac{1154516 - 4247.843}{1}}{\dfrac{4247.843}{54}} = 14622.59
\]
To verify, if we take the squareroot of this:
\[
\sqrt{14622.59} = 120.9239
\]
as desired.

\end{description}












\bigskip\bigskip
***
\bigskip\bigskip

## QUESTION 2

\textbf{In the linear regression model } $y_t = a + bt + e_t$ \textbf{where } $t=1,\dots,T$ \textbf{ ( i.e. } $t$ \textbf{ is a linear trend, and so non-stochastic) and } $e_t$ \textbf{ is an independently distributed random variable with zero mean and constant variance } $\sigma^2$, \textbf{ show that the estimator } $b^*$ \textbf{ defined by } $b^* = \frac{(y_T-y_1)}{T-1}$ \textbf{ is or is not, an unbiased estimator of } $b.$ \textbf{ Use this result to comment on the claim that } $b^*$ \textbf{ has a lower variance that the Ordinary Least Squares estimator. Explain your answer. Calculate the variance of } $b^*.$ \textbf{(Optional question: for what values of T would the estimators have the same variance?)}    

\begin{description}
\item[Answer:] Lets start with obtaining an expression for the estimator $b^*$:
\[
b^* = \frac{(Y_T - y_1)}{T-1} = \frac{(a + bT + e_T) - (a + b1 +e_1)}{T-1} = \frac{b(T-1) + e_T - e_1}{T-1}
\]
So, $b^*$ can only be unbiased if $e_T=e_1=0$. Then, $\mathbb{E}(b^*)=b.$

Next we will look at the variance of $b^*$:
\[
Var(b^*) = Var\left( \frac{y_T-y_1}{T-1} \right) = \left( \frac{1}{T-1}  \right)^2 Var(y_T-y_1) = \frac{2\sigma_e^2}{(T-1)^2}
\]
Lets compare this to OLS estimator's variance:
\[
Var(\hat{\beta}) = \frac{\sigma_e^2}{\displaystyle\sum_{t=1}^n (t-\bar{t})^2}
\]
where

\begin{align*}
\sum_{t=1}^T (t-\bar{t})^2
  &= \sum_{t=1}^T (t-\bar{t})(t-\bar{t}) \\[6pt]
  &= \sum_{t=1}^T (t-\bar{t})t \\[6pt] 
  &= \sum_{t=1}^Tt^2 - \bar{t}\sum_{t=1}^Tt \ \ \ \ \ \text{ and since } \sum t^2 = \frac{T(T+1)(2T+1)}{6} \text{ and } \sum t = \frac{T(T+1)}{2} \\[6pt]
  &= \frac{T(T+1)(2T+1)}{6} - \left( \frac{T(T+1)}{2T}  \right)\left( \frac{T(T+1)}{2} \right) \\[6pt]
  &= \frac{2T(T+1)(2T+1)-3T(T+1)^2}{12} \\[6pt]
  &= \frac{T(T+1)(T-1)}{12}
\end{align*}

So, plugging this back in to the variance expression:
\[
Var(\hat{\beta}) = \frac{12\sigma_e^2}{T(T+1)(T-1)}
\]
Thus, $Var(\hat{\beta}) < Var(b^*)$. The two can be equal if $\frac{2\sigma_e^2}{(T-1)^2} = \frac{12\sigma_e^2}{T(T+1)(T-1)}$ which can happen only if

\begin{align*}
\frac{1}{T-1} &= \frac{6}{T(T+1)} \\[4pt]
T(T+1) &= 6(T-1) \\[4pt]
T^2 - 5T + 6 &= 0 \\[4pt]
(T-2)(T-3) &= 0 \\[4pt]
T = 2 \text{ or } T=3.
\end{align*}

\end{description}








\bigskip\bigskip
***
\bigskip\bigskip

## QUESTION 3

\textbf{Briefly discuss the 'ceteris paribus' and 'partialling out' interpretations of the coefficients estimated using multiple regression analysis.}

\begin{description}
\item[Answer:] Consider the case of an \textit{estimated equation} with $k=2$ independent variables: $\hat{Y}=\hat{\beta}_0 + \hat(\beta)_1X_1 + \hat{\beta}_2X_2.$

\textbf{Ceteris paribus:} The estimates $\hat{\beta}_1$ and $\hat{\beta}_2$ have partial effect interpretations. From our estimated equation we have $\Delta\hat{Y} = \hat{\beta}_1\Delta X_1 + \hat{\beta}_2\Delta X_2$ which tells us that we can obtain the predicted change in $Y$ given the changes in $X_1$ and $X_2$. Notice we do not have the intercept here, since intercept is the predicted value of $Y$ when both $X_1$ and $X_2$ are zero. The coefficient on $X_1$ measures the change in $\hat{Y}$ due to a one unit change in $X_1$, holding all other independent variables fixed: $\Delta\hat{Y} = \hat{\beta}_1\Delta X_1$ holding $X_2$ fixed - i.e. $\Delta X_2=0$. The case with $k > 2$ is similar. This way, we have \textit{controlled for} the variables $X_2, \dots, X_k$ when estimating the effect of $X_1$ on $Y$. The other coefficients have a similar interpretation.

Another way of thinking about this is to look back how we derived $\hat{\beta}_1$ before. We took the partial derivative of of $Y$ with respect to $X_1$, $\hat{\beta}_1=\frac{\partial{Y}}{\partial{X_1}}$. This tells us the rate of change of $Y$ with respect to $X_1$ holding $X_2$ constant.

\textbf{Partialling Out:} This is another representation of the partial effect interpretation of $\hat{\beta}_1$. For this approach, we partial out, or partial out the effect of $X_2$ by first regressing $X_1$ on $X_2$ using the model $X_1 = \alpha_0 + \alpha_1X_2 + r_1$. Here, the residual $r_1$ represents all the influences on $X_1$ not related to $X_2$. If we then regress $Y$ on this residual using the model $Y = \gamma_0 + \gamma_1r_1 + v$, then Frisch, Waugh, Lovell Theorem tells us that $\hat{\gamma}_1 = \hat{\beta}_1$. That is, $\hat{\beta}_1 = \frac{Cov(Y,v)}{Var{v}}.$ Thus, this shows that $\hat{\beta}_1$ is estimating the effect of $X_1$ on $Y$ after the effect of $X_2$ has been partialled out.

\end{description}











\bigskip\bigskip
***
\bigskip\bigskip

## QUESTION 4
